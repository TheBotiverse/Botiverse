{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"cyan\">LSTM </font> From Scratch Implementation\n",
    "\n",
    "In this notebook, we shall demonstrate implementing LSTM from scratch; up from linear layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='white'>LSTM Layer</font>\n",
    "\n",
    "An LSTM layer takes in the input $x_t$ and the previous hidden and cell state $h_{t-1}$, $c_{t-1}$. It outputs the next hidden and cell state $h_t$, $c_t$ in the following fashion:\n",
    "\n",
    "Define three gates and pass the input, previous hidden and cell state through them to get the outputs of the gates:\n",
    "\n",
    "$$i_t = σ(W_{xi}x_t + W_{hi}h_{t-1} + b_i)$$\n",
    "\n",
    "$$f_t = σ(W_{xf}x_t + W_{hf}h_{t-1} +  b_f)$$\n",
    "\n",
    "$$o_t = σ(W_{xo}x_t + W_{ho}h_{t-1} + b_o)$$\n",
    "\n",
    "Compute a candidate cell state and cell stage using another linear layer and input and forget gates:\n",
    "$$\\hat{c} = tanh(W_{xg}x_t + W_{hg}h_{t-1} + b_g)$$\n",
    "\n",
    "$$c_t = f_tc_{t-1} + i_tg_t$$\n",
    "\n",
    "Compute the next hidden state using the computed cell state and output gate:\n",
    "$$h_t = o_ttanh(c_t)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import sigmoid, tanh as σ, tanh\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class LSTMCell(nn.Module): \n",
    "    '''\n",
    "    An interface for a single LSTM layer.\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        '''\n",
    "        Initialize the parameters single LSTM layer given the size of the inputs and the required hidden size.\n",
    "        :param input_size: The size of the input to the LSTM layer\n",
    "        :param hidden_size: The size of the hidden state of the LSTM layer\n",
    "        '''\n",
    "        super(LSTMCell, self).__init__()\n",
    "        # input and output dimensions\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # LSTM involves three gates: input, forget, output and tanh linear layer.\n",
    "        # We can declare them as follows and chunk them later since their two weights are of same size.\n",
    "        self.Wₓₕ = nn.Linear(input_size, hidden_size * 4)\n",
    "        self.Wₕₕ = nn.Linear(hidden_size, hidden_size * 4)\n",
    "        \n",
    "        # initalize the weights using Xavier initialization\n",
    "        σ = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():     w.data.uniform_(-σ, σ)\n",
    "\n",
    "    def forward(self, input, h, c):\n",
    "        '''\n",
    "        Forward pass of the LSTM layer which passes in the input and previous states and returns the new hidden and cell states\n",
    "        :param input: The input to the LSTM layer which is of shape (batch_size, input_size)\n",
    "        :param h: The previous hidden state of the LSTM layer which is of shape (batch_size, hidden_size)\n",
    "        :param c: The previous cell state of the LSTM layer which is of shape (batch_size, hidden_size)\n",
    "        :return: The new hidden state and cell state of the LSTM layer which are of shapes (batch_size, hidden_size) each\n",
    "        '''\n",
    "        # chunk the weights and biases of the gates\n",
    "        weights = self.Wₓₕ(input) + self.Wₕₕ(h)\n",
    "        Γi, Γf, T, Γo = weights.chunk(4, 1)\n",
    "\n",
    "        # Get gates (iₜ, fₜ,  oₜ)\n",
    "        iₜ, fₜ, oₜ = σ(Γi), σ(Γf), σ(Γo)\n",
    "        \n",
    "        # compute candidate and new cell state\n",
    "        ĉ = tanh(T)\n",
    "        cₜ  = c * fₜ + iₜ * ĉ\n",
    "\n",
    "        # compute new hidden state\n",
    "        hₜ = oₜ * tanh(cₜ)\n",
    "\n",
    "        return (hₜ, cₜ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Class\n",
    "\n",
    "Given an input sequence, each token passes by all the layers and each layer has its own hidden state and cell state which is its output due to the previous token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMX(nn.Module):\n",
    "    '''\n",
    "    An interface for a multi-layer LSTM where the hidden state of each layer is of the same size.\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        '''\n",
    "        Initialize the parameters of an LSTM with an arbitrary number of layers all of which have the same hidden size.\n",
    "        :param input_size: The size of the input to the LSTM layer\n",
    "        :param hidden_size: The size of the hidden state of the LSTM layer\n",
    "        :param num_layers: The number of LSTM layers stacked on top of each other (default: 1)\n",
    "        '''\n",
    "        super(LSTMX, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # initialize num_layers of LSTM layers\n",
    "        self.lstm_layers = nn.ModuleList([LSTMCell(input_size, hidden_size)])\n",
    "        for l in range(1, self.num_layers):\n",
    "            self.lstm_layers.append(LSTMCell(self.hidden_size, self.hidden_size))\n",
    "\n",
    "\n",
    "    def forward(self, input, hₒ=None):\n",
    "        '''\n",
    "        Forward pass of the LSTM which takes the input and initial hidden/cell state and returns the output\n",
    "        due to the last token in the sequence.\n",
    "        :param input: The input to the LSTM layer which is of shape (batch_size, seq_len, input_size)\n",
    "        :param hₒ: The initial hidden and cell states of the LSTM layer which are of shape (num_layers, batch_size, hidden_size)\n",
    "        :return: The output due to the last token in the sequence which is of shape (batch_size, hidden_size)\n",
    "        '''\n",
    "        # Input of shape (batch_size, seqence length , input_size)\n",
    "        #\n",
    "        # Output of shape (batch_size, output_size)\n",
    "        batch_size, seq_len = input.size(0), input.size(1)\n",
    "        \n",
    "        if hₒ is None:\n",
    "             v = Variable(input.new_zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "             hₒ = v.cuda() if torch.cuda.is_available() and hₒ else v\n",
    "        \n",
    "        # Will contain the output of the last layer for each token in the sequence\n",
    "        outs = []\n",
    "\n",
    "        H = [hₒ[l, :, :] for l in range(self.num_layers)]\n",
    "        C = [hₒ[l, :, :] for l in range(self.num_layers)]\n",
    "        # Now H[l], C[l] are the current hidden state for layer l (defined for every sequence in the batch)\n",
    "        \n",
    "        # for each token in the sequence\n",
    "        for t in range(seq_len):\n",
    "            # pass it through each layer\n",
    "            token = input[:, t, :]\n",
    "            for l in range(self.num_layers):\n",
    "                lstm = self.lstm_layers[l]   # takes input (batch_size, input_size) and h, c of shape (batch_size, hidden_size)\n",
    "                # layer takes the token or output from the previous layer and initial hidden, cell states\n",
    "                H[l], C[l] = lstm(token, H[l], C[l]) if l == 0 else lstm(H[l - 1], H[l], C[l])\n",
    "\n",
    "            # the output due to any token is that due to the last layer of the lstm\n",
    "            outs.append(H[l])\n",
    "\n",
    "        return outs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    '''\n",
    "    LSTMClassifier class defines a high-level interface of the LSTM model for classification.\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = LSTMX(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    def fit(self, X, y, hidden_size=64, λ=0.001, num_epochs=100, val_size=0.0):\n",
    "        Xt = torch.from_numpy(X)\n",
    "        yt = torch.from_numpy(y)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=λ)\n",
    "        pbar = tqdm(range(num_epochs))\n",
    "        for epoch in pbar:\n",
    "            outputs = self(Xt)\n",
    "            loss = self.criterion(outputs.squeeze(), yt)\n",
    "            pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def predict(self, X):\n",
    "        Xt = torch.from_numpy(X)\n",
    "        outputs = self(Xt)\n",
    "        outputs = torch.argmax(outputs, dim=1)\n",
    "        return outputs.detach().numpy()\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        Xt = torch.from_numpy(X)\n",
    "        yt = torch.from_numpy(y)\n",
    "        outputs = self(Xt)\n",
    "        outputs = torch.argmax(outputs, dim=1)\n",
    "        # compute the accuracy\n",
    "        return (outputs == yt).sum().item() / len(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook LSTM.ipynb to script\n",
      "[NbConvertApp] Writing 8093 bytes to LSTM.py\n"
     ]
    }
   ],
   "source": [
    "# if running from notebook\n",
    "if __name__ == '__main__':\n",
    "    !jupyter nbconvert --to script LSTM.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
