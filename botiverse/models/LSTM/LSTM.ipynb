{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"cyan\">LSTM </font> From Scratch Implementation\n",
    "\n",
    "In this notebook, we shall demonstrate implementing LSTM from scratch; up from linear layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='white'>LSTM Layer</font>\n",
    "\n",
    "An LSTM layer takes in the input $x_t$ and the previous hidden and cell state $h_{t-1}$, $c_{t-1}$. It outputs the next hidden and cell state $h_t$, $c_t$ in the following fashion:\n",
    "\n",
    "Define three gates and pass the input, previous hidden and cell state through them to get the outputs of the gates:\n",
    "\n",
    "$$i_t = σ(W_{xi}x_t + W_{hi}h_{t-1} + b_i)$$\n",
    "\n",
    "$$f_t = σ(W_{xf}x_t + W_{hf}h_{t-1} +  b_f)$$\n",
    "\n",
    "$$o_t = σ(W_{xo}x_t + W_{ho}h_{t-1} + b_o)$$\n",
    "\n",
    "Compute a candidate cell state and cell stage using another linear layer and input and forget gates:\n",
    "$$\\hat{c} = tanh(W_{xg}x_t + W_{hg}h_{t-1} + b_g)$$\n",
    "\n",
    "$$c_t = f_tc_{t-1} + i_tg_t$$\n",
    "\n",
    "Compute the next hidden state using the computed cell state and output gate:\n",
    "$$h_t = o_ttanh(c_t)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import sigmoid, tanh as σ, tanh\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "class LSTMCell(nn.Module): \n",
    "    '''\n",
    "    An interface for a single LSTM layer.\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        '''\n",
    "        Initialize the parameters single LSTM layer given the size of the inputs and the required hidden size.\n",
    "        :param input_size: The size of the input to the LSTM layer\n",
    "        :param hidden_size: The size of the hidden state of the LSTM layer\n",
    "        '''\n",
    "        super(LSTMCell, self).__init__()\n",
    "        # input and output dimensions\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # LSTM involves three gates: input, forget, output and tanh linear layer.\n",
    "        # We can declare them as follows and chunk them later since their two weights are of same size.\n",
    "        self.Wₓₕ = nn.Linear(input_size, hidden_size * 4)\n",
    "        self.Wₕₕ = nn.Linear(hidden_size, hidden_size * 4)\n",
    "        \n",
    "        # initalize the weights using Xavier initialization\n",
    "        σ = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():     w.data.uniform_(-σ, σ)\n",
    "\n",
    "    def forward(self, input, h, c):\n",
    "        '''\n",
    "        Forward pass of the LSTM layer which passes in the input and previous states and returns the new hidden and cell states\n",
    "        :param input: The input to the LSTM layer which is of shape (batch_size, input_size)\n",
    "        :param h: The previous hidden state of the LSTM layer which is of shape (batch_size, hidden_size)\n",
    "        :param c: The previous cell state of the LSTM layer which is of shape (batch_size, hidden_size)\n",
    "        :return: The new hidden state and cell state of the LSTM layer which are of shapes (batch_size, hidden_size) each\n",
    "        '''\n",
    "        # chunk the weights and biases of the gates\n",
    "        weights = self.Wₓₕ(input) + self.Wₕₕ(h)\n",
    "        Γi, Γf, T, Γo = weights.chunk(4, 1)\n",
    "\n",
    "        # Get gates (iₜ, fₜ,  oₜ)\n",
    "        iₜ, fₜ, oₜ = σ(Γi), σ(Γf), σ(Γo)\n",
    "        \n",
    "        # compute candidate and new cell state\n",
    "        ĉ = tanh(T)\n",
    "        cₜ  = c * fₜ + iₜ * ĉ\n",
    "\n",
    "        # compute new hidden state\n",
    "        hₜ = oₜ * tanh(cₜ)\n",
    "\n",
    "        return (hₜ, cₜ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Class\n",
    "\n",
    "Given an input sequence, each token passes by all the layers and each layer has its own hidden state and cell state which is its output due to the previous token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMX(nn.Module):\n",
    "    '''\n",
    "    An interface for a multi-layer LSTM where the hidden state of each layer is of the same size.\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        '''\n",
    "        Initialize the parameters of an LSTM with an arbitrary number of layers all of which have the same hidden size.\n",
    "        :param input_size: The size of the input to the LSTM layer\n",
    "        :param hidden_size: The size of the hidden state of the LSTM layer\n",
    "        :param num_layers: The number of LSTM layers stacked on top of each other (default: 1)\n",
    "        '''\n",
    "        super(LSTMX, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # initialize num_layers of LSTM layers\n",
    "        self.lstm_layers = nn.ModuleList([LSTMCell(input_size, hidden_size)])\n",
    "        for l in range(1, self.num_layers):\n",
    "            self.lstm_layers.append(LSTMCell(self.hidden_size, self.hidden_size))\n",
    "\n",
    "\n",
    "    def forward(self, input, hₒ=None):\n",
    "        '''\n",
    "        Forward pass of the LSTM which takes the input and initial hidden/cell state and returns the output\n",
    "        due to the last token in the sequence.\n",
    "        :param input: The input to the LSTM layer which is of shape (batch_size, seq_len, input_size)\n",
    "        :param hₒ: The initial hidden and cell states of the LSTM layer which are of shape (num_layers, batch_size, hidden_size)\n",
    "        :return: The output due to the last token in the sequence which is of shape (batch_size, hidden_size)\n",
    "        '''\n",
    "        # Input of shape (batch_size, seqence length , input_size)\n",
    "        #\n",
    "        # Output of shape (batch_size, output_size)\n",
    "        batch_size, seq_len = input.size(0), input.size(1)\n",
    "        \n",
    "        if hₒ is None:\n",
    "             v = Variable(input.new_zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "             hₒ = v.cuda() if torch.cuda.is_available() and hₒ else v\n",
    "        \n",
    "        # Will contain the output of the last layer for each token in the sequence\n",
    "        outs = []\n",
    "\n",
    "        H = [hₒ[l, :, :] for l in range(self.num_layers)]\n",
    "        C = [hₒ[l, :, :] for l in range(self.num_layers)]\n",
    "        # Now H[l], C[l] are the current hidden state for layer l (defined for every sequence in the batch)\n",
    "        \n",
    "        # for each token in the sequence\n",
    "        for t in range(seq_len):\n",
    "            # pass it through each layer\n",
    "            token = input[:, t, :]\n",
    "            for l in range(self.num_layers):\n",
    "                lstm = self.lstm_layers[l]   # takes input (batch_size, input_size) and h, c of shape (batch_size, hidden_size)\n",
    "                # layer takes the token or output from the previous layer and initial hidden, cell states\n",
    "                H[l], C[l] = lstm(token, H[l], C[l]) if l == 0 else lstm(H[l - 1], H[l], C[l])\n",
    "\n",
    "            # the output due to any token is that due to the last layer of the lstm\n",
    "            outs.append(H[l])\n",
    "\n",
    "        return outs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    '''\n",
    "    LSTMClassifier class defines a high-level interface of the LSTM model for classification.\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = LSTMX(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of the LSTMClassifier which takes the input and passes it through all the LSTM layers and an output layer to produce an output.\n",
    "        :param x: The input to the LSTMClassifier which is of shape (batch_size, seq_len, input_size)\n",
    "        :return: The output of the LSTMClassifier which is of shape (batch_size, num_classes)\n",
    "        '''\n",
    "        out = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, λ=0.001, α=1e-3, max_epochs=100, patience=5, val_ratio=0.2):\n",
    "        '''\n",
    "        Fit the LSTMClassifier to the given data.\n",
    "        :param X: The input data of shape (batch_size, seq_len, input_size)\n",
    "        :param y: The labels of the data of shape (batch_size)\n",
    "        :param hidden_size: The size of the hidden state of the LSTM layer (default: 64)\n",
    "        :param λ: The learning rate (default: 0.001)\n",
    "        :param num_epochs: The number of epochs to train the model for (default: 100)\n",
    "        '''\n",
    "        Xt = torch.from_numpy(X)\n",
    "        yt = torch.from_numpy(y)\n",
    "        if val_ratio:\n",
    "            indices = torch.randperm(len(Xt))\n",
    "            Xt, yt = Xt[indices], yt[indices]\n",
    "            # split the data into train and validation sets\n",
    "            val_size = int(val_ratio * len(Xt))\n",
    "            Xt, Xv = Xt[:-val_size], Xt[-val_size:]\n",
    "            yt, yv = yt[:-val_size], yt[-val_size:]\n",
    "            self.Xv, self.yv = Xv, yv\n",
    "                \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=λ, weight_decay=α)\n",
    "        print(\"Training the LSTMClassifier...\")\n",
    "        curr_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "        bad_epochs = 0\n",
    "        val_accuracy = 0\n",
    "        val_loss = 0\n",
    "        best_loss = np.inf\n",
    "        pbar = tqdm(range(max_epochs))\n",
    "        for epoch in pbar:\n",
    "            outputs = self(Xt)\n",
    "            loss = self.criterion(outputs.squeeze(), yt)\n",
    "            pbar.set_description(f\"Epoch {epoch+1}/{max_epochs}, Loss: {loss.item()}\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if val_ratio:\n",
    "                # randomly shuffle the data\n",
    "                val_accuracy = self.evaluate(Xv, yv)\n",
    "                with torch.no_grad():\n",
    "                    val_loss = self.criterion(self(Xv).squeeze(), yv)\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    bad_epochs = 0\n",
    "                    # save the model\n",
    "                    torch.save(self.state_dict(), os.path.join(curr_dir, \"LSTMClassifier.pt\"))\n",
    "                else:\n",
    "                    bad_epochs += 1\n",
    "                    if bad_epochs == patience:\n",
    "                        print(f\"{patience} epochs have passed without improvement. Early stopping...\")\n",
    "                        self.load_state_dict(torch.load(os.path.join(curr_dir, \"LSTMClassifier.pt\")))\n",
    "                        break\n",
    "                # every 5 epochs see\n",
    "                pbar.set_postfix({\"Validation Accuracy\": val_accuracy})             \n",
    "           \n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict the labels of the given data by passing it through the LSTMClassifier.\n",
    "        :param X: The input data of shape (batch_size, seq_len, input_size)\n",
    "        :return: The predicted labels of the data of shape (batch_size)\n",
    "        '''\n",
    "        Xt = torch.from_numpy(X)\n",
    "        outputs = self(Xt)\n",
    "        pred = torch.argmax(outputs, dim=1)\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        prob = torch.max(softmax(outputs), dim=1)\n",
    "        return pred.detach().numpy(), prob.values.detach().numpy()\n",
    "    \n",
    "    def evaluate(self, Xt, yt):\n",
    "        '''\n",
    "        Evaluate the LSTMClassifier on the given data.\n",
    "        :param X: The input data of shape (batch_size, seq_len, input_size)\n",
    "        :param y: The labels of the data of shape (batch_size)\n",
    "        :return: The accuracy of the LSTMClassifier on the given data\n",
    "        '''\n",
    "        # check ig they are torch tensors\n",
    "        if not isinstance(Xt, torch.Tensor) or not isinstance(yt, torch.Tensor):\n",
    "            Xt = torch.from_numpy(Xt)\n",
    "            yt = torch.from_numpy(yt)\n",
    "        outputs = self(Xt)\n",
    "        outputs = torch.argmax(outputs, dim=1)\n",
    "        # compute the accuracy\n",
    "        return (outputs == yt).sum().item() / len(yt)\n",
    "    \n",
    "    def save(self, path):\n",
    "        '''\n",
    "        Save the LSTMClassifier to a file.\n",
    "        :param path: The path to the file\n",
    "        '''\n",
    "        torch.save(self.state_dict(), path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        '''\n",
    "        Load the LSTMClassifier from a file.\n",
    "        :param path: The path to the file\n",
    "        '''\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook LSTM.ipynb to script\n",
      "[NbConvertApp] Writing 11637 bytes to LSTM.py\n"
     ]
    }
   ],
   "source": [
    "# if running from notebook\n",
    "if __name__ == '__main__':\n",
    "    !jupyter nbconvert --to script LSTM.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
