{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"cyan\">FastSpeech 1.0 </font> Implementation\n",
    "\n",
    "In this notebook, we shall demonstrate implementing FastSpeech 1.0 from scratch for inference purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FastSpeech 1.0 interface and implementation from scratch in PyTorch for inference.\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Image](https://i.imgur.com/ZDR7wqr.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clearly, we have three main components in the model:\n",
    "\n",
    "<div align='center'>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"1\"><font color=\"yellow\">Component</font></th>\n",
    "    <th colspan=\"3\"><font color=\"yellow\">Encoder</font></th>\n",
    "    <th colspan=\"2\"><font color=\"yellow\">Length Regulator</font></th>\n",
    "    <th colspan=\"3\"><font color=\"yellow\">Decoder</font></th>\n",
    "\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"1\"><font color=\"white\">Subcomponenets</font></th>\n",
    "    <th colspan=\"1\"><font color=\"white\">Phoneme Embedding</font></th>\n",
    "    <th colspan=\"1\"><font color=\"white\">Positional Encoding</font></th>\n",
    "    <th colspan=\"1\"><font color=\"white\">FFT Block</font></th>\n",
    "    <th colspan=\"1\"><font color=\"white\">Duration Predictor</font></th>\n",
    "    <th colspan=\"1\"><font color=\"white\">LR Logic</font></th>\n",
    "    <th colspan=\"1\"><font color=\"white\">Positional Encoding</font></th>\n",
    "    <th colspan=\"1\"><font color=\"white\">FFT Block</font></th>\n",
    "    <th colspan=\"1\"><font color=\"white\">Linear Layer</font></th>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "  <th colspan=\"1\">Takes</th>\n",
    "  <td colspan=\"3\"> <font color='cyan'>[batch_size, seq_len]</font></td>\n",
    "  <td colspan=\"2\"> <font color='cyan'>[batch_size, seq_len, emb_dim]</font> </td>\n",
    "  <td colspan=\"3\"> <font color='cyan'>[batch_size, new_seq_len, emb_dim]</font> </td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "  <th colspan=\"1\">Yields</th>\n",
    "  <td colspan=\"3\"> <font color='cyan'>[batch_size, seq_len, emb_dim]  </td>\n",
    "  <td colspan=\"2\"> <font color='cyan'>[batch_size, new_seq_len, emb_dim]</font> </td>\n",
    "  <td colspan=\"3\"> <font color='cyan'>[batch_size, new_seq_len, mel_num]</font> </td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "  <th colspan=\"1\">Purpose</th>\n",
    "  <td colspan=\"3\">Learn a representation for phonemes. The three layers within guarantee that underlying words, how they are ordered and other words all take part in the representation. </td>\n",
    "  <td colspan=\"2\">Predict the duration of each phoneme and repeat accordingly</td>\n",
    "  <td colspan=\"3\">Given time-aligned phoneme representations learn to transform them into a mel spectogram</td>\n",
    "  </tr>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "Once we have the spectogram, all that's needed it a vocoder to transform to audio by performing an approximate inverse mel-transform such as Griffin-Lim algorithm or using a pretrained model that does the task more accurately such as WaveGlow; hence, we will go with the latter but we won't implement WaveGlow from scratch.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will start by implementing \n",
    "\n",
    "<div align='left'>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\"><font color=\"cyan\">FFT Block</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"1\">Multi-head Attention</th>\n",
    "    <th colspan=\"1\">Conv1DNet</th>\n",
    "  </tr>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "#### Then we have that\n",
    "\n",
    "<div align='left'>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"3\"><font color=\"deepskyblue\">Encoder</font></th>\n",
    "    <th colspan=\"3\"><font color=\"deepskyblue\">Decoder</font></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"1\">Phoneme Emb.</th>\n",
    "    <th colspan=\"1\">Positional Enc.</th>\n",
    "    <th colspan=\"1\">FFT Block</th>\n",
    "    <th colspan=\"1\">Positional Encoding</th>\n",
    "    <th colspan=\"1\">FFT Block</th>\n",
    "    <th colspan=\"1\">Linear Layer</th>\n",
    "  </tr>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "#### So we follow with\n",
    "\n",
    "<div align='left'>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\"><font color=\"deepskyblue\">Length Regulator</font></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"1\">LR Logic</th>\n",
    "    <th colspan=\"1\">Duration Predictor</th>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "#### And we finally have that\n",
    "\n",
    "<div align='left'>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"3\"><font color=\"deepskyblue\">Model</font></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"1\">Encoder</th>\n",
    "    <th colspan=\"1\">Length Regulator</th>\n",
    "    <th colspan=\"1\">Decoder</th>\n",
    "  </tr>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"cyan\">1. Feedforward-Transformer Block </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://i.imgur.com/Kb88BwT.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"white\"> Multi-head Self-Attention </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "\n",
    "$MultiHead(Q, K, V) = \\text{Concat}(head_1, \\ldots, head_h) \\cdot W^O$\n",
    "\n",
    "where\n",
    "\n",
    "  $head = \\text{Attention}(qW_q, kW_k, vW_v) =\\text{Attention}(Q, K, V)$\n",
    "\n",
    "  such that\n",
    "\n",
    "  $\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\cdot V$\n",
    "\n",
    "  \n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    Multi-Head Attention module with a residual connection and layer normalization. Used as self-attention in the FFT block of the encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, num_head, emb_dim, h_dim, dropout=0.1):\n",
    "        '''\n",
    "        Initialize the parameters and structure of the Multi-Head Attention module.\n",
    "        num_head: number of heads\n",
    "        emb_dim: input encoder/decoder embeddings dimensions\n",
    "        h_dim: hidden dimension (output dimension of the linear layers Wq, Wk, Wv)\n",
    "        dropout: dropout probability\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_head = num_head\n",
    "        self.h_dim = h_dim                            # dimensionality of the final ouput\n",
    "        self.head_dim = h_dim // num_head             # dimensionality of each head\n",
    "\n",
    "        self.Wq = nn.Linear(emb_dim, h_dim)         # Equivalent to using head_dim, num_head times\n",
    "        self.Wk = nn.Linear(emb_dim, h_dim)\n",
    "        self.Wv = nn.Linear(emb_dim, h_dim)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = nn.Linear(h_dim, emb_dim)\n",
    "        \n",
    "       \n",
    "    def forward(self, q, k, v, mask):\n",
    "        '''\n",
    "        Pass given query, key and value through the Multi-Head Attention module.\n",
    "        q: query of shape [batch_size, seq_len, emb_dim]\n",
    "        k: key of shape [batch_size, seq_len, emb_dim]\n",
    "        v: value of shape [batch_size, seq_len, emb_dim]\n",
    "        mask: mask to apply to the attention so that padding tokens do not attend and are not attended to.\n",
    "        returns: output of shape [batch_size, seq_len, emb_dim]    \n",
    "        '''\n",
    "        residual = q\n",
    "        batch_size, num_head, seq_len, head_dim = q.size(0), self.num_head, q.size(1), self.head_dim\n",
    "        Q, K, V = self.Wq(q), self.Wk(k), self.Wv(v)              # [batch_size, seq_len, emb_dim -> h_dim]\n",
    "\n",
    "        review = lambda X: X.view(batch_size, seq_len, num_head, head_dim).transpose(1, 2).contiguous()\n",
    "        Q, K, V = review(Q), review(K), review(V)                  # [batch_size, num_head, seq_len, head_dim]\n",
    "        \n",
    "        reshape = lambda X: X.view(batch_size * num_head, seq_len, head_dim)\n",
    "        Q, K, V = reshape(Q), reshape(K), reshape(V)               # [batch_size * num_head, seq_len, head_dim]\n",
    "\n",
    "        mask = mask.repeat(num_head, 1, 1)                         # [batch_size * num_head, seq_len, seq_len]\n",
    "        \n",
    "        a = torch.bmm(Q, K.transpose(1, 2))/self.scale             # [batch_size * num_head, seq_len, seq_len]\n",
    "        \n",
    "        a = a.masked_fill(mask, -np.inf)\n",
    "\n",
    "        a = self.softmax(a)\n",
    "        \n",
    "        a = self.dropout(a)\n",
    "        \n",
    "        output = torch.bmm(a, V)                                  # [batch_size * num_head, seq_len, head_dim]   \n",
    "        \n",
    "        output = output.view(batch_size, num_head, seq_len, head_dim).transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, num_head * head_dim)\n",
    "        # [batch_size, seq_len, num_head * head_dim]\n",
    "\n",
    "        output = self.dropout(self.fc(output))\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"white\"> Conv1D Network </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Embeddings ⇒ Conv1D ⇒ ReuLU ⇒ Conv1D ⇒  Dropout ⇒ AddNorm$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DNet(nn.Module):\n",
    "    '''\n",
    "    1D convolutional network with residual connection and layer normalization. Used in the FFT block of the encoder and decoder.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
    "        '''\n",
    "        Initialize the parameters and structure of the 1D convolutional network.\n",
    "        inp_dim: input dimension\n",
    "        inner_dim: inner dimension of the convolutional layers (output dimension of the first convolutional layer)\n",
    "        dropout: dropout probability\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # Both convolutions have SAME padding so they only change the number of 1D channels\n",
    "        self.conv1 = nn.Conv1d(inp_dim, inner_dim, kernel_size=9, padding=4) # from n to n-9+2*4+1 = n\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(inner_dim, inp_dim, kernel_size=1, padding=0) # from n to n-1+2*0+1 = n\n",
    "        self.layer_norm = nn.LayerNorm(inp_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):   \n",
    "        '''\n",
    "        Pass given input through the 1D convolutional network. The input comes from the Multi-Head Attention module.\n",
    "        x: input of shape [batch_size, seq_len, d_in]\n",
    "        returns: output of shape [batch_size, seq_len, d_in]\n",
    "        '''\n",
    "        residual = x                                     # [batch_size, seq_len, d_in]\n",
    "        x = x.transpose(1, 2)                            # [batch_size, d_in, seq_len]\n",
    "        x = F.relu(self.conv1(x))                        # [batch_size, d_out, seq_len]\n",
    "        x = self.conv2(x)                                # [batch_size, din, seq_len]\n",
    "        x = x.transpose(1, 2)                            # [batch_size, seq_len, din]\n",
    "        \n",
    "        output = self.dropout(x)\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"white\"> FFT Block </font>\n",
    "\n",
    "Here we just put the two pieces together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTBlock(torch.nn.Module):\n",
    "    '''\n",
    "    FFT block used in the encoder and decoder. It consists of a Multi-Head Attention module and a 1D convolutional network.\n",
    "    '''\n",
    "    def __init__(self,  emb_dim, num_head, h_dim,  inner_dim, dropout=0.1):\n",
    "        '''\n",
    "        Initialize the structure of the FFT block.\n",
    "        emb_dim: input encoder/decoder embeddings dimensions\n",
    "        inner_dim: inner dimension of the convolutional layers (output dimension of the first convolutional layer)\n",
    "        num_head: number of heads for the Multi-Head Attention module\n",
    "        h_dim: hidden dimension (output dimension of the linear layers Wq, Wk, Wv)\n",
    "        dropout: dropout probability\n",
    "        '''\n",
    "        super(FFTBlock, self).__init__()\n",
    "        self.attn_out = MultiHeadAttention(num_head, emb_dim, h_dim, dropout=dropout)\n",
    "        self.conv_out = Conv1DNet(emb_dim, inner_dim, dropout=dropout)\n",
    "\n",
    "    def forward(self, input, non_pad_mask, attn_mask):\n",
    "        '''\n",
    "        Pass given encoder/decoder embeddings through the FFT block. The input comes from the previous FFT block or the input embeddings.\n",
    "        input: input of shape [batch_size, seq_len, emb_dim]\n",
    "        non_pad_mask: mask to nullify outputs due to padding tokens.\n",
    "        attn_mask: mask to apply to the attention so that future tokens do not attend and are not attended to.\n",
    "        returns: output of shape [batch_size, seq_len, emb_dim] and attention weights of shape [batch_size * num_head, seq_len, seq_len]\n",
    "        '''\n",
    "        non_pad_mask = non_pad_mask.float()\n",
    "        # From [batch_size, seq_len, emb_dim] to [batch_size, seq_len, h_dim->emb_dim]\n",
    "        output = self.attn_out(input, input, input, attn_mask) * non_pad_mask\n",
    "\n",
    "        # From [batch_size, seq_len, emb_dim] to [batch_size, seq_len, emb_dim]\n",
    "        output = self.conv_out(output) * non_pad_mask\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"cyan\">2. Encoder</font> & <font color=\"cyan\">Decoder</font> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/HKBCPO7.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's obvious that they share most of the structure except for the first/last layer. Hence, we will implement both in one module. But before we do we need positional encoding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"white\"> Positional Encoding </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$PE_{(pos, 2i)} = \\sin(\\frac{{pos}}{{10000^{2i/d_{\\text{inp}}}}})$$\n",
    "\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos(\\frac{{pos}}{{10000^{2i/d_{\\text{inp}}}}})$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this is to add some notion of order to the input sequence without distorting it by adding a positional depending signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidEncodingTable:\n",
    "    '''\n",
    "    Sinusoid encoding table used in the encoder and decoder. It is used to add positional information to the input embeddings.\n",
    "    '''\n",
    "    def __init__(self, max_seq_len, inp_dim, padding_idx=None):\n",
    "        '''\n",
    "        Initialize the parameters and structure of the sinusoid encoding table.\n",
    "        max_seq_len: maximum sequence length\n",
    "        inp_dim: input encoder/decoder embeddings dimensions\n",
    "        padding_idx: index of the padding token\n",
    "        '''\n",
    "        self.max_seq_len = max_seq_len + 1\n",
    "        self.inp_dim = inp_dim\n",
    "        self.padding_idx = padding_idx\n",
    "        self.sinusoid_table = self.build_table()\n",
    "\n",
    "    def build_table(self):\n",
    "        '''\n",
    "        Build the sinusoid encoding table.\n",
    "        :returns: sinusoid encoding table of shape [max_seq_len, inp_dim] which is indexed by the position of the input embeddings and the index of input emeddings value.\n",
    "        '''\n",
    "        # compute the denominator\n",
    "        inds = np.arange(self.inp_dim) // 2\n",
    "        div_term = np.power(10000, 2 * inds / self.inp_dim)\n",
    "\n",
    "        # compute the numerator\n",
    "        positions = np.arange(self.max_seq_len)\n",
    "                \n",
    "        # compute table of shape [max_seq_len, inp_dim] \n",
    "        sinusoid_table = np.outer(positions, 1/div_term)\n",
    "\n",
    "        # apply sin to even indices in the array; 2i and cos to odd indices; 2i+1\n",
    "        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) \n",
    "        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  \n",
    "\n",
    "        if self.padding_idx is not None:\n",
    "            sinusoid_table[self.padding_idx] = 0.\n",
    "\n",
    "        return torch.Tensor(sinusoid_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"white\"> Dencoder </font>\n",
    "\n",
    "This is both the encoder and decoder in one module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dencoder(nn.Module):\n",
    "    '''\n",
    "    A single module that implements the encoder and decoder of the FastSpeech 1.0 model. \n",
    "    '''\n",
    "    def __init__(self, mode, vocab_dim, max_seq_len, emb_dim, num_layer, num_head, h_dim, d_inner, mel_num, dropout):\n",
    "        '''\n",
    "        Initialize the parameters and structure of the encoder/decoder.\n",
    "        mode: 'Encoder' or 'Decoder'\n",
    "        vocab_dim: vocabulary dimension. None if mode is 'Decoder'\n",
    "        max_seq_len: maximum sequence length as needed by the sinusoid encoding table\n",
    "        emb_dim: encoder/decoder embeddings dimensions\n",
    "        num_layer: number of FFT blocks\n",
    "        num_head: number of heads for the Multi-Head Attention module\n",
    "        h_dim: hidden dimension (output dimension of the linear layers Wq, Wk, Wv)\n",
    "        d_inner: inner dimension of the convolutional layers (output dimension of the first convolutional layer)\n",
    "        mel_num: number of mel spectrogram bins (to map the final decoder embeddings). None if mode is 'Encoder'\n",
    "        dropout: dropout probability\n",
    "        '''\n",
    "        super(Dencoder, self).__init__()\n",
    "        self.mode= mode\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_dim, emb_dim, padding_idx=0) if self.mode == 'Encoder' else lambda x: x\n",
    "\n",
    "        table = SinusoidEncodingTable(max_seq_len, emb_dim, padding_idx=0).sinusoid_table\n",
    "        self.position_encoding = nn.Embedding.from_pretrained(table, freeze=True)\n",
    "\n",
    "        self.layer_stack = nn.ModuleList([FFTBlock(emb_dim, num_head, h_dim, d_inner, dropout=dropout) for _ in range(num_layer)])\n",
    "\n",
    "        self.linear = nn.Linear(emb_dim, mel_num) if self.mode == 'Decoder' else lambda x: x\n",
    "            \n",
    "    def forward(self, inp_seq, inp_seq_pos):                                            \n",
    "        '''\n",
    "        Pass given input sequence through the encoder/decoder.\n",
    "        inp_seq: input sequence of shape [batch_size, seq_len] for Encoder or [batch_size, seq_len, emb_dim] for Decoder\n",
    "        inp_seq_pos: input sequence positions of shape [batch_size, seq_len] \n",
    "        '''\n",
    "        # Prepare non-pad and attention masks\n",
    "        inp_attn = inp_seq if self.mode == 'Encoder' else inp_seq_pos              # [batch_size, seq_len]\n",
    "        \n",
    "        non_pad_mask = (inp_attn.unsqueeze(2) != 0)                                # [batch_size, seq_len, 1]\n",
    "        attn_mask = (inp_attn.unsqueeze(1) == 0).repeat(1, inp_attn.size(1), 1)    # [batch_size, seq_len, seq_len]\n",
    "\n",
    "        # Forward\n",
    "        output = self.embedding(inp_seq) + self.position_encoding(inp_seq_pos)       # [batch_size, seq_len, emb_dim]\n",
    "        for layer in self.layer_stack:\n",
    "            output = layer(output, non_pad_mask= non_pad_mask, attn_mask=attn_mask)\n",
    "        \n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"cyan\">3. Length Regulator </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/xUvPwh6.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"white\"> Duration Predictor </font>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$PhonemeEmb ⇒ Conv1D ⇒ ReuLU ⇒ LayerNorm ⇒  Dropout ⇒ Conv1D ⇒ ReuLU ⇒ LayerNorm ⇒  Dropout ⇒ Relu ⇒ Linear$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DurationPredictor(nn.Module):\n",
    "    '''\n",
    "    Duration predictor module. It predicts the duration of each phoneme in the input sequence of encoder phoneme embeddings.\n",
    "    '''\n",
    "    def __init__(self, inp_dim, inner_dim, kernel_size, padding_size, dropout=0.1):\n",
    "        '''\n",
    "        Initialize the parameters and structure of the duration predictor module.\n",
    "        inp_dim: input dimension\n",
    "        inner_dim: inner dimension of the convolutional layers (output dimension of the first convolutional layer)\n",
    "        kernel_size: kernel size of the convolutional layers\n",
    "        padding_size: padding size of the convolutional layers\n",
    "        dropout: dropout probability\n",
    "        '''\n",
    "        super(DurationPredictor, self).__init__()\n",
    "\n",
    "        self.conv1d_1 = nn.Conv1d(inp_dim, inner_dim, kernel_size, padding=padding_size)\n",
    "        self.layer_norm_1 = nn.LayerNorm(inner_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv1d_2 = nn.Conv1d(inner_dim, inner_dim, kernel_size, padding=padding_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(inner_dim)\n",
    "        self.linear_layer = nn.Linear(inner_dim, 1)     \n",
    "        # predicts a scalar mel_duration per phoneme\n",
    "\n",
    "    def forward(self, encoder_output):\n",
    "        '''\n",
    "        Pass given encoder output through the duration predictor module. The input comes from the encoder. \n",
    "        encoder_output: encoder output of shape [batch_size, seq_len, emb_dim]\n",
    "        returns: predicted duration of each phoneme in the input sequence of encoder phoneme embeddings of shape [batch_size, seq_len]\n",
    "        '''\n",
    "        x = encoder_output.contiguous().transpose(1, 2)\n",
    "        x = self.conv1d_1(x)\n",
    "        x = x.contiguous().transpose(1,2)\n",
    "        x = self.relu(self.layer_norm_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = x.contiguous().transpose(1, 2)\n",
    "        x = self.conv1d_2(x)\n",
    "        x = x.contiguous().transpose(1, 2)\n",
    "        x = self.relu(self.layer_norm_2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        out = self.relu(self.linear_layer(x))\n",
    "\n",
    "        out = out.squeeze()\n",
    "        out = out.unsqueeze(0)      # Leading dimension should not be removed in inference.\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"white\">Length Regulation Logic </font>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $H=[h_1, h_2, ..., h_n]$ as phone embeddings from the encoder predict the duration of each phoneme $d=[d_1, d_2, ..., d_n]$ and repeat accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LengthRegulator(nn.Module):\n",
    "    '''\n",
    "    Length regulator module. It repeats the encoder outputs according to the predicted duration of each phoneme in the input sequence of encoder phoneme embeddings.\n",
    "    '''\n",
    "    def __init__(self, inp_dim, inner_dim, kernel_size, padding_size, dropout=0.1):\n",
    "        '''\n",
    "        Initialize the parameters and structure of the duration predictor module.\n",
    "        inp_dim: input dimension\n",
    "        inner_dim: inner dimension of the convolutional layers (output dimension of the first convolutional layer)\n",
    "        kernel_size: kernel size of the convolutional layers\n",
    "        padding_size: padding size of the convolutional layers\n",
    "        dropout: dropout probability\n",
    "        '''\n",
    "        super(LengthRegulator, self).__init__()\n",
    "        self.duration_predictor = DurationPredictor(inp_dim, inner_dim, kernel_size, padding_size, dropout)\n",
    "\n",
    "    def forward(self, enc_output):\n",
    "        '''\n",
    "        Pass given encoder output through the length regulator module. The input comes from the encoder.\n",
    "        enc_output: encoder output of shape [batch_size, seq_len, emb_dim]\n",
    "        '''\n",
    "        tiny_slowdown = 0.5                                  # to prevent rounding from dropping phonemes \n",
    "        duration_predictions = (self.duration_predictor(enc_output) + tiny_slowdown).int()\n",
    "        # [batch_size, seq_len, enc_dim] to [batch_size, new_seq_len] (scalar mel_duration per phoneme)\n",
    "        \n",
    "        # repeat each phoneme in the encoder output according to its predicted duration\n",
    "        new_seq_lens_per_batch = torch.sum(duration_predictions, -1)\n",
    "        new_seq_len = torch.max(new_seq_lens_per_batch).item()\n",
    "        \n",
    "        batch_size, seq_len, enc_dim = enc_output.size()\n",
    "        mod_enc_output = torch.zeros((batch_size, new_seq_len, enc_dim))\n",
    "        for sequence in range(batch_size):\n",
    "            count = 0\n",
    "            for phoneme in range(seq_len):\n",
    "                hidden = enc_output[sequence][phoneme]\n",
    "                reps = duration_predictions[sequence][phoneme]\n",
    "                for _ in range(reps):\n",
    "                    mod_enc_output[sequence][count] = hidden\n",
    "                    count += 1\n",
    "        \n",
    "        # form a new positional encoding for the modified encoder output\n",
    "        output_pos = torch.LongTensor([i+1 for i in range(mod_enc_output.size(1))]).unsqueeze(0).to(device)\n",
    "        # [batch_size, seq_len, enc_dim] -> [batch_size, new_seq_len, enc_dim] and [batch_size, new_seq_len]\n",
    "        return mod_enc_output, output_pos\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"cyan\">4. Model </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/azh7cwn.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encoder\n",
    "VOCAB_SIZE = 300\n",
    "ENC_EMB_DIM = 256\n",
    "ENC_NUM_LAYER = 4\n",
    "ENC_NUM_HEAD = 2\n",
    "ENC_1D_FILTER_SIZE = 1024\n",
    "DROPOUT_PROB = 0.1\n",
    "\n",
    "# Length Regulator\n",
    "INNER_DIM = 256\n",
    "DP_KERNEL_SIZE = 3\n",
    "DROPOUT_PROB = 0.1\n",
    "DP_PADDING = 1\n",
    "\n",
    "# Decoder\n",
    "MAX_SEQ_LEN = 3000\n",
    "DEC_EMB_DIM = 256\n",
    "DEC_NUM_LAYER = 4\n",
    "DEC_NUM_HEAD = 2\n",
    "DEC_1D_FILTER_SIZE = 1024\n",
    "MEL_NUM = 80\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"white\">FastSpeech</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FastSpeech(nn.Module):\n",
    "    '''\n",
    "    FastSpeech 1.0 model. It consists of an encoder, a duration predictor, a length regulator and a decoder.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize the structure of the FastSpeech 1.0 model.\n",
    "        '''\n",
    "        super(FastSpeech, self).__init__()\n",
    "\n",
    "        self.encoder = Dencoder(mode='Encoder', vocab_dim=VOCAB_SIZE, max_seq_len=VOCAB_SIZE, emb_dim=ENC_EMB_DIM,\n",
    "                                num_layer=ENC_NUM_LAYER, num_head=ENC_NUM_HEAD, h_dim=ENC_EMB_DIM,\n",
    "                                d_inner=ENC_1D_FILTER_SIZE, mel_num=None, dropout=DROPOUT_PROB)\n",
    "        \n",
    "        self.length_regulator = LengthRegulator(inp_dim=ENC_EMB_DIM, inner_dim=INNER_DIM, kernel_size=DP_KERNEL_SIZE, \n",
    "                                                padding_size=DP_PADDING, dropout=DROPOUT_PROB)\n",
    "        \n",
    "        self.decoder = Dencoder(mode='Decoder', vocab_dim=None, max_seq_len=MAX_SEQ_LEN, emb_dim=DEC_EMB_DIM, \n",
    "                                num_layer=DEC_NUM_LAYER, num_head=DEC_NUM_HEAD, h_dim=DEC_EMB_DIM,\n",
    "                                d_inner=DEC_1D_FILTER_SIZE, mel_num=MEL_NUM,  dropout=DROPOUT_PROB)\n",
    "\n",
    "\n",
    "    def forward(self, text_seq, src_pos):\n",
    "        '''\n",
    "        Pass given input sequence through the FastSpeech 1.0 model. input sequence of shape [batch_size, seq_len] and assigns a unique id to each character in it.\n",
    "        text_seq: input sequence of shape [batch_size, seq_len]\n",
    "        src_pos: input sequence positions (indecies) of shape [batch_size, seq_len]\n",
    "        :returns: predicted mel spectrogram of shape [batch_size, mel_num, new_seq_len]\n",
    "        '''\n",
    "        enc_output = self.encoder(text_seq, src_pos)\n",
    "        \n",
    "        length_regulator_output, decoder_pos = self.length_regulator(enc_output)\n",
    "        \n",
    "        spectogram = self.decoder(length_regulator_output, decoder_pos)\n",
    "\n",
    "        return spectogram\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"cyan\">5. Waveglow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/Xl9HEgm.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\"> Inference </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from scipy.io.wavfile import write\n",
    "from playsound import playsound\n",
    "import os\n",
    "import waveglow\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTS():\n",
    "    def __init__(self, force_download_wg=False, force_download_fs=False):\n",
    "        '''\n",
    "        Initialize the FastSpeech 1.0 model and the WaveGlow model. \n",
    "        force_download_wg: whether to force download the WaveGlow weights in case they already seems to exist \n",
    "        force_download_fs: whether to force download the FastSpeech 1.0 wieghts in case they already seems to exist\n",
    "        '''\n",
    "        # see if there is a WaveGlow folder in the current directory\n",
    "        # print file and folder names in the current directory\n",
    "        curr_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        if not os.path.exists(curr_dir + '/weights.pth') or force_download_fs:\n",
    "            print(\"Weights not found. Downloading FastSpeech 1.0 weights...\")\n",
    "            # if not, download the WaveGlow folder\n",
    "            f_id = '1G630THkg1CaAZiYAK-rcg7hLNIxu2oO5' \n",
    "            gdown.download(f'https://drive.google.com/uc?export=download&confirm=pbef&id={f_id}', curr_dir + '/weights.pth', quiet=False)\n",
    "            print(\"Done.\")              \n",
    "        self.symbols = list('_-!\\'(),.:;? ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz')\n",
    "        self.symbol_to_id = {s: i for i, s in enumerate(self.symbols)}\n",
    "        self.model = nn.DataParallel(FastSpeech()).to(device)\n",
    "        # get directory of the current file\n",
    "        dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "        self.model.load_state_dict(torch.load(dir_path + \"/weights.pth\", map_location=device))\n",
    "        self.model.eval()\n",
    "        self.WaveGlow = waveglow.load.load_model(download=force_download_wg)\n",
    "    \n",
    "    def speak(self, text, play=False, save=False):\n",
    "        '''\n",
    "        Pass given text through the FastSpeech 1.0 model and the WaveGlow model to generate speech.\n",
    "        text: text to be spoken with at most 300 characters\n",
    "        play: whether to play the generated speech\n",
    "        save: whether to save the generated speech\n",
    "        '''\n",
    "        ascii_text = text.lower()\n",
    "        sequence = np.array([self.symbol_to_id[s] for s in ascii_text if s in self.symbol_to_id ])\n",
    "        sequence_inds = np.array([i+1 for w, i in enumerate(sequence)])\n",
    "        \n",
    "        sequence, sequence_inds = sequence[np.newaxis, :], sequence_inds[np.newaxis, :]\n",
    "        \n",
    "        sequence = torch.from_numpy(sequence).long() if device != torch.device('cuda') else torch.from_numpy(sequence).cuda().long()\n",
    "        sequence_inds = torch.from_numpy(sequence_inds).long() if device != torch.device('cuda') else torch.from_numpy(sequence_inds).cuda().long()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mel = self.model.module.forward(sequence, sequence_inds)\n",
    "        mel = mel.contiguous().transpose(1, 2)   \n",
    "        \n",
    "        audio = waveglow.inference.get_wav(mel, self.WaveGlow).cpu().numpy()\n",
    "        if save:\n",
    "            # get the directory of the current file\n",
    "            dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "            # save the audio file in the current directory\n",
    "            write(dir_path + \"/sample.wav\", 22050, audio.astype('int16'))\n",
    "            \n",
    "        if play:\n",
    "            write('sample.wav', 22050, audio.astype('int16'))\n",
    "            playsound(\"sample.wav\")\n",
    "            os.remove(\"sample.wav\")\n",
    "        return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook FastSpeech.ipynb to script\n",
      "[NbConvertApp] Writing 28381 bytes to FastSpeech.py\n"
     ]
    }
   ],
   "source": [
    "# if running from notebook\n",
    "if __name__ == '__main__':\n",
    "    !jupyter nbconvert --to script FastSpeech.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
