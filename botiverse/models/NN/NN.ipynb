{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"cyan\">Feedforward Neural Network </font> From Scratch Implementation\n",
    "In this notebook, we will implement a feedforward neural network with arbitrary structure, loss and activations. You can find a full tutorial on this notebook [here](hhttps://medium.com/towards-data-science/backpropagation-the-natural-proof-946c5abf63b1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This module implements and provides an interface for an arbitrary neural network architecture.\n",
    "'''\n",
    "try:\n",
    "    from botiverse.models.NN.utils import split_data, batchify\n",
    "except:\n",
    "    pass\n",
    "# check if running on notebook\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import copy \n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    '''\n",
    "    Defines the hypothesis set and learning algorithm for a neural network.\n",
    "    '''\n",
    "    def __init__(self, structure, activation='sigmoid', optimizer='ADAM'):\n",
    "        '''\n",
    "        Initialize the hyperparameters, weights and biases for the network. \n",
    "        :param structure: A list of integers representing the number of neurons in each layer. For example, [2, 3, 1] is a network with 2 neurons in the input layer, 3 in the hidden layer, and 1 in the output layer.\n",
    "        :param activation: The activation function to use. Can be 'sigmoid' or 'relu'.\n",
    "        :param optimizer: The optimizer to use. Can be 'ADAM' or 'SGD'.\n",
    "        '''\n",
    "        ### Hyperparameters\n",
    "        self.structure = structure\n",
    "        self.num_layers = len(structure) \n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        if activation == 'sigmoid':\n",
    "            def σ(z): return 1.0/(1.0+np.exp(-z))                   # activation function\n",
    "            def σࠤ(z): return σ(z)*(1-σ(z))                         # derivative of the activation function\n",
    "     \n",
    "        elif activation == 'relu':\n",
    "            def σ(z): return np.maximum(0, z)\n",
    "            def σࠤ(z): return np.greater(z, 0).astype(int)\n",
    "        self.h = σ\n",
    "        self.dh = σࠤ   \n",
    "        \n",
    "        def softmax(z): return np.exp(z-np.max(z))/np.sum(np.exp(z-np.max(z)), axis=0)\n",
    "        self.g = softmax\n",
    "        \n",
    "        ### Parameters\n",
    "        # Xaiver initialization\n",
    "        #for each layer except the first y neurons randomize a (y, 1) vector for the bias vector:\n",
    "        self.Bₙ = [np.random.randn(l, 1) * np.sqrt(2/l) for l in structure[1:]]\n",
    "        #for each two consecutive layers with x and y neurons respectively randomize a (y,x) matrix for the weight matrix:\n",
    "        self.Wₙ = [np.random.randn(l, next_l) * np.sqrt(2/l) for l, next_l in zip(structure[:-1], structure[1:])]\n",
    "        \n",
    "        ### Loss\n",
    "        def cross_entropy(aᴺ, y): return -np.sum(y * np.log(aᴺ)) / aᴺ.shape[1]\n",
    "        def cross_entropyࠤ(aᴺ, y): return (aᴺ-y)\n",
    "        self.J = cross_entropy\n",
    "        self.ᐁJ = cross_entropyࠤ\n",
    "\n",
    "        ### Misc\n",
    "        self.optimal_epochs = None\n",
    "NNClass = lambda func: setattr(NeuralNet, func.__name__, func) or func"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@NNClass\n",
    "def backprop(self, xₛ , yₛ ):\n",
    "    '''\n",
    "    Compute the loss gradients მJⳆმBₙₛ, მJⳆმWₙₛ given an observation (xₛ , yₛ ) where xₛ and yₛ are column vectors. \n",
    "    :param xₛ: The input vector of shape (d, 1).\n",
    "    :param yₛ: The output vector of shape (K, 1).\n",
    "    :return: A tuple of lists (მJⳆმBₙₛ, მJⳆმWₙₛ) where მJⳆმBₙₛ is a list of the gradients of the loss function with respect to the biases and მJⳆმWₙₛ is a list of the gradients of the loss function with respect to the weights.\n",
    "    '''\n",
    "    h, dh, ᐁJ, g = self.h, self.dh, self.ᐁJ, self.g\n",
    "\n",
    "    მJⳆმBₙₛ = [np.zeros(b.shape) for b in self.Bₙ]\n",
    "    მJⳆმWₙₛ = [np.zeros(W.shape) for W in self.Wₙ]\n",
    "\n",
    "    # forward pass (computing z for all layers)\n",
    "    Zₙ = []                     # list to store all the z vectors, layer by layer\n",
    "    Aₙ = []                     # list to store all the a vectors layer by layer\n",
    "\n",
    "    for i, (b, W) in enumerate(zip(self.Bₙ, self.Wₙ)):\n",
    "        z = W.T @ a + b if Zₙ else W.T @ xₛ  + b\n",
    "        a = h(z) if i != self.num_layers-2 else g(z)\n",
    "        Zₙ.append(z)\n",
    "        Aₙ.append(a)\n",
    "\n",
    "    #Zₙ and Aₙ are now ready.\n",
    "\n",
    "    # backward pass (computing δ and consequently მJⳆმBₙₛ and მJⳆმWₙₛ layer by layer )\n",
    "    H = self.num_layers-2\n",
    "    for L in range(H, -1, -1):\n",
    "        δ =  dh(Zₙ[L]) * (self.Wₙ[L+1] @ δ) if L != H else ᐁJ(Aₙ[L], yₛ ) \n",
    "        მJⳆმBₙₛ[L] = δ\n",
    "        მJⳆმWₙₛ[L] = Aₙ[L-1] @ δ.T  if L != 0 else xₛ  @ δ.T\n",
    "    \n",
    "    return (მJⳆმBₙₛ, მJⳆმWₙₛ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@NNClass\n",
    "def SGD(self, x_batch, y_batch, λ, α=0.01):\n",
    "    '''\n",
    "    Given a minibatch (a list/numpy array of tuples (xₛ, yₛ )) this will update Bₙ and Wₙ by applying SGD with L2 regularization.\n",
    "    :param x_batch: A list/numpy array of input vectors of shape (d, 1).\n",
    "    :param y_batch: A list/numpy array of output vectors of shape (K, 1).\n",
    "    :param λ: The learning rate.\n",
    "    :param α: The regularization parameter.\n",
    "    :return: None\n",
    "    '''\n",
    "    მJⳆმBₙ = [np.zeros(b.shape) for b in self.Bₙ]\n",
    "    მJⳆმWₙ = [np.zeros(W.shape) for W in self.Wₙ]\n",
    "\n",
    "    for x, y in zip(x_batch, y_batch):\n",
    "        მJⳆმBₙₛ, მJⳆმWₙₛ = self.backprop(x, y)\n",
    "        მJⳆმBₙ = [მJⳆმb + მJⳆმbₛ for მJⳆმb, მJⳆმbₛ in zip(მJⳆმBₙ, მJⳆმBₙₛ)]  \n",
    "        მJⳆმWₙ = [მJⳆმW + მJⳆმWₛ for მJⳆმW, მJⳆმWₛ in zip(მJⳆმWₙ, მJⳆმWₙₛ)]\n",
    "\n",
    "    d = len(x_batch)\n",
    "    self.Wₙ = [(1 - λ * α / d) * W - λ / d * მJⳆმW / np.linalg.norm(მJⳆმW) for W, მJⳆმW in zip(self.Wₙ, მJⳆმWₙ)]\n",
    "    self.Bₙ = [(1 - λ * α / d) * b - λ / d * მJⳆმb / np.linalg.norm(მJⳆმb) for b, მJⳆმb in zip(self.Bₙ, მJⳆმBₙ)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@NNClass\n",
    "def ADAM(self, x_batch, y_batch, λ, α=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    '''\n",
    "    Given a minibatch (a list of tuples (xₛ, yₛ )) this will update Bₙ and Wₙ by applying Adam optimizer with L2 regularization.\n",
    "    :param x_batch: A list/numpy array of input vectors of shape (d, 1).\n",
    "    :param y_batch: A list/numpy array of output vectors of shape (K, 1).\n",
    "    :param λ: The learning rate.\n",
    "    :param α: The regularization parameter.\n",
    "    :param beta1: The exponential decay rate for the first moment estimates.\n",
    "    :param beta2: The exponential decay rate for the second-moment estimates.\n",
    "    :param epsilon: A small constant for numerical stability.\n",
    "    '''\n",
    "    mB = [np.zeros(b.shape) for b in self.Bₙ]           # momentum for Bₙ\n",
    "    vB = [np.zeros(b.shape) for b in self.Bₙ]           # RMSprop for Bₙ\n",
    "    mW = [np.zeros(W.shape) for W in self.Wₙ]           # momentum for Wₙ\n",
    "    vW = [np.zeros(W.shape) for W in self.Wₙ]           # RMSprop for Wₙ\n",
    "\n",
    "    for x, y in zip(x_batch, y_batch):\n",
    "        mJdB, mJdW = self.backprop(x, y)\n",
    "        # update the momentum and RMSprop for Wₙ and Bₙ\n",
    "        mB = [beta1 * mb + (1 - beta1) * mJdb for mb, mJdb in zip(mB, mJdB)]\n",
    "        vB = [beta2 * vb + (1 - beta2) * (mJdb ** 2) for vb, mJdb in zip(vB, mJdB)]\n",
    "        mW = [beta1 * mw + (1 - beta1) * mJdw for mw, mJdw in zip(mW, mJdW)]\n",
    "        vW = [beta2 * vw + (1 - beta2) * (mJdw ** 2) for vw, mJdw in zip(vW, mJdW)]\n",
    "\n",
    "    # update the parameters Wₙ and Bₙ\n",
    "    d = len(x_batch)\n",
    "    self.Wₙ = [(1 - α * λ / d) * W - λ * mb / (np.sqrt(vb) + epsilon) for W, mb, vb in zip(self.Wₙ, mW, vW)]\n",
    "    self.Bₙ = [(1 - α * λ / d) * b - λ * mb / (np.sqrt(vb) + epsilon) for b, mb, vb in zip(self.Bₙ, mB, vB)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@NNClass\n",
    "def feedforward(self, x ):\n",
    "    h, g = self.h, self.g\n",
    "    '''\n",
    "    The forward pass of the network. Given an input x this will return the output of the network.\n",
    "    :param x: The input vector of shape (d, 1) where d is the number of input features.\n",
    "    :return: The output vector of shape (K, 1) where K is the number of classes.\n",
    "    '''\n",
    "    a = x\n",
    "    for i, (b, W) in enumerate(zip(self.Bₙ, self.Wₙ)):\n",
    "        z = W.T @ a + b\n",
    "        a = h(z) if i != self.num_layers-2 else g(z)\n",
    "    ŷ = a\n",
    "    return ŷ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@NNClass\n",
    "def fit(self, X, y, batch_size=32, epochs=200, λ=30, α=0.01, optimizer='ADAM', val_split=0.0, patience=50, eval_train=False):\n",
    "    '''\n",
    "    For each epoch, go over each minibatch and perform a gradient descent update accordingly and evaluate the model on the training and validation sets if needed.\n",
    "    :param X: The training data arranged as a float numpy array of shape (N, d)\n",
    "    :param y: The training labels arranged as a numpy array of shape (N,) where each element is an integer between 0 and k-1\n",
    "    :param batch_size: The size of the minibatches to use.\n",
    "    :param epochs: The number of epochs to run.\n",
    "    :param λ: The learning rate.\n",
    "    :param α: The regularization parameter.\n",
    "    :param optimizer: The optimizer to use. Can be 'ADAM' or 'SGD'.\n",
    "    :param val_split: The ratio of the validation set to the training set. If given, per-epoch validation will be performed.\n",
    "    :param eval_train: Whether to evaluate the model on the training set.\n",
    "    :return: None\n",
    "    '''\n",
    "    if val_split:\n",
    "        X, y, X_v, y_v = split_data(X, y, val_split)\n",
    "    \n",
    "    Xc, yc = copy.deepcopy(X), copy.deepcopy(y)\n",
    "    X, y = batchify(X, y, batch_size)\n",
    "\n",
    "    if self.optimal_epochs is not None:\n",
    "        print(f\"Using optimal epochs: {self.optimal_epochs} found from an earlier run.\")\n",
    "        epochs = self.optimal_epochs\n",
    "        \n",
    "    # training loop\n",
    "    self.gradient_descent = self.SGD if optimizer == 'SGD' else self.ADAM\n",
    "    train_acc, val_acc = '', ''\n",
    "    pbar = tqdm(range(epochs))\n",
    "    best_loss = np.inf\n",
    "    val_loss = 0\n",
    "    bad_epochs = 0\n",
    "    for epoch in pbar:\n",
    "        # randomly shuffle X, y\n",
    "        p = np.random.permutation(len(X))\n",
    "        X, y = X[p], y[p]\n",
    "        for x_batch, y_batch in zip(X, y):\n",
    "            self.gradient_descent(x_batch, y_batch, λ)          #update the parameters after learning from the mini_batch.\n",
    "        if eval_train:    \n",
    "            train_acc = self.evaluate(Xc, yc)\n",
    "    \n",
    "        # update bar\n",
    "        desc1, desc2 = f\"Train Acc: {train_acc}\" if eval_train else '', f\" | Val Acc: {val_acc}\" if val_split else ''\n",
    "        desc3 = f\" | Val Loss: {val_loss}\" if val_split else ''\n",
    "        desc =  desc1 + desc2 + desc3\n",
    "        pbar.set_description(desc)\n",
    "        \n",
    "        # early stopping\n",
    "        if val_split:     \n",
    "            val_loss = self.evaluate(X_v, y_v, loss=True)\n",
    "            if val_loss <= best_loss:\n",
    "                best_loss = val_loss\n",
    "                bad_epochs = 0\n",
    "            else:\n",
    "                bad_epochs += 1\n",
    "                if bad_epochs == patience:\n",
    "                    print(f\"{patience} epochs have passed without improvement. Early stopping... \\n\")\n",
    "                    self.optimal_epochs = epoch - patience + 2\n",
    "                    self.optimal_epochs = int(self.optimal_epochs * (1 + val_split))\n",
    "                    break\n",
    "            val_acc = self.evaluate(X_v, y_v)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@NNClass\n",
    "def predict(self, X):\n",
    "    '''\n",
    "    Predict the class of each example in X.\n",
    "    :param X: The test data arranged as a float numpy array of shape (N, d)\n",
    "    '''\n",
    "    if len(X.shape) == 2:   X = X[..., np.newaxis]\n",
    "    # return the predicted class and the probability of the predicted class\n",
    "    return np.array([np.argmax(self.feedforward(x)) for x in X]), np.array([np.max(self.feedforward(x)) for x in X])\n",
    "        \n",
    "@NNClass\n",
    "def evaluate(self, X,y, loss=False):\n",
    "    '''\n",
    "    Compare the one-hot vector y with the networks output yᴺ and calculate the accuracy.\n",
    "    :param X: The test data arranged as a float numpy array of shape (N, d)\n",
    "    :param y: The test labels arranged as a numpy array of shape (N,) where each element is an integer between 0 and k-1\n",
    "    '''\n",
    "    if len(X.shape) == 2:   X = X[..., np.newaxis]\n",
    "    if loss:\n",
    "        return np.sum([self.J(self.feedforward(x), y) for x, y in zip(X, y)]) / len(X)\n",
    "    else:\n",
    "        validation_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in zip(X, y)]   #the index is the number itself\n",
    "        accuracy = sum(int(ŷ == y) for (ŷ, y) in validation_results) / len(X)\n",
    "    \n",
    "    return round(accuracy, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@NNClass\n",
    "def save(self, path):\n",
    "    '''\n",
    "    Save the model to the given path.\n",
    "    :param path: The path to save the model to.\n",
    "    '''\n",
    "    # store self.Bₙ, self.Wₙ, self.structure, self.activation, self.optimizer\n",
    "    with open(path, 'wb') as f:\n",
    "        # save self.Bₙ, self.Wₙ, self.structure, self.activation, self.optimizer\n",
    "        class_dict = {key: value for key, value in self.__dict__.items() if key in ['Bn', 'Wn', 'structure', 'activation', 'optimizer']}\n",
    "        f.write(pickle.dumps(class_dict))\n",
    "\n",
    "@staticmethod        \n",
    "@NNClass\n",
    "def load(path):\n",
    "    '''\n",
    "    Load the model from the given path.\n",
    "    :param path: The path to load the model from.\n",
    "    '''\n",
    "    with open(path, 'rb') as f:\n",
    "        class_dict = pickle.loads(f.read())\n",
    "        model = NeuralNet(class_dict['structure'], class_dict['activation'], class_dict['optimizer'])\n",
    "        # now use setattr to set Bₙ and Wₙ\n",
    "        for key, value in class_dict.items():\n",
    "            if key in ['Bn', 'Wn']:\n",
    "                setattr(model, key, value)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook NN.ipynb to script\n",
      "[NbConvertApp] Writing 12992 bytes to NN.py\n"
     ]
    }
   ],
   "source": [
    "# if running from notebook\n",
    "if __name__ == '__main__':\n",
    "    !jupyter nbconvert --to script NN.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
