{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uqSL1SM8jTmN",
        "lUwJn3GGkAeJ",
        "xL8SWbdag55U",
        "OGMX4sV0hPm2",
        "OHprhkophXcd",
        "jWPsHYnQhZeC",
        "y0Gj-tIcixed",
        "s1wW97HBi_SR",
        "eZeVgmAYjA1T"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "uqSL1SM8jTmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3l0Slr5jXyL",
        "outputId": "0d73443d-c23d-49c4-a6ad-9f6f03c9ab2f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount"
      ],
      "metadata": {
        "id": "lUwJn3GGkAeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/TODS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeLgf2jlkDNP",
        "outputId": "0f1721c6-67ac-4c4b-81b6-d3cb58911c4a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/TODS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "xL8SWbdag55U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert"
      ],
      "metadata": {
        "id": "OGMX4sV0hPm2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9-LNFWoDg0-F"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Theis Module contains the BERT model architecture.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "# Embeddings\n",
        "# 1. Cluster similar words together.\n",
        "# 2. Preserve different relationships between words such as: semantic, syntactic, linear,\n",
        "# and since BERT is bidirectional it will also preserve contextual relationships as well.\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Embedding layer for BERT.\n",
        "\n",
        "    This layer takes input_ids and token_type_ids as inputs and generates word embeddings\n",
        "    using three types of embeddings: word, position, and token_type embeddings.\n",
        "\n",
        "    :param config: BERT configuration.\n",
        "    :type config: Config\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(Embeddings, self).__init__()\n",
        "        # Bert uses 3 types of embeddings: word, position, and token_type (segment type).\n",
        "        # LayerNorm is used to normalize the sum of the embeddings.\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.padding_idx)\n",
        "        self.position_embeddings = nn.Embedding(config.max_seq, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.token_types, config.hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids): # input_ids: [batch_size, seq_len] token_type_ids: [batch_size, seq_len]\n",
        "        \"\"\"\n",
        "        Forward pass of the Embeddings layer.\n",
        "\n",
        "        :param input_ids: The input token IDs.\n",
        "        :type input_ids: torch.Tensor\n",
        "        :param token_type_ids: The token type IDs.\n",
        "        :type token_type_ids: torch.Tensor\n",
        "        :return: The generated embeddings.\n",
        "        :rtype: torch.Tensor\n",
        "        \"\"\"\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        seq_len = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_len).unsqueeze(0).expand_as(input_ids).to(device) # position_ids: [batch_size, seq_len]\n",
        "        word_embeddings = self.word_embeddings(input_ids)  # word_embeddings: [batch_size, seq_len, hidden_size]\n",
        "        position_embeddings = self.position_embeddings(position_ids) # position_embeddings: [batch_size, seq_len, hidden_size]\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids) # token_type_embeddings: [batch_size, seq_len, hidden_size]\n",
        "        embeddings = word_embeddings + position_embeddings + token_type_embeddings # embeddings: [batch_size, seq_len, hidden_size]\n",
        "        # Normalize by subtracting the mean and dividing by the standard deviation calculated across the feature dimension\n",
        "        # then multiply by a learned gain parameter and add to a learned bias parameter.\n",
        "        embeddings = self.layer_norm(embeddings) # embeddings: [batch_size, seq_len, hidden_size]\n",
        "        embeddings = self.dropout(embeddings) # embeddings: [batch_size, seq_len, hidden_size]\n",
        "        return embeddings\n",
        "\n",
        "# Encoder layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder layer for BERT.\n",
        "\n",
        "    This layer contains self-attention, layer normalization, and position-wise feed-forward network.\n",
        "\n",
        "    :param config: BERT configuration.\n",
        "    :type config: Config\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(config)\n",
        "        self.self_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.self_dropout = nn.Dropout(config.dropout)\n",
        "        self.position_wise_feed_forward = PositionWiseFeedForward(config)\n",
        "        self.ffn_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.ffn_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, input, attention_mask):\n",
        "        \"\"\"\n",
        "        Forward pass of the EncoderLayer.\n",
        "\n",
        "        :param input: The input tensor.\n",
        "        :type input: torch.Tensor\n",
        "        :param attention_mask: The attention mask.\n",
        "        :type attention_mask: torch.Tensor\n",
        "        :return: The output tensor.\n",
        "        :rtype: torch.Tensor\n",
        "        \"\"\"\n",
        "\n",
        "        # Multi-head attention\n",
        "        context, attention = self.self_attention(input, input, input, attention_mask) # context: [batch_size, seq_len, hidden_size] attention: [batch_size, heads, seq_len, seq_len]\n",
        "        # Add and normalize\n",
        "        context = self.self_dropout(context) # context: [batch_size, seq_len, hidden_size]\n",
        "        output = self.self_layer_norm(input + context) # output: [batch_size, seq_len, hidden_size]\n",
        "        # Position-wise feed-forward network\n",
        "        context = self.position_wise_feed_forward(output) # context: [batch_size, seq_len, hidden_size]\n",
        "        # Add and normalize\n",
        "        context = self.ffn_dropout(context) # context: [batch_size, seq_len, hidden_size]\n",
        "        output = self.ffn_layer_norm(output + context) # output: [batch_size, seq_len, hidden_size]\n",
        "        return output, attention\n",
        "\n",
        "# Multi-head attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention layer for BERT.\n",
        "\n",
        "    This layer performs multi-head self-attention and returns the output context.\n",
        "\n",
        "    :param config: BERT configuration.\n",
        "    :type config: Config\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.config = config\n",
        "        self.w_q = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.w_k = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.w_v = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.w_o = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, query, key, value, attention_mask):\n",
        "        \"\"\"\n",
        "        Forward pass of the MultiHeadAttention.\n",
        "\n",
        "        :param query: The query tensor.\n",
        "        :type query: torch.Tensor\n",
        "        :param key: The key tensor.\n",
        "        :type key: torch.Tensor\n",
        "        :param value: The value tensor.\n",
        "        :type value: torch.Tensor\n",
        "        :param attention_mask: The attention mask.\n",
        "        :type attention_mask: torch.Tensor\n",
        "        :return: The output context.\n",
        "        :rtype: torch.Tensor\n",
        "        \"\"\"\n",
        "\n",
        "        # query: [batch_size, seq_len, hidden_size] key: [batch_size, seq_len, hidden_size]\n",
        "        # value: [batch_size, seq_len, hidden_size] attention_mask: [batch_size, seq_len_q, seq_len_k]\n",
        "\n",
        "        batch_size, seq_len, hidden_size = query.size()\n",
        "\n",
        "        query = self.w_q(query).view(batch_size, seq_len, self.config.heads, hidden_size // self.config.heads).transpose(1, 2) # query: [batch_size, heads, seq_len, hidden_size // heads]\n",
        "        key = self.w_k(key).view(batch_size, seq_len, self.config.heads, hidden_size // self.config.heads).transpose(1, 2) # key: [batch_size, heads, seq_len, hidden_size // heads]\n",
        "        value = self.w_v(value).view(batch_size, seq_len, self.config.heads, hidden_size // self.config.heads).transpose(1, 2) # value: [batch_size, heads, seq_len, hidden_size // heads]\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attention = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(float(hidden_size // self.config.heads))) # attention: [batch_size, heads, seq_len, seq_len]\n",
        "        attention_mask = attention_mask.unsqueeze(1).repeat(1, self.config.heads, 1, 1) # attention_mask: [batch_size, heads, seq_len_q, seq_len_k]\n",
        "        attention_mask = (attention_mask == 0)\n",
        "        attention.masked_fill_(attention_mask, -1e9) # attention: [batch_size, heads, seq_len, seq_len]\n",
        "        attention = self.softmax(attention) # attention: [batch_size, heads, seq_len, seq_len]\n",
        "        attention = self.dropout(attention) # attention: [batch_size, heads, seq_len, seq_len]\n",
        "        context = torch.matmul(attention, value) # context: [batch_size, heads, seq_len, hidden_size // heads]\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_size) # context: [batch_size, seq_len, hidden_size]\n",
        "        output = self.w_o(context) # output: [batch_size, seq_len, hidden_size]\n",
        "        return output, attention\n",
        "\n",
        "# Position-wise feed-forward network\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Position-wise feed-forward network layer for BERT.\n",
        "\n",
        "    This layer applies two linear transformations with a GELU activation function.\n",
        "\n",
        "    :param config: BERT configuration.\n",
        "    :type config: Config\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(config.hidden_size, config.ff_size)\n",
        "        self.linear2 = nn.Linear(config.ff_size, config.hidden_size)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Forward pass of the PositionWiseFeedForward layer.\n",
        "\n",
        "        :param input: The input tensor.\n",
        "        :type input: torch.Tensor\n",
        "        :return: The output tensor.\n",
        "        :rtype: torch.Tensor\n",
        "        \"\"\"\n",
        "        output = self.linear1(input) # output: [batch_size, seq_len, ff_size]\n",
        "        output = self.gelu(output) # output: [batch_size, seq_len, ff_size]\n",
        "        output = self.linear2(output) # output: [batch_size, seq_len, hidden_size]\n",
        "        return output\n",
        "\n",
        "# Bert\n",
        "# 1. Puts it all together.\n",
        "class Bert(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT model implementation.\n",
        "\n",
        "    This model combines the Embeddings layer, EncoderLayers, and linear transformation layers\n",
        "    to perform BERT-based processing.\n",
        "\n",
        "    :param config: BERT configuration.\n",
        "    :type config: Config\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(Bert, self).__init__()\n",
        "        self.embeddings = Embeddings(config)\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "        self.linear = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attention_mask, return_dict=False): # input_ids: [batch_size, seq_len] token_type_ids: [batch_size, seq_len] attention_mask: [batch_size, seq_len]\n",
        "        \"\"\"\n",
        "        Forward pass of the Bert model.\n",
        "\n",
        "        :param input_ids: The input token IDs.\n",
        "        :type input_ids: torch.Tensor\n",
        "        :param token_type_ids: The token type IDs.\n",
        "        :type token_type_ids: torch.Tensor\n",
        "        :param attention_mask: The attention mask.\n",
        "        :type attention_mask: torch.Tensor\n",
        "        :param return_dict: Whether to return a dictionary or not, defaults to False.\n",
        "        :type return_dict: bool\n",
        "        :return: The sequence output and pooled output.\n",
        "        :rtype: torch.Tensor, torch.Tensor\n",
        "        \"\"\"\n",
        "\n",
        "        # Embedding\n",
        "        output = self.embeddings(input_ids, token_type_ids) # output: [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        # Encoder\n",
        "        attention_mask = attention_mask.unsqueeze(1).repeat(1, output.size(1), 1) # attention_mask: [batch_size, seq_len, seq_len]\n",
        "        for encoder_layer in self.encoder:\n",
        "            output, attention = encoder_layer(output, attention_mask) # output: [batch_size, seq_len, hidden_size] attention: [batch_size, heads, seq_len, seq_len]\n",
        "\n",
        "        # Sequnce and pooled outputs\n",
        "        sequence_output = output # sequence_output: [batch_size, seq_len, hidden_size]\n",
        "        pooled_output = self.tanh(self.linear(sequence_output[:, 0])) # pooled_output: [batch_size, hidden_size]\n",
        "\n",
        "        return sequence_output, pooled_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## config"
      ],
      "metadata": {
        "id": "OHprhkophXcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This module contains the configuration class for BERT.\n",
        "\"\"\"\n",
        "\n",
        "# Bert configuration\n",
        "class BERTConfig(object):\n",
        "    \"\"\"\n",
        "    Configuration class for BERT.\n",
        "\n",
        "    This class holds the configuration parameters for the BERT model.\n",
        "\n",
        "    :param vocab_size: The size of the vocabulary, defaults to 30522.\n",
        "    :type vocab_size: int\n",
        "    :param hidden_size: The hidden size of the BERT model, defaults to 768.\n",
        "    :type hidden_size: int\n",
        "    :param encoder_layers: The number of encoder layers in the BERT model, defaults to 12.\n",
        "    :type encoder_layers: int\n",
        "    :param heads: The number of attention heads in the BERT model, defaults to 12.\n",
        "    :type heads: int\n",
        "    :param ff_size: The size of the feed-forward layer in the BERT model, defaults to 3072.\n",
        "    :type ff_size: int\n",
        "    :param token_types: The number of token types in the BERT model, defaults to 2.\n",
        "    :type token_types: int\n",
        "    :param max_seq: The maximum sequence length in the BERT model, defaults to 512.\n",
        "    :type max_seq: int\n",
        "    :param padding_idx: The padding index used in the BERT model, defaults to 0.\n",
        "    :type padding_idx: int\n",
        "    :param layer_norm_eps: The epsilon value for layer normalization in the BERT model, defaults to 1e-12.\n",
        "    :type layer_norm_eps: float\n",
        "    :param dropout: The dropout rate in the BERT model, defaults to 0.1.\n",
        "    :type dropout: float\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=30522, hidden_size=768, encoder_layers=12, heads=12, ff_size=3072, token_types=2, max_seq=512, padding_idx=0, layer_norm_eps=1e-12, dropout=0.1):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.encoder_layers = encoder_layers\n",
        "        self.heads = heads\n",
        "        self.ff_size = ff_size\n",
        "        self.token_types = token_types\n",
        "        self.max_seq = max_seq\n",
        "        self.padding_idx = padding_idx\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.dropout = dropout"
      ],
      "metadata": {
        "id": "GvFu9YgQhZAf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils"
      ],
      "metadata": {
        "id": "jWPsHYnQhZeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This module contains utility functions for the BERT model.\n",
        "\"\"\"\n",
        "\n",
        "from collections import OrderedDict\n",
        "from transformers import BertModel\n",
        "# from botiverse.models.BERT.config import BERTConfig\n",
        "# from botiverse.models.BERT.BERT import Bert\n",
        "\n",
        "\n",
        "# Load pre-trained weights from trnasformers library\n",
        "def LoadPretrainedWeights(model):\n",
        "    \"\"\"\n",
        "    Load pre-trained weights from the transformers library.\n",
        "\n",
        "    This function loads the pre-trained weights from the transformers library\n",
        "    and updates the model's state_dict accordingly.\n",
        "\n",
        "    :param model: The BERT model.\n",
        "    :type model: Bert\n",
        "    \"\"\"\n",
        "\n",
        "    # Get pre-trained weights from transformers library\n",
        "    pretrained_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    state_dict = pretrained_model.state_dict()\n",
        "\n",
        "    # Delete position_ids from the state_dict if available\n",
        "    if 'embeddings.position_ids' in state_dict.keys():\n",
        "        del state_dict['embeddings.position_ids']\n",
        "\n",
        "    # Get the new weights keys from the model\n",
        "    new_keys = list(model.state_dict().keys())\n",
        "\n",
        "    # Get the weights from the state_dict\n",
        "    old_keys = list(state_dict.keys())\n",
        "    weights = list(state_dict.values())\n",
        "\n",
        "    # Create a new state_dict with the new keys\n",
        "    new_state_dict = OrderedDict()\n",
        "    for i in range(len(new_keys)):\n",
        "        new_state_dict[new_keys[i]] = weights[i]\n",
        "        # print(old_keys[i], '->', new_keys[i])\n",
        "\n",
        "    model.load_state_dict(new_state_dict)\n",
        "\n",
        "# Example comparing the outputs of the from scratch model to the pre-trained model from transformers library\n",
        "import torch\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "def Example():\n",
        "    \"\"\"\n",
        "    Example comparing the outputs of the from scratch model to the pre-trained model from transformers library.\n",
        "    \"\"\"\n",
        "\n",
        "    # Build a BERT model from scratch\n",
        "    config = BERTConfig()\n",
        "    model = Bert(config)\n",
        "    LoadPretrainedWeights(model)\n",
        "\n",
        "    # Load pre-trained weights from the Transformers library\n",
        "    pretrained_weights = 'bert-base-uncased'\n",
        "    pretrained_model = BertModel.from_pretrained(pretrained_weights)\n",
        "\n",
        "    # Tokenize the input sequence\n",
        "    tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
        "    input_text = [\"This is a sample input sequence.\", \"batyousef is awesome\"]\n",
        "    inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "    # Set dropout to zero during inference\n",
        "    model.eval()\n",
        "    pretrained_model.eval()\n",
        "\n",
        "    ids = inputs['input_ids']\n",
        "    token_type_ids = inputs['token_type_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    # Pass the inputs through both models and compare the outputs\n",
        "    with torch.no_grad():\n",
        "        model_output1, model_output2 = model(ids, token_type_ids, attention_mask)\n",
        "        pretrained_output1, pretrained_output2 = pretrained_model(ids,\n",
        "                                            attention_mask=attention_mask,\n",
        "                                            token_type_ids=token_type_ids,\n",
        "                                            return_dict=False\n",
        "                                            )\n",
        "\n",
        "    print(model_output1.size(), model_output1)\n",
        "    print(pretrained_output1.size(), pretrained_output1)\n",
        "    print()\n",
        "    print()\n",
        "    print(model_output2.size(), model_output2)\n",
        "    print(pretrained_output2.size(), pretrained_output2)\n",
        "\n",
        "    print(model)\n",
        "    print(pretrained_model)"
      ],
      "metadata": {
        "id": "C40ZzSwQhayr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRIPPY"
      ],
      "metadata": {
        "id": "VhHBM1oMhil3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## config"
      ],
      "metadata": {
        "id": "x3wTMb1Ghwuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This Module has the configuration class for TRIPPY.\n",
        "\"\"\"\n",
        "\n",
        "import tokenizers\n",
        "import os\n",
        "\n",
        "# Trippy configuration\n",
        "class TRIPPYConfig(object):\n",
        "    \"\"\"\n",
        "    Configuration class for TRIPPY.\n",
        "\n",
        "    This class holds the configuration parameters for the TRIPPY model.\n",
        "\n",
        "    :param max_len: The maximum sequence length, defaults to 128.\n",
        "    :type max_len: int\n",
        "    :param train_batch_size: The batch size for training, defaults to 32.\n",
        "    :type train_batch_size: int\n",
        "    :param dev_batch_size: The batch size for development evaluation, defaults to 1.\n",
        "    :type dev_batch_size: int\n",
        "    :param test_batch_size: The batch size for testing, defaults to 1.\n",
        "    :type test_batch_size: int\n",
        "    :param epochs: The number of training epochs, defaults to 15.\n",
        "    :type epochs: int\n",
        "    :param hid_dim: The hidden dimension size, defaults to 768.\n",
        "    :type hid_dim: int\n",
        "    :param n_oper: The number of operations, defaults to 7.\n",
        "    :type n_oper: int\n",
        "    :param dropout: The dropout rate, defaults to 0.3.\n",
        "    :type dropout: float\n",
        "    :param vocab_path: The path to the vocabulary file, defaults to 'vocab.txt'.\n",
        "    :type vocab_path: str\n",
        "    :param ignore_idx: The index value to ignore, defaults to -100.\n",
        "    :type ignore_idx: int\n",
        "    :param oper2id: The mapping of operation names to IDs, defaults to {'carryover' : 0, 'dontcare': 1, 'update':2, 'refer':3, 'yes':4, 'no':5, 'inform':6}.\n",
        "    :type oper2id: dict[str, int]\n",
        "    :param weight_decay: The weight decay value, defaults to 0.0.\n",
        "    :type weight_decay: float\n",
        "    :param lr: The learning rate, defaults to 1e-4.\n",
        "    :type lr: float\n",
        "    :param adam_epsilon: The epsilon value for Adam optimizer, defaults to 1e-6.\n",
        "    :type adam_epsilon: float\n",
        "    :param warmup_proportion: The proportion of warmup steps, defaults to 0.1.\n",
        "    :type warmup_proportion: float\n",
        "    :param multiwoz: The path to the MultiWOZ dataset, defaults to False.\n",
        "    :type multiwoz: str\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 max_len=128,\n",
        "                 train_batch_size=32,\n",
        "                 dev_batch_size=1,\n",
        "                 test_batch_size=1,\n",
        "                 epochs=15,\n",
        "                 hid_dim=768,\n",
        "                 n_oper=7,\n",
        "                 dropout=0.3,\n",
        "                 vocab_path='vocab.txt',\n",
        "                 ignore_idx=-100,\n",
        "                 oper2id={'carryover' : 0, 'dontcare': 1, 'update':2, 'refer':3, 'yes':4, 'no':5, 'inform':6},\n",
        "                 weight_decay=0.0,\n",
        "                 lr=1e-4,\n",
        "                 adam_epsilon=1e-6,\n",
        "                 warmup_proportion=0.1,\n",
        "                 multiwoz=False):\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.dev_batch_size = dev_batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.epochs = epochs\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_oper = n_oper\n",
        "        self.dropout = dropout\n",
        "        # cur_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "        cur_dir = ''\n",
        "        self.vocab_path = os.path.join(cur_dir, vocab_path)\n",
        "        self.tokenizer = tokenizers.BertWordPieceTokenizer(self.vocab_path, lowercase=True)\n",
        "        self.ignore_idx = ignore_idx\n",
        "        self.oper2id = oper2id\n",
        "        self.weight_decay = weight_decay\n",
        "        self.lr = lr\n",
        "        self.adam_epsilon = adam_epsilon\n",
        "        self.warmup_proportion = warmup_proportion\n",
        "        self.multiwoz = multiwoz"
      ],
      "metadata": {
        "id": "BNbHE-9FhyDU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data"
      ],
      "metadata": {
        "id": "ldMtJH_Fhyat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This Module contains the data processing functions for TRIPPY.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from botiverse.models.TRIPPY.utils import RawDataInstance, DataInstance, normalize, is_included, included_with_label_maps, match_with_label_maps, mask_utterance\n",
        "\n",
        "def get_ontology_label_maps(ontology_path, label_maps_path, domains):\n",
        "  \"\"\"\n",
        "  Read ontology and label maps, and filter slots based on domains.\n",
        "\n",
        "  :param ontology_path: The path to the ontology file.\n",
        "  :type ontology_path: str\n",
        "\n",
        "  :param label_maps_path: The path to the label maps file.\n",
        "  :type label_maps_path: str\n",
        "\n",
        "  :param domains: The list of domains to filter the slots.\n",
        "  :type domains: list[str]\n",
        "\n",
        "  :return: The sorted slot list and label maps.\n",
        "  :rtype: tuple[list[str], dict]\n",
        "  \"\"\"\n",
        "\n",
        "  # read ontology\n",
        "  file = open(ontology_path)\n",
        "  slot_list = json.load(file)\n",
        "\n",
        "  # delete slots not in the domains\n",
        "  del_slot = []\n",
        "  for slot in slot_list:\n",
        "    found = False\n",
        "    for domain in domains:\n",
        "      if domain in slot:\n",
        "        found = True\n",
        "    if found == False:\n",
        "      del_slot.append(slot)\n",
        "  for slot in del_slot:\n",
        "    del slot_list[slot_list.index(slot)]\n",
        "\n",
        "  # read label_maps\n",
        "  file = open(label_maps_path)\n",
        "  label_maps = json.load(file)\n",
        "\n",
        "  return sorted(slot_list), label_maps\n",
        "\n",
        "\n",
        "def read_raw_data(data_path, slot_list, max_len, domains, multiwoz):\n",
        "  \"\"\"\n",
        "  Read raw data from the JSON file and preprocess it.\n",
        "\n",
        "  :param data_path: The path to the JSON data file.\n",
        "  :type data_path: str\n",
        "\n",
        "  :param slot_list: The list of slots.\n",
        "  :type slot_list: list[str]\n",
        "\n",
        "  :param max_len: The maximum length of the input sequence.\n",
        "  :type max_len: int\n",
        "\n",
        "  :param domains: The list of domains.\n",
        "  :type domains: list[str]\n",
        "\n",
        "  :return: The list of raw data instances.\n",
        "  :rtype: list[RawDataInstance]\n",
        "  \"\"\"\n",
        "\n",
        "  # read data\n",
        "  file = open(data_path)\n",
        "  parsed_data = json.load(file)\n",
        "\n",
        "  raw_data = []\n",
        "  # loop over dialogues\n",
        "  for dial_info in parsed_data:\n",
        "    dial_idx = dial_info['dialogue_idx']\n",
        "\n",
        "    history = []\n",
        "    # loop over dialogue turns\n",
        "    for turn in dial_info['dialogue']:\n",
        "\n",
        "      # turn id\n",
        "      turn_idx = turn['turn_idx']\n",
        "\n",
        "      # turn utterances\n",
        "      user_utter = turn['user_utterance']\n",
        "      sys_utter = turn['system_utterance']\n",
        "\n",
        "      # normalize utterances\n",
        "      user_utter = ' '.join(normalize(user_utter, multiwoz))\n",
        "      sys_utter = ' '.join(normalize(sys_utter, multiwoz))\n",
        "\n",
        "      # get the changed slots in this turn\n",
        "      turn_slots = turn['turn_slots']\n",
        "\n",
        "      # Get system actions which will be used as the inform memory\n",
        "      inform_mem = turn['system_act']\n",
        "\n",
        "      # mask the system utterance by removing labels appeared in system acts\n",
        "      sys_utter = ' '.join(mask_utterance(sys_utter, inform_mem, multiwoz, '[UNK]'))\n",
        "\n",
        "      # append current instance\n",
        "      raw_data.append(RawDataInstance(dial_idx,\n",
        "                                      turn_idx,\n",
        "                                      user_utter,\n",
        "                                      sys_utter,\n",
        "                                      history,\n",
        "                                      turn_slots,\n",
        "                                      inform_mem))\n",
        "\n",
        "      # update history & last state for next turn\n",
        "      history = [user_utter, sys_utter] + history\n",
        "\n",
        "  return raw_data\n",
        "\n",
        "\n",
        "def create_slot_span(input, target_value, tok_input_offsets, padding_len, label_maps):\n",
        "  \"\"\"\n",
        "  Create a slot span given the input, target value, and token input offsets,\n",
        "  by matching the target value as tokens with the input sequence.\n",
        "\n",
        "  :param input: The input string.\n",
        "  :type input: str\n",
        "\n",
        "  :param target_value: The target value.\n",
        "  :type target_value: str\n",
        "\n",
        "  :param tok_input_offsets: The token input offsets.\n",
        "  :type tok_input_offsets: list[tuple[int, int]]\n",
        "\n",
        "  :param padding_len: The padding length.\n",
        "  :type padding_len: int\n",
        "\n",
        "  :param label_maps: The label maps.\n",
        "  :type label_maps: dict\n",
        "\n",
        "  :return: The slot span, span start index, and span end index.\n",
        "  :rtype: tuple[list[int], int, int]\n",
        "  \"\"\"\n",
        "\n",
        "  # get all possible variants of the slot value\n",
        "  label_variants = [target_value]\n",
        "  if target_value in label_maps:\n",
        "    label_variants = label_variants + label_maps[target_value]\n",
        "\n",
        "  # match the target value as tokens\n",
        "  start, end = -1, -1\n",
        "  found = False\n",
        "  input_list = input.split()\n",
        "  first_idx = input_list.index('[SEP]')\n",
        "  max_idx = first_idx\n",
        "  for label in label_variants:\n",
        "    label_list = [item for item in map(str.strip, re.split(\"(\\W+)\", label)) if len(item) > 0]\n",
        "    if found == True:\n",
        "      break\n",
        "    for idx in (j for j, e in enumerate(input_list) if(e == label_list[0] and j < max_idx)):\n",
        "      if input_list[idx:idx + len(label_list)] == label_list:\n",
        "        start, end = idx, idx + len(label_list) - 1\n",
        "        found = True\n",
        "\n",
        "  # mark the selected part as characters in the input\n",
        "  input = \" \".join(input_list)\n",
        "  ch_start, ch_end = -1, -1\n",
        "  acc_len = 0\n",
        "  for idx, tok in enumerate(input_list):\n",
        "    if start == idx:\n",
        "      ch_start = acc_len + idx\n",
        "    acc_len += len(tok)\n",
        "    if end == idx:\n",
        "      ch_end = acc_len + idx - 1\n",
        "\n",
        "  # mark the target span in the input string\n",
        "  char_target = [0] * len(input)\n",
        "  if ch_start != -1 and ch_end != -1:\n",
        "    for j in range(ch_start, ch_end + 1):\n",
        "      if input[j] != \" \":\n",
        "        char_target[j] = 1\n",
        "\n",
        "  # mark the target span after tokenization\n",
        "  span = [0] * len(tok_input_offsets)\n",
        "  for j, (offset1, offset2) in enumerate(tok_input_offsets):\n",
        "    if sum(char_target[offset1:offset2]) > 0:\n",
        "      span[j] = 1\n",
        "\n",
        "  # update the target as tok_input_offsets doesn not include\n",
        "  # [CLS] & [SEP] in the start & end of input string\n",
        "  span = [0] + span + [0]\n",
        "\n",
        "  # get the start & end index of the span if any\n",
        "  # otherwise 0\n",
        "  span_start = 0\n",
        "  span_end = 0\n",
        "  non_zero = np.nonzero(span)[0]\n",
        "  if len(non_zero) > 0:\n",
        "    span_start = non_zero[0]\n",
        "    span_end = non_zero[-1]\n",
        "\n",
        "  # pad the target span\n",
        "  span = span + [0] * padding_len\n",
        "\n",
        "  return span, span_start, span_end\n",
        "\n",
        "\n",
        "def create_inputs(history, user_utter, sys_utter, tokenizer, max_len):\n",
        "  \"\"\"\n",
        "  Create inputs for BERT using the history, user utterance, system utterance,\n",
        "  by creating and tokenizing the input seqence and creating the masks.\n",
        "\n",
        "  :param history: The history of utterances.\n",
        "  :type history: list[str]\n",
        "\n",
        "  :param user_utter: The user's utterance.\n",
        "  :type user_utter: str\n",
        "\n",
        "  :param sys_utter: The system's utterance.\n",
        "  :type sys_utter: str\n",
        "\n",
        "  :param tokenizer: The tokenizer to tokenize the input.\n",
        "  :type tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "  :param max_len: The maximum length of the input.\n",
        "  :type max_len: int\n",
        "\n",
        "  :return: The input string, token IDs, attention mask, token type IDs,\n",
        "            token input offsets, tokenized input tokens, and padding length.\n",
        "  :rtype: tuple[str, list[int], list[int], list[int], list[tuple[int, int]],\n",
        "                list[str], int]\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # create input string\n",
        "  history = \" \".join(history)\n",
        "  current_utter = user_utter + ' [SEP] ' + sys_utter + ' [SEP] '\n",
        "  input = current_utter + history\n",
        "  input = \" \".join(input.split())\n",
        "\n",
        "  # tokenize and truncate input\n",
        "  tok_input = tokenizer.encode(input)\n",
        "  tok_input_tokens = tok_input.tokens[:max_len]\n",
        "  tok_input_ids = tok_input.ids[:max_len]\n",
        "  tok_input_offsets = tok_input.offsets[1:max_len-1]\n",
        "  if tok_input_tokens[-1] != '[SEP]':\n",
        "    tok_input_tokens[-1] = '[SEP]'\n",
        "    tok_input_ids[-1] = 102\n",
        "\n",
        "  # create mask & input type id\n",
        "  mask = [1] * len(tok_input_ids)\n",
        "  token_type_ids = []\n",
        "  cnt = 0\n",
        "  for i, token in enumerate(tok_input_tokens):\n",
        "    token_type_ids.append(1 if cnt >= 2 else 0)\n",
        "    cnt += 1 if token == '[SEP]' else 0\n",
        "\n",
        "  # pad the inputs\n",
        "  padding_len = max_len - len(tok_input_ids)\n",
        "  ids = tok_input_ids + [0] * padding_len\n",
        "  mask = mask + [0] * padding_len\n",
        "  token_type_ids = token_type_ids + [0] * padding_len\n",
        "\n",
        "  return input, ids, mask, token_type_ids, tok_input_offsets, tok_input_tokens, padding_len\n",
        "\n",
        "\n",
        "def is_informed(value, target, label_maps, multiwoz):\n",
        "  \"\"\"\n",
        "  Check if a value is informed by the system given a target value and label maps.\n",
        "\n",
        "  :param value: The value to check.\n",
        "  :type value: str\n",
        "\n",
        "  :param target: The target value.\n",
        "  :type target: str\n",
        "\n",
        "  :param label_maps: The label maps.\n",
        "  :type label_maps: dict\n",
        "\n",
        "  :return: A tuple indicating if the value is informed and the informed value.\n",
        "  :rtype: tuple[bool, str]\n",
        "  \"\"\"\n",
        "\n",
        "  informed = False\n",
        "  informed_value = 'none'\n",
        "\n",
        "  target = ' '.join(normalize(target, multiwoz))\n",
        "\n",
        "  if value == target or is_included(value, target) or is_included(target, value):\n",
        "    informed = True\n",
        "  if value in label_maps:\n",
        "    informed = included_with_label_maps(target, value, label_maps)\n",
        "  elif target in label_maps:\n",
        "    informed = included_with_label_maps(value, target, label_maps)\n",
        "  if informed: informed_value = value\n",
        "\n",
        "  return informed, informed_value\n",
        "\n",
        "\n",
        "def get_refered_slot(target_value, slot, last_state, non_referable_slots, non_referable_pairs, label_maps={}):\n",
        "    \"\"\"\n",
        "    Get the referred slot if the user refers to another slot in the dialogue state given a target value, slot, last state,\n",
        "    non-referable slots, non-referable pairs, and label maps.\n",
        "\n",
        "    :param target_value: The target value.\n",
        "    :type target_value: str\n",
        "\n",
        "    :param slot: The slot to check.\n",
        "    :type slot: str\n",
        "\n",
        "    :param last_state: The last state.\n",
        "    :type last_state: dict\n",
        "\n",
        "    :param non_referable_slots: The list of non-referable slots.\n",
        "    :type non_referable_slots: list[str]\n",
        "\n",
        "    :param non_referable_pairs: The list of non-referable slot pairs.\n",
        "    :type non_referable_pairs: list[tuple[str, str]]\n",
        "\n",
        "    :param label_maps: The label maps.\n",
        "    :type label_maps: dict, optional\n",
        "\n",
        "    :return: The referred slot.\n",
        "    :rtype: str\n",
        "    \"\"\"\n",
        "\n",
        "    referred_slot = 'none'\n",
        "\n",
        "    if slot in non_referable_slots:\n",
        "        return referred_slot\n",
        "\n",
        "    if slot in last_state and last_state[slot] == target_value:\n",
        "      return referred_slot\n",
        "\n",
        "    for s in last_state:\n",
        "\n",
        "        if s in non_referable_slots:\n",
        "            continue\n",
        "\n",
        "        if ((slot, s) in non_referable_pairs) or ((s, slot) in non_referable_pairs):\n",
        "          continue\n",
        "\n",
        "        if slot == s:\n",
        "          continue\n",
        "\n",
        "        if match_with_label_maps(last_state[s], target_value, label_maps):\n",
        "            referred_slot = s\n",
        "            break\n",
        "\n",
        "    return referred_slot\n",
        "\n",
        "\n",
        "def create_labels(target_value, slot, last_state, input, tok_input_offsets, inform_mem, label_maps, padding_len, max_len, non_referable_slots, non_referable_pairs, multiwoz):\n",
        "  \"\"\"\n",
        "  Create the target operation and the span labels for a slot.\n",
        "\n",
        "  :param target_value: The target value.\n",
        "  :type target_value: str\n",
        "\n",
        "  :param slot: The slot.\n",
        "  :type slot: str\n",
        "\n",
        "  :param last_state: The last state.\n",
        "  :type last_state: dict\n",
        "\n",
        "  :param input: The input string.\n",
        "  :type input: str\n",
        "\n",
        "  :param tok_input_offsets: The token input offsets.\n",
        "  :type tok_input_offsets: list[tuple[int, int]]\n",
        "\n",
        "  :param inform_mem: The inform memory.\n",
        "  :type inform_mem: dict\n",
        "\n",
        "  :param label_maps: The label maps.\n",
        "  :type label_maps: dict\n",
        "\n",
        "  :param padding_len: The padding length.\n",
        "  :type padding_len: int\n",
        "\n",
        "  :param max_len: The maximum length of the input.\n",
        "  :type max_len: int\n",
        "\n",
        "  :param non_referable_slots: The list of non-referable slots (slots that can not use refering).\n",
        "  :type non_referable_slots: list[str]\n",
        "\n",
        "  :param non_referable_pairs: The list of non-referable slot pairs (slots pairs that can not refer to each other).\n",
        "  :type non_referable_pairs: list[tuple[str, str]]\n",
        "\n",
        "  :return: The operation, span, span start index, span end index, referred slot, and informed value.\n",
        "  :rtype: tuple[str, list[int], int, int, str, str]\n",
        "  \"\"\"\n",
        "\n",
        "  oper = 'carryover'\n",
        "  span = [0] * max_len\n",
        "  span_start = 0\n",
        "  span_end = 0\n",
        "  refered_slot = 'none'\n",
        "  informed_value = 'none'\n",
        "\n",
        "  # assert target_value != 'none', 'target value can not be none'\n",
        "\n",
        "  if target_value in ['[NULL]', 'none']:\n",
        "    oper = 'carryover'\n",
        "  elif target_value in ['dontcare', 'yes', 'no']:\n",
        "    oper = target_value\n",
        "  else:\n",
        "    span, span_start, span_end = create_slot_span(input,\n",
        "                                                  target_value,\n",
        "                                                  tok_input_offsets,\n",
        "                                                  padding_len,\n",
        "                                                  label_maps)\n",
        "\n",
        "    informed = False\n",
        "    if slot in inform_mem:\n",
        "      assert len(inform_mem[slot]) == 1, 'greater than 1'\n",
        "      informed, informed_value = is_informed(inform_mem[slot][0], target_value, label_maps, multiwoz)\n",
        "\n",
        "    refered_slot = get_refered_slot(target_value, slot, last_state, non_referable_slots, non_referable_pairs, label_maps)\n",
        "\n",
        "    if sum(span) != 0:\n",
        "      oper = 'update'\n",
        "    elif informed == True:\n",
        "      oper = 'inform'\n",
        "    elif refered_slot != 'none':\n",
        "      oper = 'refer'\n",
        "    else:\n",
        "      oper = 'unpointable'\n",
        "\n",
        "  return oper, span, span_start, span_end, refered_slot, informed_value\n",
        "\n",
        "\n",
        "def create_data(raw_data, slot_list, label_maps, tokenizer, max_len, non_referable_slots, non_referable_pairs, multiwoz):\n",
        "  \"\"\"\n",
        "  Create the data instances for training or evaluation.\n",
        "\n",
        "  :param raw_data: The list of raw data instances.\n",
        "  :type raw_data: list[RawDataInstance]\n",
        "\n",
        "  :param slot_list: The list of slots.\n",
        "  :type slot_list: list[str]\n",
        "\n",
        "  :param label_maps: The label maps.\n",
        "  :type label_maps: dict\n",
        "\n",
        "  :param tokenizer: The tokenizer to tokenize the input.\n",
        "  :type tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "  :param max_len: The maximum length of the input.\n",
        "  :type max_len: int\n",
        "\n",
        "  :param non_referable_slots: The list of non-referable slots.\n",
        "  :type non_referable_slots: list[str]\n",
        "\n",
        "  :param non_referable_pairs: The list of non-referable slot pairs.\n",
        "  :type non_referable_pairs: list[tuple[str, str]]\n",
        "\n",
        "  :return: The list of data instances.\n",
        "  :rtype: list[DataInstance]\n",
        "  \"\"\"\n",
        "\n",
        "  data = []\n",
        "\n",
        "  last_state = {}\n",
        "  cur_state = {}\n",
        "  prev_dial_idx = -1\n",
        "  # loop over raw data\n",
        "  for turn in tqdm(raw_data):\n",
        "\n",
        "    # if new dialogue reset the state\n",
        "    if turn.dial_idx != prev_dial_idx or turn.turn_idx == 0:\n",
        "      cur_state = {}\n",
        "      last_state = {}\n",
        "\n",
        "    # update previous dialogue index\n",
        "    prev_dial_idx = turn.dial_idx\n",
        "\n",
        "    # create model inputs\n",
        "    input, ids, mask, token_type_ids, tok_input_offsets, input_tokens, padding_len = create_inputs(turn.history,\n",
        "                                                                                                   turn.user_utter,\n",
        "                                                                                                   turn.sys_utter,\n",
        "                                                                                                   tokenizer,\n",
        "                                                                                                   max_len)\n",
        "\n",
        "\n",
        "    target_values = []\n",
        "    opers = []\n",
        "    spans = []\n",
        "    spans_start = []\n",
        "    spans_end = []\n",
        "    refer = ['none'] * len(slot_list)\n",
        "    inform_aux_features = [0] * len(slot_list)\n",
        "    ds_aux_features = [0] * len(slot_list)\n",
        "\n",
        "    # for each slot determine its values\n",
        "    for slot_idx, slot in enumerate(slot_list):\n",
        "\n",
        "      # get the slot target value\n",
        "      target_value = '[NULL]'\n",
        "      if slot in turn.turn_slots:\n",
        "        target_value = turn.turn_slots[slot]\n",
        "      elif slot in cur_state:\n",
        "        target_value = cur_state[slot]\n",
        "\n",
        "\n",
        "      # get slot labels\n",
        "      (oper,\n",
        "       span,\n",
        "       span_start,\n",
        "       span_end,\n",
        "       refered_slot,\n",
        "       informed_value) = create_labels(target_value,\n",
        "                                      slot,\n",
        "                                      last_state,\n",
        "                                      input,\n",
        "                                      tok_input_offsets,\n",
        "                                      turn.inform_mem,\n",
        "                                      label_maps,\n",
        "                                      padding_len,\n",
        "                                      max_len,\n",
        "                                      non_referable_slots,\n",
        "                                      non_referable_pairs,\n",
        "                                      multiwoz)\n",
        "\n",
        "      if slot in cur_state and target_value == cur_state[slot] and oper in ['dontcare', 'yes', 'no', 'refer']:\n",
        "        oper = 'carryover'\n",
        "\n",
        "\n",
        "      # create auxiliary features\n",
        "      # mark each informed slot as 1\n",
        "      if slot in turn.inform_mem:\n",
        "        inform_aux_features[slot_idx] = 1\n",
        "      # mark each filled slot as 1\n",
        "      if slot in cur_state:\n",
        "        ds_aux_features[slot_idx] = 1\n",
        "\n",
        "      # update the state\n",
        "      if oper != 'carryover':\n",
        "        cur_state[slot] = target_value\n",
        "        if oper == 'unpointable':\n",
        "          oper = 'carryover'\n",
        "\n",
        "#       if turn.dial_idx == 'MUL2491.json' and turn.turn_idx == 8 and slot == 'restaurant-name':\n",
        "#         print(oper)\n",
        "#         print(span)\n",
        "#         print(refered_slot)\n",
        "#         print(informed_value)\n",
        "#         print(last_state)\n",
        "#         print(cur_state)\n",
        "\n",
        "      target_values.append(target_value)\n",
        "      opers.append(oper)\n",
        "      spans.append(span)\n",
        "      spans_start.append(span_start)\n",
        "      spans_end.append(span_end)\n",
        "      refer[slot_idx] = refered_slot\n",
        "\n",
        "    data.append(DataInstance(ids,\n",
        "                             mask,\n",
        "                             token_type_ids,\n",
        "                             spans,\n",
        "                             spans_start,\n",
        "                             spans_end,\n",
        "                             padding_len,\n",
        "                             input_tokens,\n",
        "                             input,\n",
        "                             opers,\n",
        "                             target_values,\n",
        "                             last_state.copy(),\n",
        "                             cur_state.copy(),\n",
        "                             refer,\n",
        "                             inform_aux_features,\n",
        "                             ds_aux_features))\n",
        "\n",
        "    # update last state\n",
        "    last_state = cur_state.copy()\n",
        "\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "def prepare_data(data_path, slot_list, label_maps, tokenizer, max_len, domains, non_referable_slots, non_referable_pairs, multiwoz):\n",
        "  \"\"\"\n",
        "  Prepare the data for training or evaluation, this usually the function you want to call to preprocess the data for\n",
        "  TripPy model, it encapsulates the whole process of preprcessing the data by calling the other functions in this\n",
        "  module.\n",
        "\n",
        "  :param data_path: The path to the JSON data file.\n",
        "  :type data_path: str\n",
        "\n",
        "  :param slot_list: The list of slots.\n",
        "  :type slot_list: list[str]\n",
        "\n",
        "  :param label_maps: The label maps.\n",
        "  :type label_maps: dict\n",
        "\n",
        "  :param tokenizer: The tokenizer to tokenize the input.\n",
        "  :type tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "  :param max_len: The maximum length of the input.\n",
        "  :type max_len: int\n",
        "\n",
        "  :param domains: The list of domains.\n",
        "  :type domains: list[str]\n",
        "\n",
        "  :param non_referable_slots: The list of non-referable slots.\n",
        "  :type non_referable_slots: list[str]\n",
        "\n",
        "  :param non_referable_pairs: The list of non-referable slot pairs.\n",
        "  :type non_referable_pairs: list[tuple[str, str]]\n",
        "\n",
        "  :return: The raw data and prepared data.\n",
        "  :rtype: tuple[list[RawDataInstance], list[DataInstance]]\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # create raw data\n",
        "  raw_data = read_raw_data(data_path, slot_list, max_len, domains, multiwoz)\n",
        "\n",
        "  # create data\n",
        "  data = create_data(raw_data, slot_list, label_maps, tokenizer, max_len, non_referable_slots, non_referable_pairs, multiwoz)\n",
        "\n",
        "  return raw_data, data\n",
        "\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "  \"\"\"\n",
        "  PyTorch Dataset for the TRIPPY model.\n",
        "\n",
        "  :param data: The list of data instances.\n",
        "  :type data: list[DataInstance]\n",
        "\n",
        "  :param n_slots: The number of slots.\n",
        "  :type n_slots: int\n",
        "\n",
        "  :param oper2id: The mapping of operations to IDs.\n",
        "  :type oper2id: dict[str, int]\n",
        "\n",
        "  :param slot_list: The list of slots.\n",
        "  :type slot_list: list[str]\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data, n_slots, oper2id, slot_list):\n",
        "\n",
        "    # for k in inputs:\n",
        "    #   inputs[k] = inputs[k][:32]\n",
        "\n",
        "    self.ids = [turn.ids for turn in data]\n",
        "    self.mask = [turn.mask for turn in data]\n",
        "    self.token_type_ids = [turn.token_type_ids for turn in data]\n",
        "    self.spans_start = [turn.spans_start for turn in data]\n",
        "    self.spans_end = [turn.spans_end for turn in data]\n",
        "    self.padding_len = [turn.padding_len for turn in data]\n",
        "    self.input_tokens = [' '.join(turn.input_tokens) for turn in data]\n",
        "    self.target_values = ['[VALUESEP]'.join(turn.target_values) for turn in data]\n",
        "    self.opers = [[oper2id[oper] for oper in turn.opers] for turn in data]\n",
        "    # get the index of the refered slot, in case the slot is not present in the slot_list then that means \"none\"\n",
        "    # index of \"none\" is n_slots\n",
        "    self.refer = [[(slot_list.index(r) if r in slot_list else n_slots) for r in turn.refer] for turn in data]\n",
        "    self.inform_aux_features = [turn.inform_aux_features for turn in data]\n",
        "    self.ds_aux_features = [turn.ds_aux_features for turn in data]\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Get the length of the dataset.\n",
        "\n",
        "    :return: The length of the dataset.\n",
        "    :rtype: int\n",
        "    \"\"\"\n",
        "    return len(self.ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    Get an item from the dataset at the given index.\n",
        "\n",
        "    :param idx: The index of the item.\n",
        "    :type idx: int\n",
        "\n",
        "    :return: The item at the given index.\n",
        "    :rtype: dict[str, torch.Tensor or str]\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'ids': torch.tensor(self.ids[idx], dtype=torch.long),\n",
        "        'mask': torch.tensor(self.mask[idx], dtype=torch.long),\n",
        "        'token_type_ids': torch.tensor(self.token_type_ids[idx], dtype=torch.long),\n",
        "        'spans_start': torch.tensor(self.spans_start[idx], dtype=torch.long),\n",
        "        'spans_end': torch.tensor(self.spans_end[idx], dtype=torch.long),\n",
        "        'padding_len': torch.tensor(self.padding_len[idx], dtype=torch.long),\n",
        "        'input_tokens': self.input_tokens[idx],\n",
        "        'target_values': self.target_values[idx],\n",
        "        'opers': torch.tensor(self.opers[idx], dtype=torch.long),\n",
        "        'refer': torch.tensor(self.refer[idx], dtype=torch.long),\n",
        "        'inform_aux_features': torch.tensor(self.inform_aux_features[idx], dtype=torch.float),\n",
        "        'ds_aux_features': torch.tensor(self.ds_aux_features[idx], dtype=torch.float)\n",
        "    }"
      ],
      "metadata": {
        "id": "z6XT14kih0QV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluate"
      ],
      "metadata": {
        "id": "GyPrcTYIh1SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This Module has the evaluation functions for TRIPPY.\n",
        "\"\"\"\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from botiverse.models.TRIPPY.utils import normalize, is_included, included_with_label_maps, create_span_output\n",
        "\n",
        "\n",
        "def get_informed_value(value, target, label_maps, multiwoz):\n",
        "  \"\"\"\n",
        "  Get the informed value based on the value and target, taking into account label maps.\n",
        "\n",
        "  :param value: The original value.\n",
        "  :type value: str\n",
        "  :param target: The target value to compare with.\n",
        "  :type target: str\n",
        "  :param label_maps: The mapping of slot values to their variants.\n",
        "  :type label_maps: dict\n",
        "  :return: The informed value.\n",
        "  :rtype: str\n",
        "  \"\"\"\n",
        "\n",
        "  informed = False\n",
        "  informed_value = value\n",
        "\n",
        "  value = ' '.join(normalize(value, multiwoz))\n",
        "  target = ' '.join(normalize(target, multiwoz))\n",
        "\n",
        "  if value == target or is_included(value, target) or is_included(target, value):\n",
        "    informed = True\n",
        "  if value in label_maps:\n",
        "    informed = included_with_label_maps(target, value, label_maps)\n",
        "  elif target in label_maps:\n",
        "    informed = included_with_label_maps(value, target, label_maps)\n",
        "  if informed: informed_value = target\n",
        "\n",
        "  return informed_value\n",
        "\n",
        "\n",
        "def eval(raw_data, data, model, device, n_slots, slot_list, label_maps, oper2id, multiwoz):\n",
        "  \"\"\"\n",
        "  Evaluate the model on the given data.\n",
        "\n",
        "  :param raw_data: The raw data.\n",
        "  :type raw_data: list\n",
        "  :param data: The processed data.\n",
        "  :type data: list\n",
        "  :param model: The model to evaluate.\n",
        "  :type model: nn.Module\n",
        "  :param device: The device to run the evaluation on.\n",
        "  :type device: torch.device\n",
        "  :param n_slots: The number of slots.\n",
        "  :type n_slots: int\n",
        "  :param slot_list: The list of slots.\n",
        "  :type slot_list: list\n",
        "  :param label_maps: The mapping of slot values to their variants.\n",
        "  :type label_maps: dict\n",
        "  :param oper2id: The mapping of operations to their IDs.\n",
        "  :type oper2id: dict\n",
        "  :return: The evaluation metrics.\n",
        "  :rtype: tuple\n",
        "  \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  # normalize label_maps\n",
        "  label_maps_tmp = {}\n",
        "  for v in label_maps:\n",
        "      label_maps_tmp[' '.join(normalize(v, multiwoz))] = [' '.join(normalize(nv, multiwoz)) for nv in label_maps[v]]\n",
        "  label_maps = label_maps_tmp\n",
        "\n",
        "\n",
        "  # metrics\n",
        "  Y_true, Y_pred = [], []\n",
        "  per_slot_acc = {slot:[] for slot in slot_list}\n",
        "  joint_goal_acc = []\n",
        "\n",
        "  # state\n",
        "  pred_last_state = {}\n",
        "  pre_dialogue_idx = -1\n",
        "\n",
        "  # debugging\n",
        "  states = []\n",
        "  sentences = []\n",
        "  indices = []\n",
        "  prev_idx = -1\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for raw_turn, turn in tqdm(zip(raw_data, data), total=len(raw_data)):\n",
        "\n",
        "      ids = torch.tensor(turn.ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "      mask = torch.tensor(turn.mask, dtype=torch.long).unsqueeze(0).to(device)\n",
        "      token_type_ids = torch.tensor(turn.token_type_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "      inform_aux_features = torch.tensor(turn.inform_aux_features, dtype=torch.float).unsqueeze(0).to(device)\n",
        "      # ds_aux_features = torch.tensor(turn.ds_aux_features, dtype=torch.float).unsqueeze(0).to(device)\n",
        "      input_tokens = ' '.join(turn.input_tokens)\n",
        "      padding_len = turn.padding_len\n",
        "      turn_idx = raw_turn.turn_idx\n",
        "      dialogue_idx = raw_turn.dial_idx\n",
        "      current_state = turn.cur_state\n",
        "      inform_mem = raw_turn.inform_mem\n",
        "      opers = turn.opers\n",
        "\n",
        "      # new dialogue reset the state and the state auxiliary features\n",
        "      if turn_idx == 0 or dialogue_idx != pre_dialogue_idx:\n",
        "        pred_last_state = {}\n",
        "        ds_aux_features = torch.zeros((1, n_slots)).to(device)\n",
        "\n",
        "      # get model outputs\n",
        "      slots_start_logits, slots_end_logits, slots_oper_logits, slots_refer_logits = model(ids=ids,\n",
        "                                                                                          mask=mask,\n",
        "                                                                                          token_type_ids=token_type_ids,\n",
        "                                                                                          inform_aux_features=inform_aux_features,\n",
        "                                                                                          ds_aux_features=ds_aux_features)\n",
        "\n",
        "      # update the predicted state of each slot\n",
        "      pred_current_state = pred_last_state.copy()\n",
        "      for slot_idx, slot in enumerate(slot_list):\n",
        "\n",
        "        # get the predicted operation\n",
        "        pred_oper = slots_oper_logits[slot_idx][0].argmax(dim=-1).item()\n",
        "\n",
        "        # keep track of operations for f1 score\n",
        "        Y_pred.append(pred_oper)\n",
        "        Y_true.append(oper2id[opers[slot_idx]])\n",
        "\n",
        "        # update the slot based on the operation\n",
        "        if pred_oper == oper2id['carryover']: # carryover\n",
        "          continue\n",
        "        elif pred_oper == oper2id['dontcare']: # dontcare\n",
        "          pred_current_state[slot] = 'dontcare'\n",
        "        elif pred_oper == oper2id['update']: # update\n",
        "          pred_current_state[slot] = create_span_output(slots_start_logits[slot_idx][0].cpu().detach().numpy(),\n",
        "                                                        slots_end_logits[slot_idx][0].cpu().detach().numpy(),\n",
        "                                                        padding_len,\n",
        "                                                        input_tokens)\n",
        "        elif pred_oper == oper2id['refer']: # refer\n",
        "          refered_slot = slots_refer_logits[slot_idx][0].argmax(dim=-1).item()\n",
        "          if refered_slot != n_slots and slot_list[refered_slot] in pred_last_state:\n",
        "            pred_current_state[slot] = pred_last_state[slot_list[refered_slot]]\n",
        "        elif pred_oper == oper2id['yes']: # yes\n",
        "          pred_current_state[slot] = 'yes'\n",
        "        elif pred_oper == oper2id['no']: # no\n",
        "          pred_current_state[slot] = 'no'\n",
        "        elif pred_oper == oper2id['inform']: # inform\n",
        "          if slot in inform_mem:\n",
        "            pred_current_state[slot] = '§§' + inform_mem[slot][0]\n",
        "\n",
        "      # update the state auxiliary features\n",
        "      for slot_idx, slot in enumerate(slot_list):\n",
        "          ds_aux_features[0, slot_idx] = 1 if slot in pred_current_state else 0\n",
        "\n",
        "      # calculate accuracy\n",
        "      joint = True\n",
        "      for slot_idx, slot in enumerate(slot_list):\n",
        "\n",
        "        # if not present in both\n",
        "        if slot not in current_state and slot not in pred_current_state:\n",
        "          per_slot_acc[slot].append(1.0)\n",
        "          continue\n",
        "\n",
        "        # if slot only in one of them then mark as 0\n",
        "        if (slot in current_state and slot not in pred_current_state) or (slot not in current_state and slot in pred_current_state):\n",
        "          joint = False\n",
        "          per_slot_acc[slot].append(0.0)\n",
        "          continue\n",
        "\n",
        "        # get values\n",
        "        val = current_state[slot]\n",
        "        pred_val = pred_current_state[slot]\n",
        "\n",
        "        # normalize values\n",
        "        val = ' '.join(normalize(val, multiwoz))\n",
        "        pred_val = ' '.join(normalize(pred_val, multiwoz))\n",
        "\n",
        "        # handle inform\n",
        "        if pred_val[0:3] == \"§§ \":\n",
        "          if pred_val[3:] != 'none':\n",
        "              pred_val = get_informed_value(pred_val[3:], val, label_maps, multiwoz)\n",
        "        elif pred_val[0:2] == \"§§\":\n",
        "            if pred_val[2:] != 'none':\n",
        "                pred_val = get_informed_value(pred_val[2:], val, label_maps, multiwoz)\n",
        "\n",
        "        # match\n",
        "        if pred_val == val:\n",
        "          per_slot_acc[slot].append(1.0)\n",
        "        elif val != 'none' and val != 'dontcare' and val != 'true' and val != 'false' and val in label_maps:\n",
        "          no_match = True\n",
        "          for variant in label_maps[val]:\n",
        "              if variant == pred_val:\n",
        "                  no_match = False\n",
        "                  break\n",
        "          if no_match:\n",
        "              per_slot_acc[slot].append(0.0)\n",
        "              joint = False\n",
        "          else:\n",
        "              per_slot_acc[slot].append(1.0)\n",
        "        else:\n",
        "            per_slot_acc[slot].append(0.0)\n",
        "            joint = False\n",
        "\n",
        "      # append joint\n",
        "      joint_goal_acc.append(1.0 if joint else 0.0)\n",
        "\n",
        "\n",
        "      # update vars for next turn\n",
        "      pred_last_state = pred_current_state.copy()\n",
        "      pre_dialogue_idx = dialogue_idx\n",
        "\n",
        "#       # debugging\n",
        "#       if per_slot_acc['attraction-name'][-1] < 0.99 and prev_idx != dialogue_idx:\n",
        "#         print('dialogue_idx', dialogue_idx)\n",
        "#         print('turn_idx', turn_idx)\n",
        "#         print('pred_state', dict(sorted(pred_current_state.items())))\n",
        "#         print('cur_state', dict(sorted(current_state.items())))\n",
        "#         print('input tok', input_tokens)\n",
        "#         print('inform_mem', inform_mem)\n",
        "#         print('inform aux', inform_aux_features)\n",
        "#         print('oper', opers[slot_list.index('attraction-name')])\n",
        "#         prev_idx = dialogue_idx\n",
        "\n",
        "      # debugging\n",
        "      if joint == False:\n",
        "        states.append((copy.deepcopy(pred_current_state), current_state))\n",
        "        sentences.append(input_tokens)\n",
        "        indices.append((dialogue_idx, turn_idx))\n",
        "\n",
        "  # debugging\n",
        "  # prev = \"\"\n",
        "  # for i in range(len(states)):\n",
        "  #   if prev == indices[i][0] or len(states[i][0]) != len(states[i][1]):\n",
        "  #     continue\n",
        "  #   if 'attraction-name' not in states[i][1]:# and 'restaurant-name' not in states[i][1]:\n",
        "  #     continue\n",
        "  #   prev = indices[i][0]\n",
        "  #   print(dict(sorted(states[i][0].items())))\n",
        "  #   print(dict(sorted(states[i][1].items())))\n",
        "  #   print(sentences[i])\n",
        "  #   print(indices[i])\n",
        "  #   print(\"\\n\")\n",
        "\n",
        "  # calculate per slot accuracy\n",
        "  per_slot_acc = {slot: np.mean(acc) for slot, acc in per_slot_acc.items()}\n",
        "\n",
        "  # calculate f1 scores\n",
        "  macro_f1_score = f1_score(Y_true, Y_pred, average='macro')\n",
        "  all_f1_score = f1_score(Y_true, Y_pred, average=None)\n",
        "\n",
        "  return np.mean(joint_goal_acc), per_slot_acc, macro_f1_score, all_f1_score"
      ],
      "metadata": {
        "id": "YceRm0dxh3RW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## infer"
      ],
      "metadata": {
        "id": "MiUQUsbTh4xJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This Module has the inference functions for TRIPPY.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# from botiverse.models.TRIPPY.data import create_inputs\n",
        "# from botiverse.models.TRIPPY.utils import create_span_output\n",
        "\n",
        "\n",
        "def infer(model, slot_list, current_state, history, sys_utter, user_utter, inform_mem, device, oper2id, tokenizer, max_len):\n",
        "  \"\"\"\n",
        "  Infer the dialogue state using the TRIPPY model.\n",
        "\n",
        "  :param model: The TRIPPY model for inference.\n",
        "  :type model: TRIPPY\n",
        "  :param slot_list: The list of slots.\n",
        "  :type slot_list: list\n",
        "  :param current_state: The current dialogue state.\n",
        "  :type current_state: dict\n",
        "  :param history: The dialogue history.\n",
        "  :type history: list\n",
        "  :param sys_utter: The system's utterance.\n",
        "  :type sys_utter: str\n",
        "  :param user_utter: The user's utterance.\n",
        "  :type user_utter: str\n",
        "  :param inform_mem: The inform memory.\n",
        "  :type inform_mem: dict\n",
        "  :param device: The device to run the inference on.\n",
        "  :type device: torch.device\n",
        "  :param oper2id: The mapping of operations to IDs.\n",
        "  :type oper2id: dict\n",
        "  :param tokenizer: The tokenizer to tokenize the input.\n",
        "  :type tokenizer: transformers.PreTrainedTokenizer\n",
        "  :param max_len: The maximum length of the input sequence.\n",
        "  :type max_len: int\n",
        "  :return: The predicted dialogue state.\n",
        "  :rtype: dict\n",
        "  \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  # turn data to inputs\n",
        "  input, ids, mask, token_type_ids, tok_input_offsets, input_tokens, padding_len = create_inputs(history,\n",
        "                                                                                                 user_utter,\n",
        "                                                                                                 sys_utter,\n",
        "                                                                                                 tokenizer,\n",
        "                                                                                                 max_len)\n",
        "\n",
        "\n",
        "  # print(input, ids, mask, token_type_ids, tok_input_offsets, input_tokens, padding_len)\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "    n_slots = len(slot_list)\n",
        "    ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    inform_aux_features = torch.zeros((1, n_slots)).to(device)\n",
        "    ds_aux_features = torch.zeros((1, n_slots)).to(device)\n",
        "    input_tokens = ' '.join(input_tokens)\n",
        "    padding_len = padding_len\n",
        "\n",
        "    for slot_idx, slot in enumerate(slot_list):\n",
        "      if slot in inform_mem:\n",
        "        inform_aux_features[0, slot_idx] = 1\n",
        "      if slot in current_state:\n",
        "        ds_aux_features[0, slot_idx] = 1\n",
        "\n",
        "    # print(slot_list)\n",
        "    # print(inform_aux_features)\n",
        "    # print(inform_mem)\n",
        "\n",
        "\n",
        "    # get model outputs\n",
        "    slots_start_logits, slots_end_logits, slots_oper_logits, slots_refer_logits = model(ids=ids,\n",
        "                                                                                        mask=mask,\n",
        "                                                                                        token_type_ids=token_type_ids,\n",
        "                                                                                        inform_aux_features=inform_aux_features,\n",
        "                                                                                        ds_aux_features=ds_aux_features)\n",
        "\n",
        "\n",
        "    # update the predicted state of each slot\n",
        "    pred_state = current_state.copy()\n",
        "    for slot_idx, slot in enumerate(slot_list):\n",
        "\n",
        "      # get the predicted operation\n",
        "      pred_oper = slots_oper_logits[slot_idx][0].argmax(dim=-1).item()\n",
        "      # print(slot, torch.softmax(slots_oper_logits[slot_idx][0], dim=-1))\n",
        "\n",
        "      # update the slot based on the operation\n",
        "      if pred_oper == oper2id['carryover']: # carryover\n",
        "        continue\n",
        "      elif pred_oper == oper2id['dontcare']: # dontcare\n",
        "        pred_state[slot] = 'dontcare'\n",
        "      elif pred_oper == oper2id['update']: # update\n",
        "        pred_state[slot] = create_span_output(slots_start_logits[slot_idx][0].cpu().detach().numpy(),\n",
        "                                              slots_end_logits[slot_idx][0].cpu().detach().numpy(),\n",
        "                                              padding_len,\n",
        "                                              input_tokens)\n",
        "      elif pred_oper == oper2id['refer']: # refer\n",
        "        refered_slot = slots_refer_logits[slot_idx][0].argmax(dim=-1).item()\n",
        "        if refered_slot != n_slots and slot_list[refered_slot] in current_state:\n",
        "          pred_state[slot] = current_state[slot_list[refered_slot]]\n",
        "      elif pred_oper == oper2id['yes']: # yes\n",
        "        pred_state[slot] = 'yes'\n",
        "      elif pred_oper == oper2id['no']: # no\n",
        "        pred_state[slot] = 'no'\n",
        "      elif pred_oper == oper2id['inform']: # inform\n",
        "        if slot in inform_mem:\n",
        "          pred_state[slot] = '§§' + inform_mem[slot][0]\n",
        "\n",
        "\n",
        "  return pred_state"
      ],
      "metadata": {
        "id": "bm2eJ-ZQh5rX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run"
      ],
      "metadata": {
        "id": "dKnyUtG6h6sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This Module has the run functions for TRIPPY that train and evaluate the model.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "# from botiverse.models.TRIPPY.data import prepare_data, Dataset\n",
        "# from botiverse.models.TRIPPY.train import train\n",
        "# from botiverse.models.TRIPPY.evaluate import eval\n",
        "\n",
        "\n",
        "def run(model, domains, slot_list, label_maps, train_json, dev_json, test_json, device, non_referable_slots, non_referable_pairs, model_path, TRIPPY_config):\n",
        "    \"\"\"\n",
        "    Train and evaluate the TRIPPY model.\n",
        "\n",
        "    :param model: The TRIPPY model.\n",
        "    :type model: TRIPPY\n",
        "    :param domains: The domains to consider in the dataset.\n",
        "    :type domains: list\n",
        "    :param slot_list: The list of slots.\n",
        "    :type slot_list: list\n",
        "    :param label_maps: The mapping of slot values to their variants.\n",
        "    :type label_maps: dict\n",
        "    :param train_json: The path to the training dataset in JSON format.\n",
        "    :type train_json: str\n",
        "    :param dev_json: The path to the development dataset in JSON format.\n",
        "    :type dev_json: str\n",
        "    :param test_json: The path to the testing dataset in JSON format.\n",
        "    :type test_json: str\n",
        "    :param device: The device to train and evaluate the model on.\n",
        "    :type device: torch.device\n",
        "    :param non_referable_slots: The slots that are not referable.\n",
        "    :type non_referable_slots: list\n",
        "    :param non_referable_pairs: The pairs of slots that are not referable.\n",
        "    :type non_referable_pairs: list\n",
        "    :param model_path: The path to save the best model.\n",
        "    :type model_path: str\n",
        "    :param TRIPPY_config: The configuration for TRIPPY.\n",
        "    :type TRIPPY_config: TRIPPYConfig\n",
        "    \"\"\"\n",
        "\n",
        "    n_slots = len(slot_list)\n",
        "\n",
        "    # train\n",
        "    print('Preprocessing train set...')\n",
        "    train_raw_data, train_data = prepare_data(train_json, slot_list, label_maps, TRIPPY_config.tokenizer, TRIPPY_config.max_len, domains, non_referable_slots, non_referable_pairs, TRIPPY_config.multiwoz)\n",
        "    train_dataset = Dataset(train_data, n_slots, TRIPPY_config.oper2id, slot_list)\n",
        "    train_sampler = torch.utils.data.RandomSampler(train_dataset)\n",
        "    train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                                    sampler=train_sampler,\n",
        "                                                    batch_size=TRIPPY_config.train_batch_size)\n",
        "\n",
        "    # dev\n",
        "    print('Preprocessing dev set...')\n",
        "    dev_raw_data, dev_data = prepare_data(dev_json, slot_list, label_maps, TRIPPY_config.tokenizer, TRIPPY_config.max_len, domains, non_referable_slots, non_referable_pairs, TRIPPY_config.multiwoz)\n",
        "    dev_dataset = Dataset(dev_data, n_slots, TRIPPY_config.oper2id, slot_list)\n",
        "    dev_data_loader = torch.utils.data.DataLoader(dev_dataset,\n",
        "                                                  batch_size=TRIPPY_config.dev_batch_size)\n",
        "\n",
        "    # test\n",
        "    print('Preprocessing test set...')\n",
        "    test_raw_data, test_data = prepare_data(test_json, slot_list, label_maps, TRIPPY_config.tokenizer, TRIPPY_config.max_len, domains, non_referable_slots, non_referable_pairs, TRIPPY_config.multiwoz)\n",
        "    test_dataset = Dataset(test_data, n_slots, TRIPPY_config.oper2id, slot_list)\n",
        "    test_data_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                                   batch_size=TRIPPY_config.test_batch_size)\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_parameters = [\n",
        "        {\n",
        "            \"params\": [\n",
        "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "            \"weight_decay\": TRIPPY_config.weight_decay,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [\n",
        "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # num_train_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE * EPOCHS)\n",
        "    num_train_steps = len(train_data_loader) * TRIPPY_config.epochs\n",
        "    num_warmup_steps = int(num_train_steps * TRIPPY_config.warmup_proportion)\n",
        "\n",
        "    optimizer = AdamW(optimizer_parameters, lr=TRIPPY_config.lr, eps=TRIPPY_config.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps\n",
        "    )\n",
        "\n",
        "    best_joint = -1\n",
        "    for epoch in range(TRIPPY_config.epochs):\n",
        "        print(f'\\nEpoch: {epoch} ---------------------------------------------------------------')\n",
        "        print('Training the model...')\n",
        "        train(train_data_loader, model, optimizer, device, scheduler, n_slots, TRIPPY_config.ignore_idx, TRIPPY_config.oper2id)\n",
        "        print('Evaluating the model on dev set...')\n",
        "        # jaccard_score, macro_f1_score, all_f1_score = eval_f1_jac(dev_data_loader, model, device, n_slots)\n",
        "        # joint_goal_acc, states, sentences, indices = eval_joint(dev_raw_data, dev_data, model, device, n_slots, slot_list, label_maps)\n",
        "        joint_goal_acc, per_slot_acc, macro_f1_score, all_f1_score = eval(dev_raw_data, dev_data, model, device, n_slots, slot_list, label_maps, TRIPPY_config.oper2id, TRIPPY_config.multiwoz)\n",
        "        # print(f'Joint Goal Acc: {joint_goal_acc}, Jaccard Score: {jaccard_score}, Macro F1 Score: {macro_f1_score}')\n",
        "        print(f'Joint Goal Acc: {joint_goal_acc}')\n",
        "        print(f'Per Slot Acc: {per_slot_acc}')\n",
        "        print(f'Macro F1 Score: {macro_f1_score}')\n",
        "        print(f'All f1 score = {all_f1_score}')\n",
        "        if joint_goal_acc > best_joint:\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            best_joint = joint_goal_acc\n",
        "\n",
        "    print('Loading best model on dev set...')\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    print('Evaluating the model on test set...')\n",
        "    joint_goal_acc, per_slot_acc, macro_f1_score, all_f1_score = eval(test_raw_data, test_data, model, device, n_slots, slot_list, label_maps, TRIPPY_config.oper2id, TRIPPY_config.multiwoz)\n",
        "    print(f'Joint Goal Acc: {joint_goal_acc}')\n",
        "    print(f'Per Slot Acc: {per_slot_acc}')\n",
        "    print(f'Macro F1 Score: {macro_f1_score}')\n",
        "    print(f'All f1 score = {all_f1_score}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# from botiverse.bots import Theorizer\n",
        "# from botiverse.models import NeuralNet\n",
        "# from botiverse.preprocessors import BertEmbedder"
      ],
      "metadata": {
        "id": "u0RztfPnh7L_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train"
      ],
      "metadata": {
        "id": "uQSKDsFih7pD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This Module has the training functions for TRIPPY.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from botiverse.models.TRIPPY.utils import AverageMeter\n",
        "\n",
        "\n",
        "def span_loss_fn(start_logits, end_logits, targets_start, targets_end, ignore_idx):\n",
        "  \"\"\"\n",
        "  Compute the span loss.\n",
        "\n",
        "  :param start_logits: The start logits.\n",
        "  :type start_logits: torch.Tensor\n",
        "  :param end_logits: The end logits.\n",
        "  :type end_logits: torch.Tensor\n",
        "  :param targets_start: The start targets.\n",
        "  :type targets_start: torch.Tensor\n",
        "  :param targets_end: The end targets.\n",
        "  :type targets_end: torch.Tensor\n",
        "  :param ignore_idx: The index to ignore in the loss calculation.\n",
        "  :type ignore_idx: int\n",
        "  :return: The span loss.\n",
        "  :rtype: torch.Tensor\n",
        "  \"\"\"\n",
        "  l1 = nn.CrossEntropyLoss(ignore_index=ignore_idx, reduction='none')(start_logits, targets_start)\n",
        "  l2 = nn.CrossEntropyLoss(ignore_index=ignore_idx, reduction='none')(end_logits, targets_end)\n",
        "  return (l1 + l2) / 2.0\n",
        "\n",
        "def oper_loss_fn(oper_logits, oper_labels, ignore_idx):\n",
        "  \"\"\"\n",
        "  Compute the operation loss.\n",
        "\n",
        "  :param oper_logits: The operation logits.\n",
        "  :type oper_logits: torch.Tensor\n",
        "  :param oper_labels: The operation labels.\n",
        "  :type oper_labels: torch.Tensor\n",
        "  :param ignore_idx: The index to ignore in the loss calculation.\n",
        "  :type ignore_idx: int\n",
        "  :return: The operation loss.\n",
        "  :rtype: torch.Tensor\n",
        "  \"\"\"\n",
        "  l = nn.CrossEntropyLoss(ignore_index=ignore_idx, reduction='none')(oper_logits, oper_labels)\n",
        "  return l\n",
        "\n",
        "def refer_loss_fn(refer_logits, refer_labels, ignore_idx):\n",
        "  \"\"\"\n",
        "  Compute the refer loss.\n",
        "\n",
        "  :param refer_logits: The refer logits.\n",
        "  :type refer_logits: torch.Tensor\n",
        "  :param refer_labels: The refer labels.\n",
        "  :type refer_labels: torch.Tensor\n",
        "  :param ignore_idx: The index to ignore in the loss calculation.\n",
        "  :type ignore_idx: int\n",
        "  :return: The refer loss.\n",
        "  :rtype: torch.Tensor\n",
        "  \"\"\"\n",
        "  l = nn.CrossEntropyLoss(ignore_index=ignore_idx, reduction='none')(refer_logits, refer_labels)\n",
        "  return l\n",
        "\n",
        "def train(data_loader, model, optimizer, device, scheduler, n_slots, ignore_idx, oper2id):\n",
        "  \"\"\"\n",
        "  Perform the training loop for a model on the given data.\n",
        "\n",
        "  :param data_loader: The data loader providing the training batches.\n",
        "  :type data_loader: DataLoader\n",
        "\n",
        "  :param model: The model to be trained.\n",
        "  :type model: nn.Module\n",
        "\n",
        "  :param optimizer: The optimizer used to update the model's parameters.\n",
        "  :type optimizer: Optimizer\n",
        "\n",
        "  :param device: The device (e.g., CPU or GPU) on which the training will be performed.\n",
        "  :type device: torch.device\n",
        "\n",
        "  :param scheduler: The scheduler for adjusting the learning rate during training.\n",
        "  :type scheduler: _LRScheduler\n",
        "\n",
        "  :param n_slots: The number of slots in the task.\n",
        "  :type n_slots: int\n",
        "\n",
        "  :param ignore_idx: The index to ignore during loss computation.\n",
        "  :type ignore_idx: int\n",
        "\n",
        "  :param oper2id: A dictionary mapping operation names to their corresponding IDs.\n",
        "  :type oper2id: dict\n",
        "  \"\"\"\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  losses = AverageMeter()\n",
        "\n",
        "  tk0 = tqdm(data_loader)\n",
        "  for i, batch in enumerate(tk0):\n",
        "\n",
        "    ids = batch['ids'].to(device)\n",
        "    mask = batch['mask'].to(device)\n",
        "    token_type_ids = batch['token_type_ids'].to(device)\n",
        "    spans_start = batch['spans_start'].to(device)\n",
        "    spans_end = batch['spans_end'].to(device)\n",
        "    opers = batch['opers'].to(device)\n",
        "    refer = batch['refer'].to(device)\n",
        "    inform_aux_features = batch['inform_aux_features'].to(device)\n",
        "    ds_aux_features = batch['ds_aux_features'].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    slots_start_logits, slots_end_logits, slots_oper_logits, slots_refer_logits = model(ids=ids,\n",
        "                                                                                        mask=mask,\n",
        "                                                                                        token_type_ids=token_type_ids,\n",
        "                                                                                        inform_aux_features=inform_aux_features,\n",
        "                                                                                        ds_aux_features=ds_aux_features)\n",
        "\n",
        "    batch_loss = 0.0\n",
        "\n",
        "    for slot in range(n_slots):\n",
        "\n",
        "      oper_loss = oper_loss_fn(slots_oper_logits[slot], opers[:,slot], ignore_idx)\n",
        "\n",
        "      span_loss = span_loss_fn(slots_start_logits[slot],\n",
        "                               slots_end_logits[slot],\n",
        "                               spans_start[:,slot],\n",
        "                               spans_end[:,slot],\n",
        "                               ignore_idx)\n",
        "      token_is_pointable = (spans_start[:,slot] > 0).float()\n",
        "      span_loss *= token_is_pointable\n",
        "\n",
        "      refer_loss = refer_loss_fn(slots_refer_logits[slot], refer[:,slot], ignore_idx)\n",
        "      token_is_referrable = (opers[:,slot] == oper2id['refer']).float()\n",
        "      refer_loss *= token_is_referrable\n",
        "\n",
        "      total_loss = 0.8 * oper_loss + 0.1 * span_loss + 0.1 * refer_loss\n",
        "\n",
        "      batch_loss += total_loss.sum()\n",
        "\n",
        "    # batch_loss /= n_slots\n",
        "    batch_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    losses.update(batch_loss.item(), ids.size(0))\n",
        "\n",
        "    tk0.set_postfix(loss=losses.avg)"
      ],
      "metadata": {
        "id": "4tLnoLG_h9ST"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRIPPY_DST"
      ],
      "metadata": {
        "id": "d642fvFxh-MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This Module has base code and interfaces for TripPy Dialogue State Tracker.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "\n",
        "# from botiverse.models.TRIPPY.utils import normalize, mask_utterance\n",
        "# from botiverse.models.TRIPPY.data import get_ontology_label_maps, prepare_data, Dataset\n",
        "# from botiverse.models.TRIPPY.run import run\n",
        "# from botiverse.models.TRIPPY.infer import infer\n",
        "# from botiverse.models.TRIPPY.config import TRIPPYConfig\n",
        "# from botiverse.models.TRIPPY.TRIPPY import TRIPPY\n",
        "\n",
        "\n",
        "class TRIPPYDST:\n",
        "    \"\"\"\n",
        "    TRIPPYDST is a class that represents the TripPy Dialogue State Tracker.\n",
        "\n",
        "    It provides methods for loading the model, training the model, updating the dialogue state,\n",
        "    getting the current dialogue state, deleting slots, resetting the tracker, and displaying the tracker information.\n",
        "\n",
        "    :param domains: The list of domains to consider.\n",
        "    :type domains: list[str]\n",
        "    :param ontology_path: The path to the ontology file.\n",
        "    :type ontology_path: str\n",
        "    :param label_maps_path: The path to the label maps file.\n",
        "    :type label_maps_path: str\n",
        "    :param non_referable_slots: The list of non-referable slots.\n",
        "    :type non_referable_slots: list[str]\n",
        "    :param non_referable_pairs: The list of non-referable slot pairs.\n",
        "    :type non_referable_pairs: list[tuple[str, str]]\n",
        "    :param from_scratch: Whether to train the model from scratch.\n",
        "    :type from_scratch: bool\n",
        "    :param TRIPPY_config: The configuration for the TRIPPY model, defaults to TRIPPYConfig()\n",
        "    :type TRIPPY_config: TRIPPYConfig, optional\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, domains, ontology_path, label_maps_path, non_referable_slots, non_referable_pairs, from_scratch, BERT_config, TRIPPY_config=TRIPPYConfig()):\n",
        "        self.domains = domains\n",
        "        self.ontology_path = ontology_path\n",
        "        self.label_maps_path = label_maps_path\n",
        "        self.non_referable_slots = non_referable_slots\n",
        "        self.non_referable_pairs = non_referable_pairs\n",
        "        self.from_scratch = from_scratch\n",
        "        self.BERT_config = BERT_config\n",
        "        self.TRIPPY_config = TRIPPY_config\n",
        "\n",
        "        slot_list, label_maps = get_ontology_label_maps(ontology_path, label_maps_path, domains)\n",
        "        self.slot_list = slot_list\n",
        "        self.n_slots = len(slot_list)\n",
        "        self.label_maps = label_maps\n",
        "        self.state = {}\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = TRIPPY(len(slot_list), TRIPPY_config.hid_dim, TRIPPY_config.n_oper, TRIPPY_config.dropout, from_scratch, BERT_config, TRIPPY_config).to(self.device)\n",
        "        self.history = []\n",
        "\n",
        "    def save_model(self, model_path):\n",
        "        \"\"\"\n",
        "        Save the trained model.\n",
        "\n",
        "        :param model_path: The path to save the model.\n",
        "        :type model_path: str\n",
        "        \"\"\"\n",
        "        torch.save(self.model.state_dict(), model_path)\n",
        "\n",
        "    def load_model(self, model_path, test_path):\n",
        "      \"\"\"\n",
        "      Load the trained model.\n",
        "\n",
        "      :param model_path: The path to the saved model.\n",
        "      :type model_path: str\n",
        "      :param test_path: The path to the test data for evaluation.\n",
        "      :type test_path: str\n",
        "      \"\"\"\n",
        "      if self.from_scratch == True:\n",
        "          # Get saved weights\n",
        "          state_dict = torch.load(model_path, map_location=self.device)\n",
        "\n",
        "          # Delete position_ids from the state_dict if available\n",
        "          if 'bert.embeddings.position_ids' in state_dict.keys():\n",
        "              del state_dict['bert.embeddings.position_ids']\n",
        "\n",
        "          # Get the new weights keys from the model\n",
        "          new_keys = list(self.model.state_dict().keys())\n",
        "\n",
        "          # Get the weights from the state_dict\n",
        "          old_keys = list(state_dict.keys())\n",
        "          weights = list(state_dict.values())\n",
        "\n",
        "          # Create a new state_dict with the new keys\n",
        "          new_state_dict = OrderedDict()\n",
        "          for i in range(len(new_keys)):\n",
        "              new_state_dict[new_keys[i]] = weights[i]\n",
        "              # print(old_keys[i], '->', new_keys[i])\n",
        "\n",
        "          self.model.load_state_dict(new_state_dict)\n",
        "\n",
        "      else:\n",
        "          self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
        "\n",
        "      print('Model loaded successfully.')\n",
        "      if test_path is not None:\n",
        "        print('Preprocessing the data...')\n",
        "        test_raw_data, test_data = prepare_data(test_path, self.slot_list, self.label_maps, self.TRIPPY_config.tokenizer, self.TRIPPY_config.max_len, self.domains, self.non_referable_slots, self.non_referable_pairs, self.TRIPPY_config.multiwoz)\n",
        "        test_dataset = Dataset(test_data, self.n_slots, self.TRIPPY_config.oper2id, self.slot_list)\n",
        "        test_data_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                                       batch_size=self.TRIPPY_config.test_batch_size)\n",
        "        print('Evaluating the model on the data...')\n",
        "        joint_goal_acc, per_slot_acc, macro_f1_score, all_f1_score = eval(test_raw_data, test_data, self.model, self.device, self.n_slots, self.slot_list, self.label_maps, self.TRIPPY_config.oper2id, self.TRIPPY_config.multiwoz)\n",
        "        print(f'Joint Goal Acc: {joint_goal_acc}')\n",
        "        print(f'Per Slot Acc: {per_slot_acc}')\n",
        "        print(f'Macro F1 Score: {macro_f1_score}')\n",
        "        print(f'All f1 score = {all_f1_score}')\n",
        "\n",
        "\n",
        "    def train(self, train_path, dev_path, test_path, model_path):\n",
        "      \"\"\"\n",
        "      Train the model.\n",
        "\n",
        "      :param train_path: The path to the training data.\n",
        "      :type train_path: str\n",
        "      :param dev_path: The path to the development data for evaluation during training.\n",
        "      :type dev_path: str\n",
        "      :param test_path: The path to the test data for evaluation after training.\n",
        "      :type test_path: str\n",
        "      :param model_path: The path to save the trained model.\n",
        "      :type model_path: str\n",
        "      \"\"\"\n",
        "      run(self.model, self.domains, self.slot_list, self.label_maps, train_path, dev_path, test_path, self.device, self.non_referable_slots, self.non_referable_pairs, model_path, self.TRIPPY_config)\n",
        "\n",
        "    def update_state(self, sys_utter, user_utter, inform_mem):\n",
        "      \"\"\"\n",
        "      Update the dialogue state based on the system and user utterances.\n",
        "\n",
        "      :param sys_utter: The system utterance.\n",
        "      :type sys_utter: str\n",
        "      :param user_utter: The user utterance.\n",
        "      :type user_utter: str\n",
        "      :param inform_mem: The inform memory containing previous slot-value pairs.\n",
        "      :type inform_mem: dict[str, list[str]]\n",
        "      :return: The updated dialogue state.\n",
        "      :rtype: dict[str, str]\n",
        "      \"\"\"\n",
        "\n",
        "      # normalize utterances\n",
        "      user_utter = ' '.join(normalize(user_utter, self.TRIPPY_config.multiwoz))\n",
        "      sys_utter = ' '.join(normalize(sys_utter, self.TRIPPY_config.multiwoz))\n",
        "      # delex the system utterance\n",
        "      sys_utter = ' '.join(mask_utterance(sys_utter, inform_mem, self.TRIPPY_config.multiwoz, '[UNK]'))\n",
        "\n",
        "      self.state = infer(self.model, self.slot_list, self.state, self.history, sys_utter, user_utter, inform_mem, self.device, self.TRIPPY_config.oper2id, self.TRIPPY_config.tokenizer, self.TRIPPY_config.max_len)\n",
        "      self.history = [user_utter, sys_utter] + self.history\n",
        "      return self.state.copy()\n",
        "\n",
        "    def get_dialogue_state(self):\n",
        "      \"\"\"\n",
        "      Get a copy of the current dialogue state.\n",
        "\n",
        "      :return: A copy of the dialogue state.\n",
        "      :rtype: dict[str, str]\n",
        "      \"\"\"\n",
        "      return self.state.copy()\n",
        "\n",
        "    def delete_slots(self, domain, slot):\n",
        "      \"\"\"\n",
        "      Delete slots from the dialogue state.\n",
        "\n",
        "      If a domain is specified, all slots in that domain will be deleted.\n",
        "      If a slot is specified, that specific slot will be deleted.\n",
        "      If neither domain nor slot is specified, all slots will be deleted.\n",
        "\n",
        "      :param domain: The domain to delete slots from.\n",
        "      :type domain: str\n",
        "      :param slot: The slot to delete.\n",
        "      :type slot: str\n",
        "      \"\"\"\n",
        "      keys = self.state.keys()\n",
        "      if domain is not None:\n",
        "        for key in keys:\n",
        "          if domain in key:\n",
        "            del self.state[key]\n",
        "      elif slot is not None:\n",
        "          if slot in keys:\n",
        "            del self.state[slot]\n",
        "      else:\n",
        "        for key in keys:\n",
        "          del self.state[key]\n",
        "\n",
        "    def reset(self):\n",
        "      \"\"\"\n",
        "      Reset the dialogue state.\n",
        "\n",
        "      Remove all slots from the dialogue state and clear the history.\n",
        "      \"\"\"\n",
        "      keys = list(self.state.keys())\n",
        "      for key in keys:\n",
        "        del self.state[key]\n",
        "\n",
        "      self.history = []\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "      \"\"\"\n",
        "      Return a string representation of the TRIPPYDST object.\n",
        "\n",
        "      :return: A string representation of the object.\n",
        "      :rtype: str\n",
        "      \"\"\"\n",
        "      string = ''\n",
        "      string = string + '\\ndomains: ' + str(self.domains)\n",
        "      string = string + '\\nontology_path: ' + str(self.ontology_path)\n",
        "      string = string + '\\nlabel_maps_path: ' + str(self.label_maps_path)\n",
        "      string = string + '\\nnon_referable_slots: ' + str(self.non_referable_slots)\n",
        "      string = string + '\\nnon_referable_pairs: ' + str(self.non_referable_pairs)\n",
        "      string = string + '\\nfrom_scratch: ' + str(self.from_scratch)\n",
        "      string = string + '\\nslot_list: ' + str(self.slot_list)\n",
        "      string = string + '\\nn_slots: ' + str(self.n_slots)\n",
        "      string = string + '\\nlabel_maps: ' + str(self.label_maps)\n",
        "      string = string + '\\nstate: ' + str(self.state)\n",
        "      string = string + '\\ndevice: ' + str(self.device)\n",
        "      # string = string + '\\nmodel: ' + str(self.model)\n",
        "      string = string + '\\nhistory: ' + str(self.history)\n",
        "      return string"
      ],
      "metadata": {
        "id": "-1xp4e7niBhv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRIPPY"
      ],
      "metadata": {
        "id": "RI0MdOTNiCOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This Module has the TRIPPY model.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# from botiverse.models.BERT.BERT import Bert\n",
        "# from botiverse.models.BERT.config import BERTConfig\n",
        "# from botiverse.models.BERT.utils import LoadPretrainedWeights\n",
        "\n",
        "class TRIPPY(nn.Module):\n",
        "  \"\"\"\n",
        "  TRIPPY (Task-oriented Reasoning and Inference for Pre-trained models with Pre-trained Ypesystem) model.\n",
        "\n",
        "  This class implements the TRIPPY model for task-oriented dialogue understanding and slot filling.\n",
        "\n",
        "  :param n_slots: The number of slots, corresponding to the number of dialogue slots to be filled.\n",
        "  :type n_slots: int\n",
        "  :param hid_dim: The hidden dimension size.\n",
        "  :type hid_dim: int\n",
        "  :param n_oper: The number of operations.\n",
        "  :type n_oper: int\n",
        "  :param dropout: The dropout rate.\n",
        "  :type dropout: float\n",
        "  :param from_scratch: Whether to build the BERT model from scratch or load pre-trained weights, defaults to False.\n",
        "  :type from_scratch: bool\n",
        "  :param BERT_config: The configuration for the BERT model, defaults to BERTConfig().\n",
        "  :type BERT_config: BERTConfig\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, n_slots, hid_dim, n_oper, dropout, from_scratch, BERT_config=BERTConfig(), TRIPPY_config=TRIPPYConfig()):\n",
        "    super(TRIPPY, self).__init__()\n",
        "\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_oper = n_oper\n",
        "    self.n_slots = n_slots\n",
        "\n",
        "    if from_scratch == True:\n",
        "        # Build a BERT model from scratch\n",
        "        self.bert = Bert(BERT_config)\n",
        "        LoadPretrainedWeights(self.bert)\n",
        "    else:\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    aux_dim = 2 * n_slots\n",
        "    self.oper_layers = nn.ModuleList([nn.Linear(hid_dim + aux_dim, n_oper) for _ in range(n_slots)])\n",
        "    self.span_layers = nn.ModuleList([nn.Linear(hid_dim, 2) for _ in range(n_slots)])\n",
        "    self.refer_layers = nn.ModuleList([nn.Linear(hid_dim + aux_dim, n_slots + 1) for _ in range(n_slots)])\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # auxiliary features layers\n",
        "    self.inform_aux_layer = nn.Linear(n_slots, n_slots)\n",
        "    self.ds_aux_layer = nn.Linear(n_slots, n_slots)\n",
        "\n",
        "  def forward(self, ids, mask, token_type_ids, inform_aux_features, ds_aux_features):\n",
        "    \"\"\"\n",
        "    Forward pass of the TRIPPY model.\n",
        "\n",
        "    :param ids: The input token IDs.\n",
        "    :type ids: torch.Tensor, shape [batch size, seq len]\n",
        "    :param mask: The attention mask indicating which tokens are valid.\n",
        "    :type mask: torch.Tensor, shape [batch size, seq len]\n",
        "    :param token_type_ids: The token type IDs.\n",
        "    :type token_type_ids: torch.Tensor, shape [batch size, seq len]\n",
        "    :param inform_aux_features: The auxiliary features for informing slots.\n",
        "    :type inform_aux_features: torch.Tensor, shape [batch size, n_slots]\n",
        "    :param ds_aux_features: The auxiliary features for dialogue state tracking.\n",
        "    :type ds_aux_features: torch.Tensor, shape [batch size, n_slots]\n",
        "    :return: Tuple containing the logits for slot start positions, slot end positions, slot operations, and slot references.\n",
        "    :rtype: tuple(torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor)\n",
        "    \"\"\"\n",
        "\n",
        "    # ids = [batch size, seq len]\n",
        "    # mask = [batch size, seq len]\n",
        "    # token_type_ids = [batch size, seq len]\n",
        "\n",
        "    # sequence_output = [batch size, seq len, hid_dim]\n",
        "    # pooled_output = [batch size, hid_dim]\n",
        "    sequence_output, pooled_output = self.bert(ids,\n",
        "                                               attention_mask=mask,\n",
        "                                               token_type_ids=token_type_ids,\n",
        "                                               return_dict=False\n",
        "                                               )\n",
        "\n",
        "    sequence_output = self.dropout(sequence_output)\n",
        "    pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "    # concatenate the auxiliary features\n",
        "    # pooled_output = [batch size, hid_dim + 2 * n_slots]\n",
        "    # print(pooled_output.shape)\n",
        "    # print(inform_aux_features.shape)\n",
        "    # print(ds_aux_features.shape)\n",
        "    pooled_output = torch.cat((pooled_output, self.inform_aux_layer(inform_aux_features), self.ds_aux_layer(ds_aux_features)), 1)\n",
        "    # print(pooled_output.shape)\n",
        "    # print(\"\\n\\n\\n\\n\")\n",
        "\n",
        "    slots_start_logits = []\n",
        "    slots_end_logits = []\n",
        "    slots_oper_logits = []\n",
        "    slots_refer_logits = []\n",
        "    for slot in range(self.n_slots):\n",
        "\n",
        "      # oper_logits = [batch size, n_oper]\n",
        "      oper_logits = self.oper_layers[slot](pooled_output)\n",
        "\n",
        "      # span_logits = [batch size, seq len, 2]\n",
        "      span_logits = self.span_layers[slot](sequence_output)\n",
        "\n",
        "      # start_logits = [batch size, seq len, 1]\n",
        "      # end_logits = [batch size, seq len, 1]\n",
        "      start_logits, end_logits = span_logits.split(1, dim=-1)\n",
        "\n",
        "      # start_logits = [batch size, seq len]\n",
        "      # end_logits = [batch size, seq len]\n",
        "      start_logits = start_logits.squeeze(-1)\n",
        "      end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "      # refer_logits = [batch size, n_slots + 1]\n",
        "      refer_logits = self.refer_layers[slot](pooled_output)\n",
        "\n",
        "      slots_start_logits.append(start_logits)\n",
        "      slots_end_logits.append(end_logits)\n",
        "      slots_oper_logits.append(oper_logits)\n",
        "      slots_refer_logits.append(refer_logits)\n",
        "\n",
        "    return slots_start_logits, slots_end_logits, slots_oper_logits, slots_refer_logits"
      ],
      "metadata": {
        "id": "UEXJn3UciDBw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils"
      ],
      "metadata": {
        "id": "RTiwPPNniDsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "# from botiverse.models.TRIPPY.config import MULTIWOZ\n",
        "\n",
        "\n",
        "class RawDataInstance():\n",
        "  \"\"\"\n",
        "  Represents a raw data instance.\n",
        "\n",
        "  :param dial_idx: Dialogue index.\n",
        "  :type dial_idx: str\n",
        "  :param turn_idx: Turn index.\n",
        "  :type turn_idx: int\n",
        "  :param user_utter: User utterance.\n",
        "  :type user_utter: str\n",
        "  :param sys_utter: System utterance.\n",
        "  :type sys_utter: str\n",
        "  :param history: Dialogue history.\n",
        "  :type history: list[str]\n",
        "  :param turn_slots: Slots for the current turn.\n",
        "  :type turn_slots: dict[str, str]\n",
        "  :param inform_mem: Informed slots from previous turns.\n",
        "  :type inform_mem: dict[str, list[str]]\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               dial_idx,\n",
        "               turn_idx,\n",
        "               user_utter,\n",
        "               sys_utter,\n",
        "               history,\n",
        "               turn_slots,\n",
        "               inform_mem):\n",
        "    self.dial_idx = dial_idx\n",
        "    self.turn_idx = turn_idx\n",
        "    self.user_utter = user_utter\n",
        "    self.sys_utter = sys_utter\n",
        "    self.history = history\n",
        "    self.turn_slots = turn_slots\n",
        "    self.inform_mem = inform_mem\n",
        "\n",
        "  def __str__(self):\n",
        "    \"\"\"\n",
        "    Return a string representation of the RawDataInstance object.\n",
        "\n",
        "    :return: A string representation of the object.\n",
        "    :rtype: str\n",
        "    \"\"\"\n",
        "    string = ''\n",
        "    string = string + '\\ndial_idx: ' + str(self.dial_idx)\n",
        "    string = string + '\\nturn_idx: ' + str(self.turn_idx)\n",
        "    string = string + '\\nuser_utter: ' + str(self.user_utter)\n",
        "    string = string + '\\nsys_utter: ' + str(self.sys_utter)\n",
        "    string = string + '\\nhistory: ' + str(self.history)\n",
        "    string = string + '\\nturn_slots: ' + str(self.turn_slots)\n",
        "    string = string + '\\ninform_mem: ' + str(self.inform_mem)\n",
        "    return string\n",
        "\n",
        "class DataInstance():\n",
        "  \"\"\"\n",
        "  Represents a processed data instance.\n",
        "\n",
        "  :param ids: Input IDs.\n",
        "  :type ids: list[int]\n",
        "  :param mask: Attention mask.\n",
        "  :type mask: list[int]\n",
        "  :param token_type_ids: Token type IDs.\n",
        "  :type token_type_ids: list[int]\n",
        "  :param spans: Spans.\n",
        "  :type spans: list[int]\n",
        "  :param spans_start: Start positions of spans.\n",
        "  :type spans_start: list[int]\n",
        "  :param spans_end: End positions of spans.\n",
        "  :type spans_end: list[int]\n",
        "  :param padding_len: Padding length.\n",
        "  :type padding_len: int\n",
        "  :param input_tokens: Input tokens.\n",
        "  :type input_tokens: str\n",
        "  :param input: Input text.\n",
        "  :type input: str\n",
        "  :param opers: Slot operations.\n",
        "  :type opers: list[int]\n",
        "  :param target_values: Target slot values.\n",
        "  :type target_values: list[str]\n",
        "  :param last_state: Last dialogue state.\n",
        "  :type last_state: dict[str, str]\n",
        "  :param cur_state: Current dialogue state.\n",
        "  :type cur_state: dict[str, str]\n",
        "  :param refer: Referenced slots.\n",
        "  :type refer: list[int]\n",
        "  :param inform_aux_features: Informed auxiliary features.\n",
        "  :type inform_aux_features: list[float]\n",
        "  :param ds_aux_features: Filled slot auxiliary features.\n",
        "  :type ds_aux_features: list[float]\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               ids,\n",
        "               mask,\n",
        "               token_type_ids,\n",
        "               spans,\n",
        "               spans_start,\n",
        "               spans_end,\n",
        "               padding_len,\n",
        "               input_tokens,\n",
        "               input,\n",
        "               opers,\n",
        "               target_values,\n",
        "               last_state,\n",
        "               cur_state,\n",
        "               refer,\n",
        "               inform_aux_features,\n",
        "               ds_aux_features):\n",
        "    self.ids = ids\n",
        "    self.mask = mask\n",
        "    self.token_type_ids = token_type_ids\n",
        "    self.spans = spans\n",
        "    self.spans_start = spans_start\n",
        "    self.spans_end = spans_end\n",
        "    self.padding_len = padding_len\n",
        "    self.input_tokens = input_tokens\n",
        "    self.input = input\n",
        "    self.opers = opers\n",
        "    self.target_values = target_values\n",
        "    self.last_state = last_state\n",
        "    self.cur_state = cur_state\n",
        "    self.refer = refer\n",
        "    self.inform_aux_features = inform_aux_features\n",
        "    self.ds_aux_features = ds_aux_features\n",
        "\n",
        "  def __str__(self):\n",
        "    \"\"\"\n",
        "    Return a string representation of the DataInstance object.\n",
        "\n",
        "    :return: A string representation of the object.\n",
        "    :rtype: str\n",
        "    \"\"\"\n",
        "    string = ''\n",
        "    string = string + '\\nids: ' + str(self.ids)\n",
        "    string = string + '\\nmask: ' + str(self.mask)\n",
        "    string = string + '\\ntoken_type_ids: ' + str(self.token_type_ids)\n",
        "    string = string + '\\nspans: ' + str(self.spans)\n",
        "    string = string + '\\nspans_start: ' + str(self.spans_start)\n",
        "    string = string + '\\nspans_end: ' + str(self.spans_end)\n",
        "    string = string + '\\npadding_len: ' + str(self.padding_len)\n",
        "    string = string + '\\ninput_tokens: ' + str(self.input_tokens)\n",
        "    string = string + '\\ninput: ' + str(self.input)\n",
        "    string = string + '\\nopers: ' + str(self.opers)\n",
        "    string = string + '\\ntarget_values: ' + str(self.target_values)\n",
        "    string = string + '\\nlast_state: ' + str(self.last_state)\n",
        "    string = string + '\\ncur_state: ' + str(self.cur_state)\n",
        "    string = string + '\\nrefer: ' + str(self.refer)\n",
        "    string = string + '\\ninform_aux_features: ' + str(self.inform_aux_features)\n",
        "    string = string + '\\nds_aux_features: ' + str(self.ds_aux_features)\n",
        "\n",
        "    return string\n",
        "\n",
        "class AverageMeter():\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the average meter.\n",
        "        \"\"\"\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        \"\"\"\n",
        "        Update the average meter with a new value.\n",
        "\n",
        "        :param val: New value.\n",
        "        :type val: float\n",
        "        :param n: Number of instances the value represents.\n",
        "        :type n: int\n",
        "        \"\"\"\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def normalize(text, multiwoz):\n",
        "  \"\"\"\n",
        "  Normalize the given text by converting it to lowercase and splitting it into tokens.\n",
        "\n",
        "  :param text: Input text.\n",
        "  :type text: str\n",
        "  :return: Normalized tokens.\n",
        "  :rtype: list[str]\n",
        "  \"\"\"\n",
        "  text_lower = text.lower()\n",
        "  if multiwoz == True:\n",
        "    text_norm = normalize_text(text_lower) # for mutliwoz only\n",
        "  else:\n",
        "    text_norm = text_lower\n",
        "  text_tok = [tok for tok in map(lambda x: re.sub(\" \", \"\", x), re.split(\"(\\W+)\", text_norm)) if len(tok) > 0]\n",
        "  return text_tok\n",
        "\n",
        "\n",
        "def is_included(value, target):\n",
        "  \"\"\"\n",
        "  Check if the target is included in the value.\n",
        "\n",
        "  :param value: The value to check.\n",
        "  :type value: str\n",
        "  :param target: The target value to search for.\n",
        "  :type target: str\n",
        "  :return: True if the target is included in the value, False otherwise.\n",
        "  :rtype: bool\n",
        "  \"\"\"\n",
        "  included = False\n",
        "\n",
        "  value = [item for item in map(str.strip, re.split(\"(\\W+)\", value)) if len(item) > 0]\n",
        "  target = [item for item in map(str.strip, re.split(\"(\\W+)\", target)) if len(item) > 0]\n",
        "\n",
        "  for i in range(len(value)):\n",
        "    if value[i:i + len(target)] == target:\n",
        "      included = True\n",
        "\n",
        "  return included\n",
        "\n",
        "\n",
        "def included_with_label_maps(value, target, label_maps):\n",
        "  \"\"\"\n",
        "  Check if the value is included in the target or any of its variants based on the label maps.\n",
        "\n",
        "  :param value: The value to check.\n",
        "  :type value: str\n",
        "  :param target: The target value to search for.\n",
        "  :type target: str\n",
        "  :param label_maps: Dictionary of label maps.\n",
        "  :type label_maps: dict[str, list[str]]\n",
        "  :return: True if the value is included in the target or any of its variants, False otherwise.\n",
        "  :rtype: bool\n",
        "  \"\"\"\n",
        "  included = False\n",
        "\n",
        "  variants = [target]\n",
        "  if target in label_maps:\n",
        "    variants += label_maps[target]\n",
        "\n",
        "  for variant in variants:\n",
        "    if value == variant or is_included(value, variant) or is_included(variant, value):\n",
        "      included = True\n",
        "\n",
        "  return included\n",
        "\n",
        "\n",
        "def match_with_label_maps(value, target, label_maps={}):\n",
        "    \"\"\"\n",
        "    Check if the value matches the target or any of its variants based on the label maps.\n",
        "\n",
        "    :param value: The value to check.\n",
        "    :type value: str\n",
        "    :param target: The target value to match against.\n",
        "    :type target: str\n",
        "    :param label_maps: Dictionary of label maps.\n",
        "    :type label_maps: dict[str, list[str]]\n",
        "    :return: True if the value matches the target or any of its variants, False otherwise.\n",
        "    :rtype: bool\n",
        "    \"\"\"\n",
        "    equal = False\n",
        "    if value == target:\n",
        "      equal = True\n",
        "    elif target in label_maps:\n",
        "      for variant in label_maps[target]:\n",
        "        if value == variant:\n",
        "          equal = True\n",
        "\n",
        "    return equal\n",
        "\n",
        "\n",
        "def create_span_output(output_start, output_end, padding_len, input_tokens):\n",
        "  \"\"\"\n",
        "  Create the span output based on the output start and end positions.\n",
        "\n",
        "  :param output_start: Output start positions.\n",
        "  :type output_start: list[int]\n",
        "  :param output_end: Output end positions.\n",
        "  :type output_end: list[int]\n",
        "  :param padding_len: Padding length.\n",
        "  :type padding_len: int\n",
        "  :param input_tokens: Input tokens.\n",
        "  :type input_tokens: str\n",
        "  :return: The created span output.\n",
        "  :rtype: str\n",
        "  \"\"\"\n",
        "  mask = [0] * (len(output_start) - padding_len)\n",
        "\n",
        "  if padding_len > 0:\n",
        "    idx_start = np.argmax(output_start[1:-padding_len]) + 1\n",
        "    idx_end = np.argmax(output_end[1:-padding_len]) + 1\n",
        "  else:\n",
        "    idx_start = np.argmax(output_start[1:]) + 1\n",
        "    idx_end = np.argmax(output_end[1:]) + 1\n",
        "\n",
        "  for mj in range(idx_start, idx_end + 1):\n",
        "    mask[mj] = 1\n",
        "\n",
        "  output_tokens = [x for p, x in enumerate(input_tokens.split()) if mask[p] == 1]\n",
        "  output_tokens = [x for x in output_tokens if x not in ('[CLS]', '[SEP]')]\n",
        "\n",
        "  final_output = ''\n",
        "  for ot in output_tokens:\n",
        "    if ot.startswith('##'):\n",
        "      final_output = final_output + ot[2:]\n",
        "    elif len(ot) == 1 and ot in string.punctuation:\n",
        "      final_output = final_output + ot\n",
        "    elif len(final_output) > 0 and final_output[-1] in string.punctuation:\n",
        "      final_output = final_output + ot\n",
        "    else:\n",
        "      final_output = final_output + \" \" + ot\n",
        "\n",
        "  final_output = final_output.strip()\n",
        "\n",
        "  return final_output\n",
        "\n",
        "\n",
        "def mask_utterance(utter, inform_mem, multiwoz, replace_with='[UNK]'):\n",
        "  \"\"\"\n",
        "  Mask the utterance by replacing the informed values in the inform memory.\n",
        "\n",
        "  :param utter: The utterance to mask.\n",
        "  :type utter: list[str]\n",
        "  :param inform_mem: The inform memory containing slot-value pairs.\n",
        "  :type inform_mem: dict[str, list[str]]\n",
        "  :param replace_with: The replacement token.\n",
        "  :type replace_with: str\n",
        "  :return: The masked utterance.\n",
        "  :rtype: list[str]\n",
        "  \"\"\"\n",
        "  utter = normalize(utter, multiwoz)\n",
        "  for slot, informed_values in inform_mem.items():\n",
        "    for informed_value in informed_values:\n",
        "      informed_tok = normalize(informed_value, multiwoz)\n",
        "      for i in range(len(utter)):\n",
        "        if utter[i:i + len(informed_tok)] == informed_tok:\n",
        "          utter[i:i + len(informed_tok)] = [replace_with] * len(informed_tok)\n",
        "  return utter\n",
        "\n",
        "\n",
        "def normalize_time(text):\n",
        "    \"\"\"\n",
        "    Normalize the time format in the given text (specific to MultiWoz dataset).\n",
        "\n",
        "    :param text: The input text.\n",
        "    :type text: str\n",
        "    :return: The normalized text.\n",
        "    :rtype: str\n",
        "    \"\"\"\n",
        "\n",
        "    # This code is only related to MultiWoz Dataset\n",
        "\n",
        "    text = re.sub(\"(\\d{1})(a\\.?m\\.?|p\\.?m\\.?)\", r\"\\1 \\2\", text) # am/pm without space\n",
        "    text = re.sub(\"(^| )(\\d{1,2}) (a\\.?m\\.?|p\\.?m\\.?)\", r\"\\1\\2:00 \\3\", text) # am/pm short to long form\n",
        "    text = re.sub(\"(^| )(at|from|by|until|after) ?(\\d{1,2}) ?(\\d{2})([^0-9]|$)\", r\"\\1\\2 \\3:\\4\\5\", text) # Missing separator\n",
        "    text = re.sub(\"(^| )(\\d{2})[;.,](\\d{2})\", r\"\\1\\2:\\3\", text) # Wrong separator\n",
        "    text = re.sub(\"(^| )(at|from|by|until|after) ?(\\d{1,2})([;., ]|$)\", r\"\\1\\2 \\3:00\\4\", text) # normalize simple full hour time\n",
        "    text = re.sub(\"(^| )(\\d{1}:\\d{2})\", r\"\\g<1>0\\2\", text) # Add missing leading 0\n",
        "    # Map 12 hour times to 24 hour times\n",
        "    text = re.sub(\"(\\d{2})(:\\d{2}) ?p\\.?m\\.?\", lambda x: str(int(x.groups()[0]) + 12 if int(x.groups()[0]) < 12 else int(x.groups()[0])) + x.groups()[1], text)\n",
        "    text = re.sub(\"(^| )24:(\\d{2})\", r\"\\g<1>00:\\2\", text) # Correct times that use 24 as hour\n",
        "    return text\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    Normalize the text (specific to MultiWoz dataset).\n",
        "\n",
        "    :param text: The input text.\n",
        "    :type text: str\n",
        "    :return: The normalized text.\n",
        "    :rtype: str\n",
        "    \"\"\"\n",
        "\n",
        "    # This code is only related to MultiWoz Dataset\n",
        "\n",
        "    text = normalize_time(text)\n",
        "    text = re.sub(\"n't\", \" not\", text)\n",
        "    text = re.sub(\"(^| )zero(-| )star([s.,? ]|$)\", r\"\\g<1>0 star\\3\", text)\n",
        "    text = re.sub(\"(^| )one(-| )star([s.,? ]|$)\", r\"\\g<1>1 star\\3\", text)\n",
        "    text = re.sub(\"(^| )two(-| )star([s.,? ]|$)\", r\"\\g<1>2 star\\3\", text)\n",
        "    text = re.sub(\"(^| )three(-| )star([s.,? ]|$)\", r\"\\g<1>3 star\\3\", text)\n",
        "    text = re.sub(\"(^| )four(-| )star([s.,? ]|$)\", r\"\\g<1>4 star\\3\", text)\n",
        "    text = re.sub(\"(^| )five(-| )star([s.,? ]|$)\", r\"\\g<1>5 star\\3\", text)\n",
        "    text = re.sub(\"archaelogy\", \"archaeology\", text) # Systematic typo\n",
        "    text = re.sub(\"guesthouse\", \"guest house\", text) # Normalization\n",
        "    text = re.sub(\"(^| )b ?& ?b([.,? ]|$)\", r\"\\1bed and breakfast\\2\", text) # Normalization\n",
        "    text = re.sub(\"bed & breakfast\", \"bed and breakfast\", text) # Normalization\n",
        "    text = re.sub(\"\\t\", \" \", text) # Error\n",
        "    text = re.sub(\"\\n\", \" \", text) # Error\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "JGljLfdLiEgZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# deep_TODS"
      ],
      "metadata": {
        "id": "y0Gj-tIcixed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils"
      ],
      "metadata": {
        "id": "s1wW97HBi_SR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This module ontains utility code used by the deep TODS module\n",
        "such as Natural Language Understanding (NLU), Dialogue State Tracker (DST) ... etc.\n",
        "\"\"\"\n",
        "\n",
        "import random\n",
        "\n",
        "class PriorityDP:\n",
        "    \"\"\"\n",
        "    Dialogue Policy Optimizer that selects the next action based on priority.\n",
        "\n",
        "    This policy selects the action with the highest priority that does not conflict with\n",
        "    the slots already filled in the dialogue state.\n",
        "\n",
        "    \"\"\"\n",
        "    def get_action(self, state, templates_slots):\n",
        "        \"\"\"\n",
        "        Get the next action based on the given dialogue state and available action templates.\n",
        "\n",
        "        :param state: The current dialogue state.\n",
        "        :type state: dict\n",
        "\n",
        "        :param templates_slots: The available different combinations of slots that can be filled by templates.\n",
        "        :type templates_slots: list[tuple]\n",
        "\n",
        "        :return: The index of the selected action template, or None if no action is available.\n",
        "        :rtype: int or None\n",
        "        \"\"\"\n",
        "\n",
        "        filled = []\n",
        "        for slot, value in state.items():\n",
        "          if value is not None:\n",
        "            filled.append(slot)\n",
        "\n",
        "        filled = tuple(sorted(filled))\n",
        "\n",
        "        top_idx = -1\n",
        "        for idx, slots in enumerate(templates_slots):\n",
        "          if all(element not in slots for element in filled):\n",
        "            top_idx = idx\n",
        "            break\n",
        "\n",
        "        action = None\n",
        "        if top_idx != -1:\n",
        "          action = top_idx\n",
        "\n",
        "        return action\n",
        "\n",
        "    def __str__(self):\n",
        "      \"\"\"\n",
        "      Return a string representation of the PriorityDP policy.\n",
        "\n",
        "      :return: A string representation of the PriorityDP policy.\n",
        "      :rtype: str\n",
        "      \"\"\"\n",
        "      string = ''\n",
        "      string = string + '\\nPriorityDP'\n",
        "      return string\n",
        "\n",
        "\n",
        "class RandomDP:\n",
        "    \"\"\"\n",
        "    Dialogue Policy Optimizer that selects the next action randomly.\n",
        "\n",
        "    This policy selects the action randomly from the available action templates\n",
        "    that do not conflict with the slots already filled in the dialogue state.\n",
        "\n",
        "    \"\"\"\n",
        "    def get_action(self, state, templates_slots):\n",
        "        \"\"\"\n",
        "        Get the next action based on the given dialogue state and available action templates.\n",
        "\n",
        "        :param state: The current dialogue state.\n",
        "        :type state: dict\n",
        "\n",
        "        :param templates_slots: The available different combinations of slots that can be filled by templates.\n",
        "        :type templates_slots: list[tuple]\n",
        "\n",
        "        :return: The index of the selected action template, or None if no action is available.\n",
        "        :rtype: int or None\n",
        "        \"\"\"\n",
        "        filled = []\n",
        "        for slot, value in state.items():\n",
        "          if value is not None:\n",
        "            filled.append(slot)\n",
        "\n",
        "        filled = tuple(sorted(filled))\n",
        "\n",
        "        candidates = []\n",
        "        for idx, slots in enumerate(templates_slots):\n",
        "          if all(element not in slots for element in filled):\n",
        "            candidates.append(idx)\n",
        "\n",
        "        # print('candidates', candidates)\n",
        "\n",
        "        max_len = len(candidates)\n",
        "\n",
        "        action = None\n",
        "\n",
        "        if max_len > 0:\n",
        "          action = candidates[random.randint(0, max_len-1)]\n",
        "\n",
        "        return action\n",
        "\n",
        "    def __str__(self):\n",
        "      \"\"\"\n",
        "      Return a string representation of the RandomDP policy.\n",
        "\n",
        "      :return: A string representation of the RandomDP policy.\n",
        "      :rtype: str\n",
        "      \"\"\"\n",
        "      string = ''\n",
        "      string = string + '\\nRandomDP'\n",
        "      return string\n",
        "\n",
        "\n",
        "class TemplateBasedNLG:\n",
        "    \"\"\"\n",
        "    Natural Language Generation module that generates responses based on predefined templates.\n",
        "\n",
        "    This module uses a set of predefined templates containing utterances and corresponding system acts.\n",
        "    Given an index, it generates the corresponding system utterance and system act.\n",
        "\n",
        "    :param templates: The predefined templates for generating responses.\n",
        "    :type templates: list[dict]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, templates):\n",
        "      self.templates = templates\n",
        "      self.templates_slots = []\n",
        "\n",
        "      for template in templates:\n",
        "        self.templates_slots.append(tuple(sorted(template['slots'])))\n",
        "\n",
        "    def get_templates(self):\n",
        "      \"\"\"\n",
        "      Get the predefined templates.\n",
        "\n",
        "      :return: The predefined templates.\n",
        "      :rtype: list[dict]\n",
        "      \"\"\"\n",
        "      return self.templates\n",
        "\n",
        "    def get_templates_slots(self):\n",
        "      \"\"\"\n",
        "      Get the slots associated with the predefined templates.\n",
        "\n",
        "      :return: The slots associated with the predefined templates.\n",
        "      :rtype: list[tuple]\n",
        "      \"\"\"\n",
        "      return self.templates_slots\n",
        "\n",
        "    def generate(self, idx):\n",
        "      \"\"\"\n",
        "      Generate a response based on the given index.\n",
        "\n",
        "      :param idx: The index of the template to generate a response from.\n",
        "      :type idx: int\n",
        "\n",
        "      :return: The generated system utterance and system act.\n",
        "      :rtype: tuple[str, list] or None\n",
        "      \"\"\"\n",
        "\n",
        "      if idx < 0 or idx >= len(self.templates):\n",
        "        return None, None\n",
        "\n",
        "      return self.templates[idx]['utterance'], self.templates[idx]['system_act']\n",
        "\n",
        "    def __str__(self):\n",
        "      \"\"\"\n",
        "      Return a string representation of the TemplateBasedNLG module.\n",
        "\n",
        "      :return: A string representation of the TemplateBasedNLG module.\n",
        "      :rtype: str\n",
        "      \"\"\"\n",
        "      string = ''\n",
        "      string = string + '\\ntemplates: ' + str(self.templates)\n",
        "      string = string + '\\ntemplates_slots: ' + str(self.templates_slots)\n",
        "      return string"
      ],
      "metadata": {
        "id": "jPPmKlpTi0fU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## deep_TODS"
      ],
      "metadata": {
        "id": "eZeVgmAYjA1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This module contains the base code and interface of deep TODS .\n",
        "\"\"\"\n",
        "\n",
        "# from botiverse.bots.deep_TODS.utils import RandomDP, PriorityDP, TemplateBasedNLG\n",
        "# from botiverse.models.TRIPPY.config import TRIPPYConfig\n",
        "# from botiverse.models.TRIPPY.TRIPPY_DST import TRIPPYDST\n",
        "\n",
        "import random\n",
        "\n",
        "class DeepTODS:\n",
        "  \"\"\"\n",
        "  Instantiate a Deep Task Oriented Dialogue System chat bot.\n",
        "  It aims to assist the user in completing certain tasks in specific domains.\n",
        "  The chat bot can use a Deep learning approach for training and inference.\n",
        "\n",
        "  :param name: The chatbot's name.\n",
        "  :type name: str\n",
        "\n",
        "  :param domains: List of domain names.\n",
        "  :type domains: list[str]\n",
        "\n",
        "  :param ontology_path: Path to the ontology file.\n",
        "  :type ontology_path: str\n",
        "\n",
        "  :param label_maps_path: Path to the label maps file.\n",
        "  :type label_maps_path: str\n",
        "\n",
        "  :param policy: The dialogue policy to be used ('Random' or 'Priority').\n",
        "  :type policy: str\n",
        "\n",
        "  :param start: List of initial system utterances and corresponding system acts.\n",
        "  :type start: list[dict]\n",
        "\n",
        "  :param templates: The predefined templates for generating responses.\n",
        "  :type templates: list[dict]\n",
        "\n",
        "  :param non_referable_slots: List of non-referable slots, defaults to an empty list.\n",
        "  :type non_referable_slots: list[str]\n",
        "\n",
        "  :param non_referable_pairs: List of non-referable slot-value pairs, defaults to an empty list.\n",
        "  :type non_referable_pairs: list[tuple]\n",
        "\n",
        "  :param from_scratch: Indicates whether to use BERT model implemented from scratch in the library, defaults to False.\n",
        "  :type from_scratch: bool\n",
        "  \"\"\"\n",
        "  def __init__(self, name, domains, ontology_path, label_maps_path, policy, start, templates, non_referable_slots=[], non_referable_pairs=[], from_scratch=False, BERT_config=BERTConfig(), TRIPPY_config=TRIPPYConfig()):\n",
        "    self.name = name\n",
        "    self.domains = domains\n",
        "    self.policy = policy\n",
        "    self.start = start\n",
        "    self.is_start = True\n",
        "    self.dst = TRIPPYDST(domains, ontology_path, label_maps_path, non_referable_slots, non_referable_pairs, from_scratch, BERT_config, TRIPPY_config)\n",
        "    self.dpo = RandomDP() if policy == 'Random' else PriorityDP() if policy == 'Priority' else None\n",
        "    self.nlg = TemplateBasedNLG(templates)\n",
        "    self.sys_utter = ''\n",
        "    self.inform_mem = {}\n",
        "\n",
        "  def train(self, train_path, dev_path, test_path, model_path):\n",
        "    \"\"\"\n",
        "    Train the chatbot model with the given training data.\n",
        "\n",
        "    :param train_path: Path to the training data file.\n",
        "    :type train_path: str\n",
        "\n",
        "    :param dev_path: Path to the development data file.\n",
        "    :type dev_path: str\n",
        "\n",
        "    :param test_path: Path to the testing data file.\n",
        "    :type test_path: str\n",
        "\n",
        "    :param model_path: Path to save the trained model.\n",
        "    :type model_path: str\n",
        "    \"\"\"\n",
        "    self.dst.train(train_path, dev_path, test_path, model_path)\n",
        "\n",
        "  def load_dst_model(self, model_path, test_path=None):\n",
        "    \"\"\"\n",
        "    Load a trained DST model from the given path.\n",
        "\n",
        "    :param model_path: Path to the trained DST model.\n",
        "    :type model_path: str\n",
        "\n",
        "    :param test_path: Path to the testing data file, if applicable, defaults to None.\n",
        "    :type test_path: str\n",
        "    \"\"\"\n",
        "    self.dst.load_model(model_path, test_path)\n",
        "\n",
        "  def infer(self, user_utter):\n",
        "    \"\"\"\n",
        "    Infer a suitable response to the user's utterance.\n",
        "\n",
        "    :param user_utter: The user's input utterance.\n",
        "    :type user_utter: str\n",
        "\n",
        "    :return: The chatbot's response.\n",
        "    :rtype: str\n",
        "    \"\"\"\n",
        "    response = None\n",
        "\n",
        "    if self.is_start and len(self.start) > 0:\n",
        "      temp = self.start[random.randint(0, len(self.start)-1)]\n",
        "      response, inform_mem = temp['utterance'], temp['system_act']\n",
        "      self.sys_utter = response\n",
        "      self.inform_mem = inform_mem\n",
        "    else:\n",
        "      state = self.dst.update_state(self.sys_utter, user_utter, self.inform_mem)\n",
        "      action = self.dpo.get_action(state, self.nlg.get_templates_slots())\n",
        "      if action is not None:\n",
        "        response, inform_mem = self.nlg.generate(action)\n",
        "        self.sys_utter = response\n",
        "        self.inform_mem = inform_mem\n",
        "\n",
        "    self.is_start = False\n",
        "    return response\n",
        "\n",
        "  def suggest(self, template):\n",
        "    \"\"\"\n",
        "    Set the system utterance and system act to suggest a specific response.\n",
        "\n",
        "    :param template: The template containing the suggested system utterance and system act.\n",
        "    :type template: dict\n",
        "    \"\"\"\n",
        "    self.sys_utter = template['utterance']\n",
        "    self.inform_mem = template['system_act']\n",
        "\n",
        "  def get_dialogue_state(self):\n",
        "    \"\"\"\n",
        "    Get the dialogue state.\n",
        "\n",
        "    :return: The dialogue state.\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "    return self.dst.get_dialogue_state()\n",
        "\n",
        "  def delete_slots(self, domain=None, slot=None):\n",
        "    \"\"\"\n",
        "    Delete slots from the dialogue state.\n",
        "\n",
        "    Note that:\n",
        "    if domain!=None will delete all slots in that domain.\n",
        "    if slot!=None will delete that slot.\n",
        "    if both are None will delete all slots in all domains.\n",
        "\n",
        "    :param domain: The domain from which to delete slots, defaults to None.\n",
        "    :type domain: str\n",
        "\n",
        "    :param slot: The slot to delete, defaults to None.\n",
        "    :type slot: str\n",
        "    \"\"\"\n",
        "    self.dst.delete_slots(domain, slot)\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the chatbot's state.\n",
        "    \"\"\"\n",
        "    self.dst.reset()\n",
        "    self.sys_utter = ''\n",
        "    self.inform_mem = {}\n",
        "    self.domain = self.domains[0]\n",
        "    self.is_start = True\n",
        "\n",
        "  def __str__(self):\n",
        "    \"\"\"\n",
        "    Return a string representation of the chatbot.\n",
        "\n",
        "    :return: A string representation of the chatbot.\n",
        "    :rtype: str\n",
        "    \"\"\"\n",
        "    string = ''\n",
        "    string = string + '\\nname: ' + str(self.name)\n",
        "    string = string + '\\ndomains: ' + str(self.domains)\n",
        "    string = string + '\\npolicy: ' + str(self.policy)\n",
        "    string = string + '\\nstart: ' + str(self.start)\n",
        "    string = string + '\\nis_start: ' + str(self.is_start)\n",
        "    string = string + '\\n\\ndst: ' + str(self.dst)\n",
        "    string = string + '\\n\\ndpo: ' + str(self.dpo)\n",
        "    string = string + '\\n\\nnlg: ' + str(self.nlg)\n",
        "    string = string + '\\n\\nsys_utter: ' + str(self.sys_utter)\n",
        "    string = string + '\\ninform_mem: ' + str(self.inform_mem)\n",
        "    return string"
      ],
      "metadata": {
        "id": "DAEnFYYCjClM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo"
      ],
      "metadata": {
        "id": "AERiu-AkjIMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ],
      "metadata": {
        "id": "x7gpppja-weh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHATBOT_NAME = 'Tody'\n",
        "\n",
        "DOMAINS = [\"restaurant\"]\n",
        "\n",
        "ONTOLOGY_PATH = './Woz2/fixed/ontology.json'\n",
        "\n",
        "LABEL_MAPS_PATH = './Woz2/fixed/label_maps.json'\n",
        "\n",
        "TRAIN_DATA_PATH = './Woz2/fixed/train_dials.json'\n",
        "\n",
        "DEV_DATA_PATH = './Woz2/fixed/dev_dials.json'\n",
        "\n",
        "TEST_DATA_PATH = './Woz2/fixed/test_dials.json'\n",
        "\n",
        "POLICY = 'Priority' # Priority or Random\n",
        "\n",
        "START = [\n",
        "    {\n",
        "        'utterance': 'Hi I am Tody, I can help you reserve a restaurant?',\n",
        "        'slots': [],\n",
        "        'system_act': {}\n",
        "    }\n",
        "]\n",
        "\n",
        "TEMPLATES = [\n",
        "    {\n",
        "        'utterance': 'what area do you want?',\n",
        "        'slots': ['restaurant-area'],\n",
        "        'system_act': {}\n",
        "    },\n",
        "    {\n",
        "        'utterance': 'what is your preferred price range?',\n",
        "        'slots': ['restaurant-price_range'],\n",
        "        'system_act': {}\n",
        "    },\n",
        "    {\n",
        "        'utterance': 'What kind of food do you want to eat?',\n",
        "        'slots': ['restaurant-food'],\n",
        "        'system_act': {}\n",
        "    }\n",
        "\n",
        "]\n",
        "\n",
        "NON_REFERABLE_SLOTS = []\n",
        "\n",
        "NON_REFERABLE_PAIRS = []\n",
        "\n",
        "FROM_SCRATCH = True\n",
        "\n",
        "MODEL_PATH = './Models/model.pt'\n",
        "\n",
        "TRIPPY_CONFIG = TRIPPYConfig(multiwoz=False, epochs=5)\n",
        "\n",
        "BERT_CONFIG = BERTConfig()"
      ],
      "metadata": {
        "id": "KFNex6XNjHiy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create chatbot"
      ],
      "metadata": {
        "id": "nN2fVS6F-zo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tods = DeepTODS(CHATBOT_NAME, DOMAINS, ONTOLOGY_PATH, LABEL_MAPS_PATH, POLICY, START, TEMPLATES, NON_REFERABLE_SLOTS, NON_REFERABLE_PAIRS, FROM_SCRATCH, BERT_CONFIG, TRIPPY_CONFIG)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUjWVWDwlAuP",
        "outputId": "7bfb5119-534e-46f3-9651-99b79bbe8c72"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train the chatbot"
      ],
      "metadata": {
        "id": "nM1AWHHX-1fG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tods.train(TRAIN_DATA_PATH, DEV_DATA_PATH, TEST_DATA_PATH, MODEL_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXE8j07VlChR",
        "outputId": "4ff75074-5286-4653-b664-cfc71cc816fb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing train set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2536/2536 [00:05<00:00, 481.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing dev set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 830/830 [00:01<00:00, 801.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1646/1646 [00:02<00:00, 714.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0 ---------------------------------------------------------------\n",
            "Training the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:52<00:00,  1.52it/s, loss=85.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model on dev set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 830/830 [00:12<00:00, 66.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joint Goal Acc: 0.6397590361445783\n",
            "Per Slot Acc: {'restaurant-area': 0.8879518072289156, 'restaurant-food': 0.8674698795180723, 'restaurant-price_range': 0.844578313253012}\n",
            "Macro F1 Score: 0.6629450218742398\n",
            "All f1 score = [0.94145902 0.         0.94623656 0.76408451]\n",
            "\n",
            "Epoch: 1 ---------------------------------------------------------------\n",
            "Training the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:53<00:00,  1.50it/s, loss=19.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model on dev set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 830/830 [00:12<00:00, 64.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joint Goal Acc: 0.8024096385542169\n",
            "Per Slot Acc: {'restaurant-area': 0.946987951807229, 'restaurant-food': 0.9240963855421687, 'restaurant-price_range': 0.908433734939759}\n",
            "Macro F1 Score: 0.8569038731906121\n",
            "All f1 score = [0.96505212 0.64583333 0.97120159 0.84552846]\n",
            "\n",
            "Epoch: 2 ---------------------------------------------------------------\n",
            "Training the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:54<00:00,  1.47it/s, loss=9.38]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model on dev set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 830/830 [00:12<00:00, 64.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joint Goal Acc: 0.8409638554216867\n",
            "Per Slot Acc: {'restaurant-area': 0.9530120481927711, 'restaurant-food': 0.9578313253012049, 'restaurant-price_range': 0.9192771084337349}\n",
            "Macro F1 Score: 0.9056948296028039\n",
            "All f1 score = [0.98000615 0.72727273 0.9851925  0.93030794]\n",
            "\n",
            "Epoch: 3 ---------------------------------------------------------------\n",
            "Training the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:55<00:00,  1.44it/s, loss=5.14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model on dev set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 830/830 [00:11<00:00, 71.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joint Goal Acc: 0.8819277108433735\n",
            "Per Slot Acc: {'restaurant-area': 0.9662650602409638, 'restaurant-food': 0.9734939759036144, 'restaurant-price_range': 0.9349397590361446}\n",
            "Macro F1 Score: 0.9367088980586386\n",
            "All f1 score = [0.98426412 0.83333333 0.98711596 0.94212219]\n",
            "\n",
            "Epoch: 4 ---------------------------------------------------------------\n",
            "Training the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:56<00:00,  1.43it/s, loss=2.81]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model on dev set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 830/830 [00:12<00:00, 65.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joint Goal Acc: 0.8963855421686747\n",
            "Per Slot Acc: {'restaurant-area': 0.9710843373493976, 'restaurant-food': 0.9795180722891567, 'restaurant-price_range': 0.9409638554216867}\n",
            "Macro F1 Score: 0.9392466766749592\n",
            "All f1 score = [0.98516687 0.83636364 0.99011858 0.94533762]\n",
            "Loading best model on dev set...\n",
            "Evaluating the model on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1646/1646 [00:27<00:00, 60.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joint Goal Acc: 0.9009720534629405\n",
            "Per Slot Acc: {'restaurant-area': 0.9939246658566221, 'restaurant-food': 0.9586877278250304, 'restaurant-price_range': 0.945321992709599}\n",
            "Macro F1 Score: 0.9537028346064818\n",
            "All f1 score = [0.98610002 0.88301887 0.98693759 0.95875486]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load saved model"
      ],
      "metadata": {
        "id": "soaqWfYk-7DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tods.load_dst_model(MODEL_PATH, TEST_DATA_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXKYKWEHld5I",
        "outputId": "b24f8803-a754-4afa-878a-278fe92f9746"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "Preprocessing the data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1646/1646 [00:01<00:00, 1449.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model on the data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1646/1646 [00:24<00:00, 67.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joint Goal Acc: 0.9009720534629405\n",
            "Per Slot Acc: {'restaurant-area': 0.9939246658566221, 'restaurant-food': 0.9586877278250304, 'restaurant-price_range': 0.945321992709599}\n",
            "Macro F1 Score: 0.9537028346064818\n",
            "All f1 score = [0.98610002 0.88301887 0.98693759 0.95875486]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# infer"
      ],
      "metadata": {
        "id": "IR7bUg-EAJao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tods.reset()\n",
        "print(tods.get_dialogue_state())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK1phQAjP7FL",
        "outputId": "e7f7c9a7-29a4-4337-dc11-eeb035a45a2e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = tods.infer('')\n",
        "print(tods.get_dialogue_state())\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FERMNnrnAH0e",
        "outputId": "5f5ed425-260f-4d5a-cf6d-ebf67a3860b7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n",
            "Hi I am Tody, I can help you reserve a restaurant?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = tods.infer('Hi, I want to book a table in city center.')\n",
        "print(tods.get_dialogue_state())\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIk89wuWAPmi",
        "outputId": "7ce2655d-2c66-4e19-f143-92df0152e0ce"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'restaurant-area': 'center'}\n",
            "what is your preferred price range?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = tods.infer('a cheap egyptain restaurant.')\n",
        "print(tods.get_dialogue_state())\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XPdkN4wATS7",
        "outputId": "f07bd513-8b43-4a86-f4cc-b53e23ad0508"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'restaurant-area': 'center', 'restaurant-food': 'egyptain', 'restaurant-price_range': 'cheap'}\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PBPJt3iKQW5A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}