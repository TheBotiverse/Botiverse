{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botiverse import chat_gui\n",
    "from botiverse.TODS.DNN_TODS import DNNTODS\n",
    "import gdown\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download necessary files to run the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ontology already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Download ontology\n",
    "f_id = '1rPmf2RFT5BiFOrc96zqAjZiBUyZEjxbu'\n",
    "file_url = f'https://drive.google.com/uc?export=download&confirm=pbef&id={f_id}'\n",
    "output_file = 'ontology.json'\n",
    "if not os.path.exists(output_file):\n",
    "    gdown.download(file_url, output_file)\n",
    "    print('Ontology downloaded successfully.')\n",
    "else:\n",
    "    print('Ontology already exists. Skipping download.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label maps already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Download label maps\n",
    "f_id = '1PBKA6PyMDyQMYqmMycMb2rSeU1RHY6OF'\n",
    "file_url = f'https://drive.google.com/uc?export=download&confirm=pbef&id={f_id}'\n",
    "output_file = 'label_maps.json'\n",
    "if not os.path.exists(output_file):\n",
    "    gdown.download(file_url, output_file)\n",
    "    print('Label maps downloaded successfully.')\n",
    "else:\n",
    "    print('Label maps already exists. Skipping download.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Download DST weights trained on MultiWOZ\n",
    "f_id = '1P88oxRs3ZolIS66nCXOyW9QmlOI32dz9'\n",
    "file_url = f'https://drive.google.com/uc?export=download&confirm=pbef&id={f_id}'\n",
    "output_file = 'model.pt'\n",
    "if not os.path.exists(output_file):\n",
    "    gdown.download(file_url, output_file)\n",
    "    print('Model downloaded successfully.')\n",
    "else:\n",
    "    print('Model already exists. Skipping download.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHATBOT_NAME = 'Tody'\n",
    "\n",
    "DOMAINS = [\"hotel\", \"train\", \"restaurant\", \"attraction\", \"taxi\"]\n",
    "\n",
    "ONTOLOGY_PATH = './ontology.json'\n",
    "\n",
    "LABEL_MAPS_PATH = './label_maps.json'\n",
    "\n",
    "POLICY = 'Priority' # Priority or Random\n",
    "\n",
    "START = [\n",
    "    {\n",
    "        'utterance': 'Hi I am Tody, I can help you reserve a hottel or a taxi?',\n",
    "        'slots': [],\n",
    "        'system_act': {}\n",
    "    }\n",
    "]\n",
    "\n",
    "TEMPLATES = [\n",
    "    {\n",
    "        'utterance': 'Do you have a price reference?',\n",
    "        'slots': ['hotel-pricerange'],\n",
    "        'system_act': {}\n",
    "    },\n",
    "    {\n",
    "        'utterance': 'What day would you like to stay and how many days?',\n",
    "        'slots': ['hotel-book_day', 'hotel-book_stay'],\n",
    "        'system_act': {}\n",
    "    },\n",
    "    {\n",
    "        'utterance': 'Which area of town do you prefer to stay in?',\n",
    "        'slots': ['hotel-area'],\n",
    "        'system_act': {}\n",
    "    },\n",
    "    {\n",
    "        'utterance': 'I have one called the continental hotel that has 5 stars.',\n",
    "        'slots': ['hotel-name', 'hotel-stars'],\n",
    "        'system_act': {'hotel-stars': ['5'], 'hotel-name': ['continental hotel']}\n",
    "    },\n",
    "    {\n",
    "        'utterance': 'Do you need parking or internet?',\n",
    "        'slots': ['hotel-parking', 'hotel-internet'],\n",
    "        'system_act': {}\n",
    "    },\n",
    "    {\n",
    "        'utterance': 'How many people is the booking for?',\n",
    "        'slots': ['hotel-book_people'],\n",
    "        'system_act': {}\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'utterance': 'What is your desired destination?',\n",
    "        'slots': ['taxi-destination'],\n",
    "        'system_act': {}\n",
    "    },\n",
    "    {\n",
    "        'utterance': 'From where you will be departing, and what time do you want to leave?',\n",
    "        'slots': ['taxi-departure', 'taxi-leaveAt'],\n",
    "        'system_act': {}\n",
    "    }\n",
    "]\n",
    "\n",
    "NON_REFERABLE_SLOTS = ['hotel-stars', 'hotel-internet', 'hotel-parking']\n",
    "\n",
    "NON_REFERABLE_PAIRS = [('hotel-book_people','hotel-book_stay'), ('restaurant-book_people','hotel-book_stay')]\n",
    "\n",
    "FROM_SCRATCH = True\n",
    "\n",
    "MODEL_PATH = './model.pt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model & Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight -> embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight -> embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight -> embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight -> embeddings.layer_norm.weight\n",
      "embeddings.LayerNorm.bias -> embeddings.layer_norm.bias\n",
      "encoder.layer.0.attention.self.query.weight -> encoder.0.self_attention.w_q.weight\n",
      "encoder.layer.0.attention.self.query.bias -> encoder.0.self_attention.w_q.bias\n",
      "encoder.layer.0.attention.self.key.weight -> encoder.0.self_attention.w_k.weight\n",
      "encoder.layer.0.attention.self.key.bias -> encoder.0.self_attention.w_k.bias\n",
      "encoder.layer.0.attention.self.value.weight -> encoder.0.self_attention.w_v.weight\n",
      "encoder.layer.0.attention.self.value.bias -> encoder.0.self_attention.w_v.bias\n",
      "encoder.layer.0.attention.output.dense.weight -> encoder.0.self_attention.w_o.weight\n",
      "encoder.layer.0.attention.output.dense.bias -> encoder.0.self_attention.w_o.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight -> encoder.0.self_layer_norm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias -> encoder.0.self_layer_norm.bias\n",
      "encoder.layer.0.intermediate.dense.weight -> encoder.0.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.0.intermediate.dense.bias -> encoder.0.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.0.output.dense.weight -> encoder.0.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.0.output.dense.bias -> encoder.0.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.0.output.LayerNorm.weight -> encoder.0.ffn_layer_norm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias -> encoder.0.ffn_layer_norm.bias\n",
      "encoder.layer.1.attention.self.query.weight -> encoder.1.self_attention.w_q.weight\n",
      "encoder.layer.1.attention.self.query.bias -> encoder.1.self_attention.w_q.bias\n",
      "encoder.layer.1.attention.self.key.weight -> encoder.1.self_attention.w_k.weight\n",
      "encoder.layer.1.attention.self.key.bias -> encoder.1.self_attention.w_k.bias\n",
      "encoder.layer.1.attention.self.value.weight -> encoder.1.self_attention.w_v.weight\n",
      "encoder.layer.1.attention.self.value.bias -> encoder.1.self_attention.w_v.bias\n",
      "encoder.layer.1.attention.output.dense.weight -> encoder.1.self_attention.w_o.weight\n",
      "encoder.layer.1.attention.output.dense.bias -> encoder.1.self_attention.w_o.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight -> encoder.1.self_layer_norm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias -> encoder.1.self_layer_norm.bias\n",
      "encoder.layer.1.intermediate.dense.weight -> encoder.1.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.1.intermediate.dense.bias -> encoder.1.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.1.output.dense.weight -> encoder.1.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.1.output.dense.bias -> encoder.1.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.1.output.LayerNorm.weight -> encoder.1.ffn_layer_norm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias -> encoder.1.ffn_layer_norm.bias\n",
      "encoder.layer.2.attention.self.query.weight -> encoder.2.self_attention.w_q.weight\n",
      "encoder.layer.2.attention.self.query.bias -> encoder.2.self_attention.w_q.bias\n",
      "encoder.layer.2.attention.self.key.weight -> encoder.2.self_attention.w_k.weight\n",
      "encoder.layer.2.attention.self.key.bias -> encoder.2.self_attention.w_k.bias\n",
      "encoder.layer.2.attention.self.value.weight -> encoder.2.self_attention.w_v.weight\n",
      "encoder.layer.2.attention.self.value.bias -> encoder.2.self_attention.w_v.bias\n",
      "encoder.layer.2.attention.output.dense.weight -> encoder.2.self_attention.w_o.weight\n",
      "encoder.layer.2.attention.output.dense.bias -> encoder.2.self_attention.w_o.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight -> encoder.2.self_layer_norm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias -> encoder.2.self_layer_norm.bias\n",
      "encoder.layer.2.intermediate.dense.weight -> encoder.2.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.2.intermediate.dense.bias -> encoder.2.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.2.output.dense.weight -> encoder.2.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.2.output.dense.bias -> encoder.2.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.2.output.LayerNorm.weight -> encoder.2.ffn_layer_norm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias -> encoder.2.ffn_layer_norm.bias\n",
      "encoder.layer.3.attention.self.query.weight -> encoder.3.self_attention.w_q.weight\n",
      "encoder.layer.3.attention.self.query.bias -> encoder.3.self_attention.w_q.bias\n",
      "encoder.layer.3.attention.self.key.weight -> encoder.3.self_attention.w_k.weight\n",
      "encoder.layer.3.attention.self.key.bias -> encoder.3.self_attention.w_k.bias\n",
      "encoder.layer.3.attention.self.value.weight -> encoder.3.self_attention.w_v.weight\n",
      "encoder.layer.3.attention.self.value.bias -> encoder.3.self_attention.w_v.bias\n",
      "encoder.layer.3.attention.output.dense.weight -> encoder.3.self_attention.w_o.weight\n",
      "encoder.layer.3.attention.output.dense.bias -> encoder.3.self_attention.w_o.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight -> encoder.3.self_layer_norm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias -> encoder.3.self_layer_norm.bias\n",
      "encoder.layer.3.intermediate.dense.weight -> encoder.3.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.3.intermediate.dense.bias -> encoder.3.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.3.output.dense.weight -> encoder.3.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.3.output.dense.bias -> encoder.3.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.3.output.LayerNorm.weight -> encoder.3.ffn_layer_norm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias -> encoder.3.ffn_layer_norm.bias\n",
      "encoder.layer.4.attention.self.query.weight -> encoder.4.self_attention.w_q.weight\n",
      "encoder.layer.4.attention.self.query.bias -> encoder.4.self_attention.w_q.bias\n",
      "encoder.layer.4.attention.self.key.weight -> encoder.4.self_attention.w_k.weight\n",
      "encoder.layer.4.attention.self.key.bias -> encoder.4.self_attention.w_k.bias\n",
      "encoder.layer.4.attention.self.value.weight -> encoder.4.self_attention.w_v.weight\n",
      "encoder.layer.4.attention.self.value.bias -> encoder.4.self_attention.w_v.bias\n",
      "encoder.layer.4.attention.output.dense.weight -> encoder.4.self_attention.w_o.weight\n",
      "encoder.layer.4.attention.output.dense.bias -> encoder.4.self_attention.w_o.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight -> encoder.4.self_layer_norm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias -> encoder.4.self_layer_norm.bias\n",
      "encoder.layer.4.intermediate.dense.weight -> encoder.4.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.4.intermediate.dense.bias -> encoder.4.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.4.output.dense.weight -> encoder.4.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.4.output.dense.bias -> encoder.4.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.4.output.LayerNorm.weight -> encoder.4.ffn_layer_norm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias -> encoder.4.ffn_layer_norm.bias\n",
      "encoder.layer.5.attention.self.query.weight -> encoder.5.self_attention.w_q.weight\n",
      "encoder.layer.5.attention.self.query.bias -> encoder.5.self_attention.w_q.bias\n",
      "encoder.layer.5.attention.self.key.weight -> encoder.5.self_attention.w_k.weight\n",
      "encoder.layer.5.attention.self.key.bias -> encoder.5.self_attention.w_k.bias\n",
      "encoder.layer.5.attention.self.value.weight -> encoder.5.self_attention.w_v.weight\n",
      "encoder.layer.5.attention.self.value.bias -> encoder.5.self_attention.w_v.bias\n",
      "encoder.layer.5.attention.output.dense.weight -> encoder.5.self_attention.w_o.weight\n",
      "encoder.layer.5.attention.output.dense.bias -> encoder.5.self_attention.w_o.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight -> encoder.5.self_layer_norm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias -> encoder.5.self_layer_norm.bias\n",
      "encoder.layer.5.intermediate.dense.weight -> encoder.5.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.5.intermediate.dense.bias -> encoder.5.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.5.output.dense.weight -> encoder.5.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.5.output.dense.bias -> encoder.5.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.5.output.LayerNorm.weight -> encoder.5.ffn_layer_norm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias -> encoder.5.ffn_layer_norm.bias\n",
      "encoder.layer.6.attention.self.query.weight -> encoder.6.self_attention.w_q.weight\n",
      "encoder.layer.6.attention.self.query.bias -> encoder.6.self_attention.w_q.bias\n",
      "encoder.layer.6.attention.self.key.weight -> encoder.6.self_attention.w_k.weight\n",
      "encoder.layer.6.attention.self.key.bias -> encoder.6.self_attention.w_k.bias\n",
      "encoder.layer.6.attention.self.value.weight -> encoder.6.self_attention.w_v.weight\n",
      "encoder.layer.6.attention.self.value.bias -> encoder.6.self_attention.w_v.bias\n",
      "encoder.layer.6.attention.output.dense.weight -> encoder.6.self_attention.w_o.weight\n",
      "encoder.layer.6.attention.output.dense.bias -> encoder.6.self_attention.w_o.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight -> encoder.6.self_layer_norm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias -> encoder.6.self_layer_norm.bias\n",
      "encoder.layer.6.intermediate.dense.weight -> encoder.6.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.6.intermediate.dense.bias -> encoder.6.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.6.output.dense.weight -> encoder.6.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.6.output.dense.bias -> encoder.6.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.6.output.LayerNorm.weight -> encoder.6.ffn_layer_norm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias -> encoder.6.ffn_layer_norm.bias\n",
      "encoder.layer.7.attention.self.query.weight -> encoder.7.self_attention.w_q.weight\n",
      "encoder.layer.7.attention.self.query.bias -> encoder.7.self_attention.w_q.bias\n",
      "encoder.layer.7.attention.self.key.weight -> encoder.7.self_attention.w_k.weight\n",
      "encoder.layer.7.attention.self.key.bias -> encoder.7.self_attention.w_k.bias\n",
      "encoder.layer.7.attention.self.value.weight -> encoder.7.self_attention.w_v.weight\n",
      "encoder.layer.7.attention.self.value.bias -> encoder.7.self_attention.w_v.bias\n",
      "encoder.layer.7.attention.output.dense.weight -> encoder.7.self_attention.w_o.weight\n",
      "encoder.layer.7.attention.output.dense.bias -> encoder.7.self_attention.w_o.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight -> encoder.7.self_layer_norm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias -> encoder.7.self_layer_norm.bias\n",
      "encoder.layer.7.intermediate.dense.weight -> encoder.7.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.7.intermediate.dense.bias -> encoder.7.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.7.output.dense.weight -> encoder.7.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.7.output.dense.bias -> encoder.7.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.7.output.LayerNorm.weight -> encoder.7.ffn_layer_norm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias -> encoder.7.ffn_layer_norm.bias\n",
      "encoder.layer.8.attention.self.query.weight -> encoder.8.self_attention.w_q.weight\n",
      "encoder.layer.8.attention.self.query.bias -> encoder.8.self_attention.w_q.bias\n",
      "encoder.layer.8.attention.self.key.weight -> encoder.8.self_attention.w_k.weight\n",
      "encoder.layer.8.attention.self.key.bias -> encoder.8.self_attention.w_k.bias\n",
      "encoder.layer.8.attention.self.value.weight -> encoder.8.self_attention.w_v.weight\n",
      "encoder.layer.8.attention.self.value.bias -> encoder.8.self_attention.w_v.bias\n",
      "encoder.layer.8.attention.output.dense.weight -> encoder.8.self_attention.w_o.weight\n",
      "encoder.layer.8.attention.output.dense.bias -> encoder.8.self_attention.w_o.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight -> encoder.8.self_layer_norm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias -> encoder.8.self_layer_norm.bias\n",
      "encoder.layer.8.intermediate.dense.weight -> encoder.8.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.8.intermediate.dense.bias -> encoder.8.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.8.output.dense.weight -> encoder.8.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.8.output.dense.bias -> encoder.8.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.8.output.LayerNorm.weight -> encoder.8.ffn_layer_norm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias -> encoder.8.ffn_layer_norm.bias\n",
      "encoder.layer.9.attention.self.query.weight -> encoder.9.self_attention.w_q.weight\n",
      "encoder.layer.9.attention.self.query.bias -> encoder.9.self_attention.w_q.bias\n",
      "encoder.layer.9.attention.self.key.weight -> encoder.9.self_attention.w_k.weight\n",
      "encoder.layer.9.attention.self.key.bias -> encoder.9.self_attention.w_k.bias\n",
      "encoder.layer.9.attention.self.value.weight -> encoder.9.self_attention.w_v.weight\n",
      "encoder.layer.9.attention.self.value.bias -> encoder.9.self_attention.w_v.bias\n",
      "encoder.layer.9.attention.output.dense.weight -> encoder.9.self_attention.w_o.weight\n",
      "encoder.layer.9.attention.output.dense.bias -> encoder.9.self_attention.w_o.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight -> encoder.9.self_layer_norm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias -> encoder.9.self_layer_norm.bias\n",
      "encoder.layer.9.intermediate.dense.weight -> encoder.9.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.9.intermediate.dense.bias -> encoder.9.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.9.output.dense.weight -> encoder.9.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.9.output.dense.bias -> encoder.9.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.9.output.LayerNorm.weight -> encoder.9.ffn_layer_norm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias -> encoder.9.ffn_layer_norm.bias\n",
      "encoder.layer.10.attention.self.query.weight -> encoder.10.self_attention.w_q.weight\n",
      "encoder.layer.10.attention.self.query.bias -> encoder.10.self_attention.w_q.bias\n",
      "encoder.layer.10.attention.self.key.weight -> encoder.10.self_attention.w_k.weight\n",
      "encoder.layer.10.attention.self.key.bias -> encoder.10.self_attention.w_k.bias\n",
      "encoder.layer.10.attention.self.value.weight -> encoder.10.self_attention.w_v.weight\n",
      "encoder.layer.10.attention.self.value.bias -> encoder.10.self_attention.w_v.bias\n",
      "encoder.layer.10.attention.output.dense.weight -> encoder.10.self_attention.w_o.weight\n",
      "encoder.layer.10.attention.output.dense.bias -> encoder.10.self_attention.w_o.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight -> encoder.10.self_layer_norm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias -> encoder.10.self_layer_norm.bias\n",
      "encoder.layer.10.intermediate.dense.weight -> encoder.10.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.10.intermediate.dense.bias -> encoder.10.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.10.output.dense.weight -> encoder.10.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.10.output.dense.bias -> encoder.10.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.10.output.LayerNorm.weight -> encoder.10.ffn_layer_norm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias -> encoder.10.ffn_layer_norm.bias\n",
      "encoder.layer.11.attention.self.query.weight -> encoder.11.self_attention.w_q.weight\n",
      "encoder.layer.11.attention.self.query.bias -> encoder.11.self_attention.w_q.bias\n",
      "encoder.layer.11.attention.self.key.weight -> encoder.11.self_attention.w_k.weight\n",
      "encoder.layer.11.attention.self.key.bias -> encoder.11.self_attention.w_k.bias\n",
      "encoder.layer.11.attention.self.value.weight -> encoder.11.self_attention.w_v.weight\n",
      "encoder.layer.11.attention.self.value.bias -> encoder.11.self_attention.w_v.bias\n",
      "encoder.layer.11.attention.output.dense.weight -> encoder.11.self_attention.w_o.weight\n",
      "encoder.layer.11.attention.output.dense.bias -> encoder.11.self_attention.w_o.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight -> encoder.11.self_layer_norm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias -> encoder.11.self_layer_norm.bias\n",
      "encoder.layer.11.intermediate.dense.weight -> encoder.11.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.11.intermediate.dense.bias -> encoder.11.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.11.output.dense.weight -> encoder.11.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.11.output.dense.bias -> encoder.11.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.11.output.LayerNorm.weight -> encoder.11.ffn_layer_norm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias -> encoder.11.ffn_layer_norm.bias\n",
      "pooler.dense.weight -> linear.weight\n",
      "pooler.dense.bias -> linear.bias\n",
      "bert.embeddings.word_embeddings.weight -> bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight -> bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight -> bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight -> bert.embeddings.layer_norm.weight\n",
      "bert.embeddings.LayerNorm.bias -> bert.embeddings.layer_norm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight -> bert.encoder.0.self_attention.w_q.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias -> bert.encoder.0.self_attention.w_q.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight -> bert.encoder.0.self_attention.w_k.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias -> bert.encoder.0.self_attention.w_k.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight -> bert.encoder.0.self_attention.w_v.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias -> bert.encoder.0.self_attention.w_v.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight -> bert.encoder.0.self_attention.w_o.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias -> bert.encoder.0.self_attention.w_o.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight -> bert.encoder.0.self_layer_norm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias -> bert.encoder.0.self_layer_norm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight -> bert.encoder.0.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias -> bert.encoder.0.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.layer.0.output.dense.weight -> bert.encoder.0.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.layer.0.output.dense.bias -> bert.encoder.0.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight -> bert.encoder.0.ffn_layer_norm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias -> bert.encoder.0.ffn_layer_norm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight -> bert.encoder.1.self_attention.w_q.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias -> bert.encoder.1.self_attention.w_q.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight -> bert.encoder.1.self_attention.w_k.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias -> bert.encoder.1.self_attention.w_k.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight -> bert.encoder.1.self_attention.w_v.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias -> bert.encoder.1.self_attention.w_v.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight -> bert.encoder.1.self_attention.w_o.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias -> bert.encoder.1.self_attention.w_o.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight -> bert.encoder.1.self_layer_norm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias -> bert.encoder.1.self_layer_norm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight -> bert.encoder.1.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias -> bert.encoder.1.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.layer.1.output.dense.weight -> bert.encoder.1.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.layer.1.output.dense.bias -> bert.encoder.1.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight -> bert.encoder.1.ffn_layer_norm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias -> bert.encoder.1.ffn_layer_norm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight -> bert.encoder.2.self_attention.w_q.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias -> bert.encoder.2.self_attention.w_q.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight -> bert.encoder.2.self_attention.w_k.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias -> bert.encoder.2.self_attention.w_k.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight -> bert.encoder.2.self_attention.w_v.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias -> bert.encoder.2.self_attention.w_v.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight -> bert.encoder.2.self_attention.w_o.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias -> bert.encoder.2.self_attention.w_o.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight -> bert.encoder.2.self_layer_norm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias -> bert.encoder.2.self_layer_norm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight -> bert.encoder.2.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias -> bert.encoder.2.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.layer.2.output.dense.weight -> bert.encoder.2.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.layer.2.output.dense.bias -> bert.encoder.2.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight -> bert.encoder.2.ffn_layer_norm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias -> bert.encoder.2.ffn_layer_norm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight -> bert.encoder.3.self_attention.w_q.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias -> bert.encoder.3.self_attention.w_q.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight -> bert.encoder.3.self_attention.w_k.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias -> bert.encoder.3.self_attention.w_k.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight -> bert.encoder.3.self_attention.w_v.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias -> bert.encoder.3.self_attention.w_v.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight -> bert.encoder.3.self_attention.w_o.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias -> bert.encoder.3.self_attention.w_o.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight -> bert.encoder.3.self_layer_norm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias -> bert.encoder.3.self_layer_norm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight -> bert.encoder.3.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias -> bert.encoder.3.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.layer.3.output.dense.weight -> bert.encoder.3.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.layer.3.output.dense.bias -> bert.encoder.3.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight -> bert.encoder.3.ffn_layer_norm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias -> bert.encoder.3.ffn_layer_norm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight -> bert.encoder.4.self_attention.w_q.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias -> bert.encoder.4.self_attention.w_q.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight -> bert.encoder.4.self_attention.w_k.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias -> bert.encoder.4.self_attention.w_k.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight -> bert.encoder.4.self_attention.w_v.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias -> bert.encoder.4.self_attention.w_v.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight -> bert.encoder.4.self_attention.w_o.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias -> bert.encoder.4.self_attention.w_o.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight -> bert.encoder.4.self_layer_norm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias -> bert.encoder.4.self_layer_norm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight -> bert.encoder.4.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias -> bert.encoder.4.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.layer.4.output.dense.weight -> bert.encoder.4.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.layer.4.output.dense.bias -> bert.encoder.4.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight -> bert.encoder.4.ffn_layer_norm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias -> bert.encoder.4.ffn_layer_norm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight -> bert.encoder.5.self_attention.w_q.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias -> bert.encoder.5.self_attention.w_q.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight -> bert.encoder.5.self_attention.w_k.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias -> bert.encoder.5.self_attention.w_k.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight -> bert.encoder.5.self_attention.w_v.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias -> bert.encoder.5.self_attention.w_v.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight -> bert.encoder.5.self_attention.w_o.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias -> bert.encoder.5.self_attention.w_o.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight -> bert.encoder.5.self_layer_norm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias -> bert.encoder.5.self_layer_norm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight -> bert.encoder.5.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias -> bert.encoder.5.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.layer.5.output.dense.weight -> bert.encoder.5.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.layer.5.output.dense.bias -> bert.encoder.5.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight -> bert.encoder.5.ffn_layer_norm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias -> bert.encoder.5.ffn_layer_norm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight -> bert.encoder.6.self_attention.w_q.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias -> bert.encoder.6.self_attention.w_q.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight -> bert.encoder.6.self_attention.w_k.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias -> bert.encoder.6.self_attention.w_k.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight -> bert.encoder.6.self_attention.w_v.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias -> bert.encoder.6.self_attention.w_v.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight -> bert.encoder.6.self_attention.w_o.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias -> bert.encoder.6.self_attention.w_o.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight -> bert.encoder.6.self_layer_norm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias -> bert.encoder.6.self_layer_norm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight -> bert.encoder.6.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias -> bert.encoder.6.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.layer.6.output.dense.weight -> bert.encoder.6.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.layer.6.output.dense.bias -> bert.encoder.6.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight -> bert.encoder.6.ffn_layer_norm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias -> bert.encoder.6.ffn_layer_norm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight -> bert.encoder.7.self_attention.w_q.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias -> bert.encoder.7.self_attention.w_q.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight -> bert.encoder.7.self_attention.w_k.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias -> bert.encoder.7.self_attention.w_k.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight -> bert.encoder.7.self_attention.w_v.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias -> bert.encoder.7.self_attention.w_v.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight -> bert.encoder.7.self_attention.w_o.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias -> bert.encoder.7.self_attention.w_o.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight -> bert.encoder.7.self_layer_norm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias -> bert.encoder.7.self_layer_norm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight -> bert.encoder.7.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias -> bert.encoder.7.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.layer.7.output.dense.weight -> bert.encoder.7.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.layer.7.output.dense.bias -> bert.encoder.7.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight -> bert.encoder.7.ffn_layer_norm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias -> bert.encoder.7.ffn_layer_norm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight -> bert.encoder.8.self_attention.w_q.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias -> bert.encoder.8.self_attention.w_q.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight -> bert.encoder.8.self_attention.w_k.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias -> bert.encoder.8.self_attention.w_k.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight -> bert.encoder.8.self_attention.w_v.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias -> bert.encoder.8.self_attention.w_v.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight -> bert.encoder.8.self_attention.w_o.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias -> bert.encoder.8.self_attention.w_o.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight -> bert.encoder.8.self_layer_norm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias -> bert.encoder.8.self_layer_norm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight -> bert.encoder.8.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias -> bert.encoder.8.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.layer.8.output.dense.weight -> bert.encoder.8.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.layer.8.output.dense.bias -> bert.encoder.8.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight -> bert.encoder.8.ffn_layer_norm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias -> bert.encoder.8.ffn_layer_norm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight -> bert.encoder.9.self_attention.w_q.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias -> bert.encoder.9.self_attention.w_q.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight -> bert.encoder.9.self_attention.w_k.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias -> bert.encoder.9.self_attention.w_k.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight -> bert.encoder.9.self_attention.w_v.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias -> bert.encoder.9.self_attention.w_v.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight -> bert.encoder.9.self_attention.w_o.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias -> bert.encoder.9.self_attention.w_o.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight -> bert.encoder.9.self_layer_norm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias -> bert.encoder.9.self_layer_norm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight -> bert.encoder.9.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias -> bert.encoder.9.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.layer.9.output.dense.weight -> bert.encoder.9.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.layer.9.output.dense.bias -> bert.encoder.9.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight -> bert.encoder.9.ffn_layer_norm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias -> bert.encoder.9.ffn_layer_norm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight -> bert.encoder.10.self_attention.w_q.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias -> bert.encoder.10.self_attention.w_q.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight -> bert.encoder.10.self_attention.w_k.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias -> bert.encoder.10.self_attention.w_k.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight -> bert.encoder.10.self_attention.w_v.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias -> bert.encoder.10.self_attention.w_v.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight -> bert.encoder.10.self_attention.w_o.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias -> bert.encoder.10.self_attention.w_o.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight -> bert.encoder.10.self_layer_norm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias -> bert.encoder.10.self_layer_norm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight -> bert.encoder.10.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias -> bert.encoder.10.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.layer.10.output.dense.weight -> bert.encoder.10.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.layer.10.output.dense.bias -> bert.encoder.10.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight -> bert.encoder.10.ffn_layer_norm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias -> bert.encoder.10.ffn_layer_norm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight -> bert.encoder.11.self_attention.w_q.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias -> bert.encoder.11.self_attention.w_q.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight -> bert.encoder.11.self_attention.w_k.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias -> bert.encoder.11.self_attention.w_k.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight -> bert.encoder.11.self_attention.w_v.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias -> bert.encoder.11.self_attention.w_v.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight -> bert.encoder.11.self_attention.w_o.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias -> bert.encoder.11.self_attention.w_o.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight -> bert.encoder.11.self_layer_norm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias -> bert.encoder.11.self_layer_norm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight -> bert.encoder.11.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias -> bert.encoder.11.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.layer.11.output.dense.weight -> bert.encoder.11.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.layer.11.output.dense.bias -> bert.encoder.11.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight -> bert.encoder.11.ffn_layer_norm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias -> bert.encoder.11.ffn_layer_norm.bias\n",
      "bert.pooler.dense.weight -> bert.linear.weight\n",
      "bert.pooler.dense.bias -> bert.linear.bias\n",
      "oper_layers.0.weight -> oper_layers.0.weight\n",
      "oper_layers.0.bias -> oper_layers.0.bias\n",
      "oper_layers.1.weight -> oper_layers.1.weight\n",
      "oper_layers.1.bias -> oper_layers.1.bias\n",
      "oper_layers.2.weight -> oper_layers.2.weight\n",
      "oper_layers.2.bias -> oper_layers.2.bias\n",
      "oper_layers.3.weight -> oper_layers.3.weight\n",
      "oper_layers.3.bias -> oper_layers.3.bias\n",
      "oper_layers.4.weight -> oper_layers.4.weight\n",
      "oper_layers.4.bias -> oper_layers.4.bias\n",
      "oper_layers.5.weight -> oper_layers.5.weight\n",
      "oper_layers.5.bias -> oper_layers.5.bias\n",
      "oper_layers.6.weight -> oper_layers.6.weight\n",
      "oper_layers.6.bias -> oper_layers.6.bias\n",
      "oper_layers.7.weight -> oper_layers.7.weight\n",
      "oper_layers.7.bias -> oper_layers.7.bias\n",
      "oper_layers.8.weight -> oper_layers.8.weight\n",
      "oper_layers.8.bias -> oper_layers.8.bias\n",
      "oper_layers.9.weight -> oper_layers.9.weight\n",
      "oper_layers.9.bias -> oper_layers.9.bias\n",
      "oper_layers.10.weight -> oper_layers.10.weight\n",
      "oper_layers.10.bias -> oper_layers.10.bias\n",
      "oper_layers.11.weight -> oper_layers.11.weight\n",
      "oper_layers.11.bias -> oper_layers.11.bias\n",
      "oper_layers.12.weight -> oper_layers.12.weight\n",
      "oper_layers.12.bias -> oper_layers.12.bias\n",
      "oper_layers.13.weight -> oper_layers.13.weight\n",
      "oper_layers.13.bias -> oper_layers.13.bias\n",
      "oper_layers.14.weight -> oper_layers.14.weight\n",
      "oper_layers.14.bias -> oper_layers.14.bias\n",
      "oper_layers.15.weight -> oper_layers.15.weight\n",
      "oper_layers.15.bias -> oper_layers.15.bias\n",
      "oper_layers.16.weight -> oper_layers.16.weight\n",
      "oper_layers.16.bias -> oper_layers.16.bias\n",
      "oper_layers.17.weight -> oper_layers.17.weight\n",
      "oper_layers.17.bias -> oper_layers.17.bias\n",
      "oper_layers.18.weight -> oper_layers.18.weight\n",
      "oper_layers.18.bias -> oper_layers.18.bias\n",
      "oper_layers.19.weight -> oper_layers.19.weight\n",
      "oper_layers.19.bias -> oper_layers.19.bias\n",
      "oper_layers.20.weight -> oper_layers.20.weight\n",
      "oper_layers.20.bias -> oper_layers.20.bias\n",
      "oper_layers.21.weight -> oper_layers.21.weight\n",
      "oper_layers.21.bias -> oper_layers.21.bias\n",
      "oper_layers.22.weight -> oper_layers.22.weight\n",
      "oper_layers.22.bias -> oper_layers.22.bias\n",
      "oper_layers.23.weight -> oper_layers.23.weight\n",
      "oper_layers.23.bias -> oper_layers.23.bias\n",
      "oper_layers.24.weight -> oper_layers.24.weight\n",
      "oper_layers.24.bias -> oper_layers.24.bias\n",
      "oper_layers.25.weight -> oper_layers.25.weight\n",
      "oper_layers.25.bias -> oper_layers.25.bias\n",
      "oper_layers.26.weight -> oper_layers.26.weight\n",
      "oper_layers.26.bias -> oper_layers.26.bias\n",
      "oper_layers.27.weight -> oper_layers.27.weight\n",
      "oper_layers.27.bias -> oper_layers.27.bias\n",
      "oper_layers.28.weight -> oper_layers.28.weight\n",
      "oper_layers.28.bias -> oper_layers.28.bias\n",
      "oper_layers.29.weight -> oper_layers.29.weight\n",
      "oper_layers.29.bias -> oper_layers.29.bias\n",
      "span_layers.0.weight -> span_layers.0.weight\n",
      "span_layers.0.bias -> span_layers.0.bias\n",
      "span_layers.1.weight -> span_layers.1.weight\n",
      "span_layers.1.bias -> span_layers.1.bias\n",
      "span_layers.2.weight -> span_layers.2.weight\n",
      "span_layers.2.bias -> span_layers.2.bias\n",
      "span_layers.3.weight -> span_layers.3.weight\n",
      "span_layers.3.bias -> span_layers.3.bias\n",
      "span_layers.4.weight -> span_layers.4.weight\n",
      "span_layers.4.bias -> span_layers.4.bias\n",
      "span_layers.5.weight -> span_layers.5.weight\n",
      "span_layers.5.bias -> span_layers.5.bias\n",
      "span_layers.6.weight -> span_layers.6.weight\n",
      "span_layers.6.bias -> span_layers.6.bias\n",
      "span_layers.7.weight -> span_layers.7.weight\n",
      "span_layers.7.bias -> span_layers.7.bias\n",
      "span_layers.8.weight -> span_layers.8.weight\n",
      "span_layers.8.bias -> span_layers.8.bias\n",
      "span_layers.9.weight -> span_layers.9.weight\n",
      "span_layers.9.bias -> span_layers.9.bias\n",
      "span_layers.10.weight -> span_layers.10.weight\n",
      "span_layers.10.bias -> span_layers.10.bias\n",
      "span_layers.11.weight -> span_layers.11.weight\n",
      "span_layers.11.bias -> span_layers.11.bias\n",
      "span_layers.12.weight -> span_layers.12.weight\n",
      "span_layers.12.bias -> span_layers.12.bias\n",
      "span_layers.13.weight -> span_layers.13.weight\n",
      "span_layers.13.bias -> span_layers.13.bias\n",
      "span_layers.14.weight -> span_layers.14.weight\n",
      "span_layers.14.bias -> span_layers.14.bias\n",
      "span_layers.15.weight -> span_layers.15.weight\n",
      "span_layers.15.bias -> span_layers.15.bias\n",
      "span_layers.16.weight -> span_layers.16.weight\n",
      "span_layers.16.bias -> span_layers.16.bias\n",
      "span_layers.17.weight -> span_layers.17.weight\n",
      "span_layers.17.bias -> span_layers.17.bias\n",
      "span_layers.18.weight -> span_layers.18.weight\n",
      "span_layers.18.bias -> span_layers.18.bias\n",
      "span_layers.19.weight -> span_layers.19.weight\n",
      "span_layers.19.bias -> span_layers.19.bias\n",
      "span_layers.20.weight -> span_layers.20.weight\n",
      "span_layers.20.bias -> span_layers.20.bias\n",
      "span_layers.21.weight -> span_layers.21.weight\n",
      "span_layers.21.bias -> span_layers.21.bias\n",
      "span_layers.22.weight -> span_layers.22.weight\n",
      "span_layers.22.bias -> span_layers.22.bias\n",
      "span_layers.23.weight -> span_layers.23.weight\n",
      "span_layers.23.bias -> span_layers.23.bias\n",
      "span_layers.24.weight -> span_layers.24.weight\n",
      "span_layers.24.bias -> span_layers.24.bias\n",
      "span_layers.25.weight -> span_layers.25.weight\n",
      "span_layers.25.bias -> span_layers.25.bias\n",
      "span_layers.26.weight -> span_layers.26.weight\n",
      "span_layers.26.bias -> span_layers.26.bias\n",
      "span_layers.27.weight -> span_layers.27.weight\n",
      "span_layers.27.bias -> span_layers.27.bias\n",
      "span_layers.28.weight -> span_layers.28.weight\n",
      "span_layers.28.bias -> span_layers.28.bias\n",
      "span_layers.29.weight -> span_layers.29.weight\n",
      "span_layers.29.bias -> span_layers.29.bias\n",
      "refer_layers.0.weight -> refer_layers.0.weight\n",
      "refer_layers.0.bias -> refer_layers.0.bias\n",
      "refer_layers.1.weight -> refer_layers.1.weight\n",
      "refer_layers.1.bias -> refer_layers.1.bias\n",
      "refer_layers.2.weight -> refer_layers.2.weight\n",
      "refer_layers.2.bias -> refer_layers.2.bias\n",
      "refer_layers.3.weight -> refer_layers.3.weight\n",
      "refer_layers.3.bias -> refer_layers.3.bias\n",
      "refer_layers.4.weight -> refer_layers.4.weight\n",
      "refer_layers.4.bias -> refer_layers.4.bias\n",
      "refer_layers.5.weight -> refer_layers.5.weight\n",
      "refer_layers.5.bias -> refer_layers.5.bias\n",
      "refer_layers.6.weight -> refer_layers.6.weight\n",
      "refer_layers.6.bias -> refer_layers.6.bias\n",
      "refer_layers.7.weight -> refer_layers.7.weight\n",
      "refer_layers.7.bias -> refer_layers.7.bias\n",
      "refer_layers.8.weight -> refer_layers.8.weight\n",
      "refer_layers.8.bias -> refer_layers.8.bias\n",
      "refer_layers.9.weight -> refer_layers.9.weight\n",
      "refer_layers.9.bias -> refer_layers.9.bias\n",
      "refer_layers.10.weight -> refer_layers.10.weight\n",
      "refer_layers.10.bias -> refer_layers.10.bias\n",
      "refer_layers.11.weight -> refer_layers.11.weight\n",
      "refer_layers.11.bias -> refer_layers.11.bias\n",
      "refer_layers.12.weight -> refer_layers.12.weight\n",
      "refer_layers.12.bias -> refer_layers.12.bias\n",
      "refer_layers.13.weight -> refer_layers.13.weight\n",
      "refer_layers.13.bias -> refer_layers.13.bias\n",
      "refer_layers.14.weight -> refer_layers.14.weight\n",
      "refer_layers.14.bias -> refer_layers.14.bias\n",
      "refer_layers.15.weight -> refer_layers.15.weight\n",
      "refer_layers.15.bias -> refer_layers.15.bias\n",
      "refer_layers.16.weight -> refer_layers.16.weight\n",
      "refer_layers.16.bias -> refer_layers.16.bias\n",
      "refer_layers.17.weight -> refer_layers.17.weight\n",
      "refer_layers.17.bias -> refer_layers.17.bias\n",
      "refer_layers.18.weight -> refer_layers.18.weight\n",
      "refer_layers.18.bias -> refer_layers.18.bias\n",
      "refer_layers.19.weight -> refer_layers.19.weight\n",
      "refer_layers.19.bias -> refer_layers.19.bias\n",
      "refer_layers.20.weight -> refer_layers.20.weight\n",
      "refer_layers.20.bias -> refer_layers.20.bias\n",
      "refer_layers.21.weight -> refer_layers.21.weight\n",
      "refer_layers.21.bias -> refer_layers.21.bias\n",
      "refer_layers.22.weight -> refer_layers.22.weight\n",
      "refer_layers.22.bias -> refer_layers.22.bias\n",
      "refer_layers.23.weight -> refer_layers.23.weight\n",
      "refer_layers.23.bias -> refer_layers.23.bias\n",
      "refer_layers.24.weight -> refer_layers.24.weight\n",
      "refer_layers.24.bias -> refer_layers.24.bias\n",
      "refer_layers.25.weight -> refer_layers.25.weight\n",
      "refer_layers.25.bias -> refer_layers.25.bias\n",
      "refer_layers.26.weight -> refer_layers.26.weight\n",
      "refer_layers.26.bias -> refer_layers.26.bias\n",
      "refer_layers.27.weight -> refer_layers.27.weight\n",
      "refer_layers.27.bias -> refer_layers.27.bias\n",
      "refer_layers.28.weight -> refer_layers.28.weight\n",
      "refer_layers.28.bias -> refer_layers.28.bias\n",
      "refer_layers.29.weight -> refer_layers.29.weight\n",
      "refer_layers.29.bias -> refer_layers.29.bias\n",
      "inform_aux_layer.weight -> inform_aux_layer.weight\n",
      "inform_aux_layer.bias -> inform_aux_layer.bias\n",
      "ds_aux_layer.weight -> ds_aux_layer.weight\n",
      "ds_aux_layer.bias -> ds_aux_layer.bias\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "tods = DNNTODS(CHATBOT_NAME, DOMAINS, ONTOLOGY_PATH, LABEL_MAPS_PATH, POLICY, START, TEMPLATES, NON_REFERABLE_SLOTS, NON_REFERABLE_PAIRS, FROM_SCRATCH)\n",
    "tods.load_dst_model(MODEL_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_gui = chat_gui(tods.infer)\n",
    "# chat_gui.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "tods.reset()\n",
    "print(tods.get_dialogue_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi I am Tody, I can help you reserve a hottel or a taxi?\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hotel-area': 'center'}\n",
      "Do you have a price reference?\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('Hi Tody, I want to book a hotel in the city center for 2 days.')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hotel-area': 'center', 'hotel-pricerange': 'moderate'}\n",
      "What day would you like to stay and how many days?\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('yes I want a moderate price range.')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hotel-area': 'center', 'hotel-pricerange': 'moderate', 'hotel-book_day': 'monday', 'hotel-book_stay': '2'}\n",
      "I have one called the continental hotel that has 5 stars.\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('I want to check in on monday and I am staying for 2 days.')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hotel-area': 'center', 'hotel-pricerange': 'moderate', 'hotel-book_day': 'monday', 'hotel-book_stay': '2', 'hotel-name': 'continental hotel'}\n",
      "Do you need parking or internet?\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('No I hate this hotel find something else.')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hotel-area': 'center', 'hotel-pricerange': 'moderate', 'hotel-book_day': 'monday', 'hotel-book_stay': '2', 'hotel-name': 'continental hotel', 'hotel-internet': 'dontcare', 'hotel-parking': 'dontcare'}\n",
      "How many people is the booking for?\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('Yes I need internet but I do not care about parking.')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hotel-area': 'center', 'hotel-pricerange': 'moderate', 'hotel-book_day': 'monday', 'hotel-book_stay': '2', 'hotel-name': 'continental hotel', 'hotel-internet': 'dontcare', 'hotel-parking': 'dontcare', 'hotel-book_people': '2'}\n",
      "What is your desired destination?\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('I am booking for 2 people.')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hotel-area': 'center', 'hotel-pricerange': 'moderate', 'hotel-book_day': 'monday', 'hotel-book_stay': '2', 'hotel-name': 'continental hotel', 'hotel-internet': 'dontcare', 'hotel-parking': 'dontcare', 'hotel-book_people': '2', 'taxi-destination': 'continental hotel'}\n",
      "From where you will be departing, and what time do you want to leave?\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('I want to book a taxi to go to the hotel.')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hotel-area': 'center', 'hotel-pricerange': 'moderate', 'hotel-book_day': 'monday', 'hotel-book_stay': '2', 'hotel-name': 'continental hotel', 'hotel-internet': 'dontcare', 'hotel-parking': 'dontcare', 'hotel-book_people': '2', 'taxi-destination': 'continental hotel', 'taxi-leaveAt': '10:00', 'taxi-departure': 'ramses station'}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('I am leavivng from the ramses station at 10 am')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
