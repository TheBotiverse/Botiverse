{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botiverse import chat_gui\n",
    "from botiverse.bots import DeepTODS\n",
    "import gdown\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download necessary files to run the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ontology already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Download ontology\n",
    "f_id = '1tRprClzgNCtVks-SgZilBD7fdc5W3XxY'\n",
    "file_url = f'https://drive.google.com/uc?export=download&confirm=pbef&id={f_id}'\n",
    "output_file = 'ontology.json'\n",
    "if not os.path.exists(output_file):\n",
    "    gdown.download(file_url, output_file)\n",
    "    print('Ontology downloaded successfully.')\n",
    "else:\n",
    "    print('Ontology already exists. Skipping download.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label maps already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Download label maps\n",
    "f_id = '15guVrztM12KMMDp6jgu53ivUU945iRNp'\n",
    "file_url = f'https://drive.google.com/uc?export=download&confirm=pbef&id={f_id}'\n",
    "output_file = 'label_maps.json'\n",
    "if not os.path.exists(output_file):\n",
    "    gdown.download(file_url, output_file)\n",
    "    print('Label maps downloaded successfully.')\n",
    "else:\n",
    "    print('Label maps already exists. Skipping download.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Download DST weights trained on sim-R\n",
    "f_id = '1POjBULmqxBrebvINfl989bskAstV3Zld'\n",
    "file_url = f'https://drive.google.com/uc?export=download&confirm=pbef&id={f_id}'\n",
    "output_file = 'model.pt'\n",
    "if not os.path.exists(output_file):\n",
    "    gdown.download(file_url, output_file)\n",
    "    print('Model downloaded successfully.')\n",
    "else:\n",
    "    print('Model already exists. Skipping download.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHATBOT_NAME = 'Tody'\n",
    "\n",
    "DOMAINS = [\"restaurant\"]\n",
    "\n",
    "ONTOLOGY_PATH = './ontology.json'\n",
    "\n",
    "LABEL_MAPS_PATH = './label_maps.json'\n",
    "\n",
    "POLICY = 'Priority' # Priority or Random\n",
    "\n",
    "START = [\n",
    "    {\n",
    "        'utterance': 'Hi I am Tody, I can help you reserve a restaurant?',\n",
    "        'slots': [],\n",
    "        'system_act': {}\n",
    "    }\n",
    "]\n",
    "\n",
    "TEMPLATES = [\n",
    "    {\n",
    "        'utterance': 'what type of food do you want and in what area?',\n",
    "        'slots': ['restaurant-location', 'restaurant-category'],\n",
    "        'system_act': {}\n",
    "    },\n",
    "    {\n",
    "        'utterance': 'what is your preferred price range and rating?',\n",
    "        'slots': ['restaurant-price_range', 'ressaurant-rating'],\n",
    "        'system_act': {}\n",
    "    },\n",
    "    {\n",
    "        'utterance': 'how many people will be in your party?',\n",
    "        'slots': ['restaurant-num_people'],\n",
    "        'system_act': {}\n",
    "    },\n",
    "    {\n",
    "        'utterance': 'what time and date would you like to reserve a table for?',\n",
    "        'slots': ['restaurant-time', 'restaurant-date'],\n",
    "        'system_act': {}\n",
    "    },\n",
    "    {\n",
    "        'utterance': 'May I suggest kfc restaurant?',\n",
    "        'slots': ['restaurant-restaurant_name'],\n",
    "        'system_act': {'restaurant-restaurant_name': ['kfc']}\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "NON_REFERABLE_SLOTS = []\n",
    "\n",
    "NON_REFERABLE_PAIRS = []\n",
    "\n",
    "FROM_SCRATCH = True\n",
    "\n",
    "MODEL_PATH = './model.pt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model & Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight -> embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight -> embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight -> embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight -> embeddings.layer_norm.weight\n",
      "embeddings.LayerNorm.bias -> embeddings.layer_norm.bias\n",
      "encoder.layer.0.attention.self.query.weight -> encoder.0.self_attention.w_q.weight\n",
      "encoder.layer.0.attention.self.query.bias -> encoder.0.self_attention.w_q.bias\n",
      "encoder.layer.0.attention.self.key.weight -> encoder.0.self_attention.w_k.weight\n",
      "encoder.layer.0.attention.self.key.bias -> encoder.0.self_attention.w_k.bias\n",
      "encoder.layer.0.attention.self.value.weight -> encoder.0.self_attention.w_v.weight\n",
      "encoder.layer.0.attention.self.value.bias -> encoder.0.self_attention.w_v.bias\n",
      "encoder.layer.0.attention.output.dense.weight -> encoder.0.self_attention.w_o.weight\n",
      "encoder.layer.0.attention.output.dense.bias -> encoder.0.self_attention.w_o.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight -> encoder.0.self_layer_norm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias -> encoder.0.self_layer_norm.bias\n",
      "encoder.layer.0.intermediate.dense.weight -> encoder.0.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.0.intermediate.dense.bias -> encoder.0.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.0.output.dense.weight -> encoder.0.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.0.output.dense.bias -> encoder.0.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.0.output.LayerNorm.weight -> encoder.0.ffn_layer_norm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias -> encoder.0.ffn_layer_norm.bias\n",
      "encoder.layer.1.attention.self.query.weight -> encoder.1.self_attention.w_q.weight\n",
      "encoder.layer.1.attention.self.query.bias -> encoder.1.self_attention.w_q.bias\n",
      "encoder.layer.1.attention.self.key.weight -> encoder.1.self_attention.w_k.weight\n",
      "encoder.layer.1.attention.self.key.bias -> encoder.1.self_attention.w_k.bias\n",
      "encoder.layer.1.attention.self.value.weight -> encoder.1.self_attention.w_v.weight\n",
      "encoder.layer.1.attention.self.value.bias -> encoder.1.self_attention.w_v.bias\n",
      "encoder.layer.1.attention.output.dense.weight -> encoder.1.self_attention.w_o.weight\n",
      "encoder.layer.1.attention.output.dense.bias -> encoder.1.self_attention.w_o.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight -> encoder.1.self_layer_norm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias -> encoder.1.self_layer_norm.bias\n",
      "encoder.layer.1.intermediate.dense.weight -> encoder.1.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.1.intermediate.dense.bias -> encoder.1.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.1.output.dense.weight -> encoder.1.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.1.output.dense.bias -> encoder.1.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.1.output.LayerNorm.weight -> encoder.1.ffn_layer_norm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias -> encoder.1.ffn_layer_norm.bias\n",
      "encoder.layer.2.attention.self.query.weight -> encoder.2.self_attention.w_q.weight\n",
      "encoder.layer.2.attention.self.query.bias -> encoder.2.self_attention.w_q.bias\n",
      "encoder.layer.2.attention.self.key.weight -> encoder.2.self_attention.w_k.weight\n",
      "encoder.layer.2.attention.self.key.bias -> encoder.2.self_attention.w_k.bias\n",
      "encoder.layer.2.attention.self.value.weight -> encoder.2.self_attention.w_v.weight\n",
      "encoder.layer.2.attention.self.value.bias -> encoder.2.self_attention.w_v.bias\n",
      "encoder.layer.2.attention.output.dense.weight -> encoder.2.self_attention.w_o.weight\n",
      "encoder.layer.2.attention.output.dense.bias -> encoder.2.self_attention.w_o.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight -> encoder.2.self_layer_norm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias -> encoder.2.self_layer_norm.bias\n",
      "encoder.layer.2.intermediate.dense.weight -> encoder.2.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.2.intermediate.dense.bias -> encoder.2.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.2.output.dense.weight -> encoder.2.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.2.output.dense.bias -> encoder.2.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.2.output.LayerNorm.weight -> encoder.2.ffn_layer_norm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias -> encoder.2.ffn_layer_norm.bias\n",
      "encoder.layer.3.attention.self.query.weight -> encoder.3.self_attention.w_q.weight\n",
      "encoder.layer.3.attention.self.query.bias -> encoder.3.self_attention.w_q.bias\n",
      "encoder.layer.3.attention.self.key.weight -> encoder.3.self_attention.w_k.weight\n",
      "encoder.layer.3.attention.self.key.bias -> encoder.3.self_attention.w_k.bias\n",
      "encoder.layer.3.attention.self.value.weight -> encoder.3.self_attention.w_v.weight\n",
      "encoder.layer.3.attention.self.value.bias -> encoder.3.self_attention.w_v.bias\n",
      "encoder.layer.3.attention.output.dense.weight -> encoder.3.self_attention.w_o.weight\n",
      "encoder.layer.3.attention.output.dense.bias -> encoder.3.self_attention.w_o.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight -> encoder.3.self_layer_norm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias -> encoder.3.self_layer_norm.bias\n",
      "encoder.layer.3.intermediate.dense.weight -> encoder.3.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.3.intermediate.dense.bias -> encoder.3.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.3.output.dense.weight -> encoder.3.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.3.output.dense.bias -> encoder.3.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.3.output.LayerNorm.weight -> encoder.3.ffn_layer_norm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias -> encoder.3.ffn_layer_norm.bias\n",
      "encoder.layer.4.attention.self.query.weight -> encoder.4.self_attention.w_q.weight\n",
      "encoder.layer.4.attention.self.query.bias -> encoder.4.self_attention.w_q.bias\n",
      "encoder.layer.4.attention.self.key.weight -> encoder.4.self_attention.w_k.weight\n",
      "encoder.layer.4.attention.self.key.bias -> encoder.4.self_attention.w_k.bias\n",
      "encoder.layer.4.attention.self.value.weight -> encoder.4.self_attention.w_v.weight\n",
      "encoder.layer.4.attention.self.value.bias -> encoder.4.self_attention.w_v.bias\n",
      "encoder.layer.4.attention.output.dense.weight -> encoder.4.self_attention.w_o.weight\n",
      "encoder.layer.4.attention.output.dense.bias -> encoder.4.self_attention.w_o.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight -> encoder.4.self_layer_norm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias -> encoder.4.self_layer_norm.bias\n",
      "encoder.layer.4.intermediate.dense.weight -> encoder.4.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.4.intermediate.dense.bias -> encoder.4.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.4.output.dense.weight -> encoder.4.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.4.output.dense.bias -> encoder.4.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.4.output.LayerNorm.weight -> encoder.4.ffn_layer_norm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias -> encoder.4.ffn_layer_norm.bias\n",
      "encoder.layer.5.attention.self.query.weight -> encoder.5.self_attention.w_q.weight\n",
      "encoder.layer.5.attention.self.query.bias -> encoder.5.self_attention.w_q.bias\n",
      "encoder.layer.5.attention.self.key.weight -> encoder.5.self_attention.w_k.weight\n",
      "encoder.layer.5.attention.self.key.bias -> encoder.5.self_attention.w_k.bias\n",
      "encoder.layer.5.attention.self.value.weight -> encoder.5.self_attention.w_v.weight\n",
      "encoder.layer.5.attention.self.value.bias -> encoder.5.self_attention.w_v.bias\n",
      "encoder.layer.5.attention.output.dense.weight -> encoder.5.self_attention.w_o.weight\n",
      "encoder.layer.5.attention.output.dense.bias -> encoder.5.self_attention.w_o.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight -> encoder.5.self_layer_norm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias -> encoder.5.self_layer_norm.bias\n",
      "encoder.layer.5.intermediate.dense.weight -> encoder.5.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.5.intermediate.dense.bias -> encoder.5.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.5.output.dense.weight -> encoder.5.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.5.output.dense.bias -> encoder.5.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.5.output.LayerNorm.weight -> encoder.5.ffn_layer_norm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias -> encoder.5.ffn_layer_norm.bias\n",
      "encoder.layer.6.attention.self.query.weight -> encoder.6.self_attention.w_q.weight\n",
      "encoder.layer.6.attention.self.query.bias -> encoder.6.self_attention.w_q.bias\n",
      "encoder.layer.6.attention.self.key.weight -> encoder.6.self_attention.w_k.weight\n",
      "encoder.layer.6.attention.self.key.bias -> encoder.6.self_attention.w_k.bias\n",
      "encoder.layer.6.attention.self.value.weight -> encoder.6.self_attention.w_v.weight\n",
      "encoder.layer.6.attention.self.value.bias -> encoder.6.self_attention.w_v.bias\n",
      "encoder.layer.6.attention.output.dense.weight -> encoder.6.self_attention.w_o.weight\n",
      "encoder.layer.6.attention.output.dense.bias -> encoder.6.self_attention.w_o.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight -> encoder.6.self_layer_norm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias -> encoder.6.self_layer_norm.bias\n",
      "encoder.layer.6.intermediate.dense.weight -> encoder.6.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.6.intermediate.dense.bias -> encoder.6.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.6.output.dense.weight -> encoder.6.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.6.output.dense.bias -> encoder.6.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.6.output.LayerNorm.weight -> encoder.6.ffn_layer_norm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias -> encoder.6.ffn_layer_norm.bias\n",
      "encoder.layer.7.attention.self.query.weight -> encoder.7.self_attention.w_q.weight\n",
      "encoder.layer.7.attention.self.query.bias -> encoder.7.self_attention.w_q.bias\n",
      "encoder.layer.7.attention.self.key.weight -> encoder.7.self_attention.w_k.weight\n",
      "encoder.layer.7.attention.self.key.bias -> encoder.7.self_attention.w_k.bias\n",
      "encoder.layer.7.attention.self.value.weight -> encoder.7.self_attention.w_v.weight\n",
      "encoder.layer.7.attention.self.value.bias -> encoder.7.self_attention.w_v.bias\n",
      "encoder.layer.7.attention.output.dense.weight -> encoder.7.self_attention.w_o.weight\n",
      "encoder.layer.7.attention.output.dense.bias -> encoder.7.self_attention.w_o.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight -> encoder.7.self_layer_norm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias -> encoder.7.self_layer_norm.bias\n",
      "encoder.layer.7.intermediate.dense.weight -> encoder.7.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.7.intermediate.dense.bias -> encoder.7.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.7.output.dense.weight -> encoder.7.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.7.output.dense.bias -> encoder.7.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.7.output.LayerNorm.weight -> encoder.7.ffn_layer_norm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias -> encoder.7.ffn_layer_norm.bias\n",
      "encoder.layer.8.attention.self.query.weight -> encoder.8.self_attention.w_q.weight\n",
      "encoder.layer.8.attention.self.query.bias -> encoder.8.self_attention.w_q.bias\n",
      "encoder.layer.8.attention.self.key.weight -> encoder.8.self_attention.w_k.weight\n",
      "encoder.layer.8.attention.self.key.bias -> encoder.8.self_attention.w_k.bias\n",
      "encoder.layer.8.attention.self.value.weight -> encoder.8.self_attention.w_v.weight\n",
      "encoder.layer.8.attention.self.value.bias -> encoder.8.self_attention.w_v.bias\n",
      "encoder.layer.8.attention.output.dense.weight -> encoder.8.self_attention.w_o.weight\n",
      "encoder.layer.8.attention.output.dense.bias -> encoder.8.self_attention.w_o.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight -> encoder.8.self_layer_norm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias -> encoder.8.self_layer_norm.bias\n",
      "encoder.layer.8.intermediate.dense.weight -> encoder.8.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.8.intermediate.dense.bias -> encoder.8.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.8.output.dense.weight -> encoder.8.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.8.output.dense.bias -> encoder.8.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.8.output.LayerNorm.weight -> encoder.8.ffn_layer_norm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias -> encoder.8.ffn_layer_norm.bias\n",
      "encoder.layer.9.attention.self.query.weight -> encoder.9.self_attention.w_q.weight\n",
      "encoder.layer.9.attention.self.query.bias -> encoder.9.self_attention.w_q.bias\n",
      "encoder.layer.9.attention.self.key.weight -> encoder.9.self_attention.w_k.weight\n",
      "encoder.layer.9.attention.self.key.bias -> encoder.9.self_attention.w_k.bias\n",
      "encoder.layer.9.attention.self.value.weight -> encoder.9.self_attention.w_v.weight\n",
      "encoder.layer.9.attention.self.value.bias -> encoder.9.self_attention.w_v.bias\n",
      "encoder.layer.9.attention.output.dense.weight -> encoder.9.self_attention.w_o.weight\n",
      "encoder.layer.9.attention.output.dense.bias -> encoder.9.self_attention.w_o.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight -> encoder.9.self_layer_norm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias -> encoder.9.self_layer_norm.bias\n",
      "encoder.layer.9.intermediate.dense.weight -> encoder.9.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.9.intermediate.dense.bias -> encoder.9.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.9.output.dense.weight -> encoder.9.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.9.output.dense.bias -> encoder.9.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.9.output.LayerNorm.weight -> encoder.9.ffn_layer_norm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias -> encoder.9.ffn_layer_norm.bias\n",
      "encoder.layer.10.attention.self.query.weight -> encoder.10.self_attention.w_q.weight\n",
      "encoder.layer.10.attention.self.query.bias -> encoder.10.self_attention.w_q.bias\n",
      "encoder.layer.10.attention.self.key.weight -> encoder.10.self_attention.w_k.weight\n",
      "encoder.layer.10.attention.self.key.bias -> encoder.10.self_attention.w_k.bias\n",
      "encoder.layer.10.attention.self.value.weight -> encoder.10.self_attention.w_v.weight\n",
      "encoder.layer.10.attention.self.value.bias -> encoder.10.self_attention.w_v.bias\n",
      "encoder.layer.10.attention.output.dense.weight -> encoder.10.self_attention.w_o.weight\n",
      "encoder.layer.10.attention.output.dense.bias -> encoder.10.self_attention.w_o.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight -> encoder.10.self_layer_norm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias -> encoder.10.self_layer_norm.bias\n",
      "encoder.layer.10.intermediate.dense.weight -> encoder.10.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.10.intermediate.dense.bias -> encoder.10.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.10.output.dense.weight -> encoder.10.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.10.output.dense.bias -> encoder.10.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.10.output.LayerNorm.weight -> encoder.10.ffn_layer_norm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias -> encoder.10.ffn_layer_norm.bias\n",
      "encoder.layer.11.attention.self.query.weight -> encoder.11.self_attention.w_q.weight\n",
      "encoder.layer.11.attention.self.query.bias -> encoder.11.self_attention.w_q.bias\n",
      "encoder.layer.11.attention.self.key.weight -> encoder.11.self_attention.w_k.weight\n",
      "encoder.layer.11.attention.self.key.bias -> encoder.11.self_attention.w_k.bias\n",
      "encoder.layer.11.attention.self.value.weight -> encoder.11.self_attention.w_v.weight\n",
      "encoder.layer.11.attention.self.value.bias -> encoder.11.self_attention.w_v.bias\n",
      "encoder.layer.11.attention.output.dense.weight -> encoder.11.self_attention.w_o.weight\n",
      "encoder.layer.11.attention.output.dense.bias -> encoder.11.self_attention.w_o.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight -> encoder.11.self_layer_norm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias -> encoder.11.self_layer_norm.bias\n",
      "encoder.layer.11.intermediate.dense.weight -> encoder.11.position_wise_feed_forward.linear1.weight\n",
      "encoder.layer.11.intermediate.dense.bias -> encoder.11.position_wise_feed_forward.linear1.bias\n",
      "encoder.layer.11.output.dense.weight -> encoder.11.position_wise_feed_forward.linear2.weight\n",
      "encoder.layer.11.output.dense.bias -> encoder.11.position_wise_feed_forward.linear2.bias\n",
      "encoder.layer.11.output.LayerNorm.weight -> encoder.11.ffn_layer_norm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias -> encoder.11.ffn_layer_norm.bias\n",
      "pooler.dense.weight -> linear.weight\n",
      "pooler.dense.bias -> linear.bias\n",
      "bert.embeddings.word_embeddings.weight -> bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight -> bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight -> bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.layer_norm.weight -> bert.embeddings.layer_norm.weight\n",
      "bert.embeddings.layer_norm.bias -> bert.embeddings.layer_norm.bias\n",
      "bert.encoder.0.self_attention.w_q.weight -> bert.encoder.0.self_attention.w_q.weight\n",
      "bert.encoder.0.self_attention.w_q.bias -> bert.encoder.0.self_attention.w_q.bias\n",
      "bert.encoder.0.self_attention.w_k.weight -> bert.encoder.0.self_attention.w_k.weight\n",
      "bert.encoder.0.self_attention.w_k.bias -> bert.encoder.0.self_attention.w_k.bias\n",
      "bert.encoder.0.self_attention.w_v.weight -> bert.encoder.0.self_attention.w_v.weight\n",
      "bert.encoder.0.self_attention.w_v.bias -> bert.encoder.0.self_attention.w_v.bias\n",
      "bert.encoder.0.self_attention.w_o.weight -> bert.encoder.0.self_attention.w_o.weight\n",
      "bert.encoder.0.self_attention.w_o.bias -> bert.encoder.0.self_attention.w_o.bias\n",
      "bert.encoder.0.self_layer_norm.weight -> bert.encoder.0.self_layer_norm.weight\n",
      "bert.encoder.0.self_layer_norm.bias -> bert.encoder.0.self_layer_norm.bias\n",
      "bert.encoder.0.position_wise_feed_forward.linear1.weight -> bert.encoder.0.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.0.position_wise_feed_forward.linear1.bias -> bert.encoder.0.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.0.position_wise_feed_forward.linear2.weight -> bert.encoder.0.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.0.position_wise_feed_forward.linear2.bias -> bert.encoder.0.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.0.ffn_layer_norm.weight -> bert.encoder.0.ffn_layer_norm.weight\n",
      "bert.encoder.0.ffn_layer_norm.bias -> bert.encoder.0.ffn_layer_norm.bias\n",
      "bert.encoder.1.self_attention.w_q.weight -> bert.encoder.1.self_attention.w_q.weight\n",
      "bert.encoder.1.self_attention.w_q.bias -> bert.encoder.1.self_attention.w_q.bias\n",
      "bert.encoder.1.self_attention.w_k.weight -> bert.encoder.1.self_attention.w_k.weight\n",
      "bert.encoder.1.self_attention.w_k.bias -> bert.encoder.1.self_attention.w_k.bias\n",
      "bert.encoder.1.self_attention.w_v.weight -> bert.encoder.1.self_attention.w_v.weight\n",
      "bert.encoder.1.self_attention.w_v.bias -> bert.encoder.1.self_attention.w_v.bias\n",
      "bert.encoder.1.self_attention.w_o.weight -> bert.encoder.1.self_attention.w_o.weight\n",
      "bert.encoder.1.self_attention.w_o.bias -> bert.encoder.1.self_attention.w_o.bias\n",
      "bert.encoder.1.self_layer_norm.weight -> bert.encoder.1.self_layer_norm.weight\n",
      "bert.encoder.1.self_layer_norm.bias -> bert.encoder.1.self_layer_norm.bias\n",
      "bert.encoder.1.position_wise_feed_forward.linear1.weight -> bert.encoder.1.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.1.position_wise_feed_forward.linear1.bias -> bert.encoder.1.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.1.position_wise_feed_forward.linear2.weight -> bert.encoder.1.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.1.position_wise_feed_forward.linear2.bias -> bert.encoder.1.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.1.ffn_layer_norm.weight -> bert.encoder.1.ffn_layer_norm.weight\n",
      "bert.encoder.1.ffn_layer_norm.bias -> bert.encoder.1.ffn_layer_norm.bias\n",
      "bert.encoder.2.self_attention.w_q.weight -> bert.encoder.2.self_attention.w_q.weight\n",
      "bert.encoder.2.self_attention.w_q.bias -> bert.encoder.2.self_attention.w_q.bias\n",
      "bert.encoder.2.self_attention.w_k.weight -> bert.encoder.2.self_attention.w_k.weight\n",
      "bert.encoder.2.self_attention.w_k.bias -> bert.encoder.2.self_attention.w_k.bias\n",
      "bert.encoder.2.self_attention.w_v.weight -> bert.encoder.2.self_attention.w_v.weight\n",
      "bert.encoder.2.self_attention.w_v.bias -> bert.encoder.2.self_attention.w_v.bias\n",
      "bert.encoder.2.self_attention.w_o.weight -> bert.encoder.2.self_attention.w_o.weight\n",
      "bert.encoder.2.self_attention.w_o.bias -> bert.encoder.2.self_attention.w_o.bias\n",
      "bert.encoder.2.self_layer_norm.weight -> bert.encoder.2.self_layer_norm.weight\n",
      "bert.encoder.2.self_layer_norm.bias -> bert.encoder.2.self_layer_norm.bias\n",
      "bert.encoder.2.position_wise_feed_forward.linear1.weight -> bert.encoder.2.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.2.position_wise_feed_forward.linear1.bias -> bert.encoder.2.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.2.position_wise_feed_forward.linear2.weight -> bert.encoder.2.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.2.position_wise_feed_forward.linear2.bias -> bert.encoder.2.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.2.ffn_layer_norm.weight -> bert.encoder.2.ffn_layer_norm.weight\n",
      "bert.encoder.2.ffn_layer_norm.bias -> bert.encoder.2.ffn_layer_norm.bias\n",
      "bert.encoder.3.self_attention.w_q.weight -> bert.encoder.3.self_attention.w_q.weight\n",
      "bert.encoder.3.self_attention.w_q.bias -> bert.encoder.3.self_attention.w_q.bias\n",
      "bert.encoder.3.self_attention.w_k.weight -> bert.encoder.3.self_attention.w_k.weight\n",
      "bert.encoder.3.self_attention.w_k.bias -> bert.encoder.3.self_attention.w_k.bias\n",
      "bert.encoder.3.self_attention.w_v.weight -> bert.encoder.3.self_attention.w_v.weight\n",
      "bert.encoder.3.self_attention.w_v.bias -> bert.encoder.3.self_attention.w_v.bias\n",
      "bert.encoder.3.self_attention.w_o.weight -> bert.encoder.3.self_attention.w_o.weight\n",
      "bert.encoder.3.self_attention.w_o.bias -> bert.encoder.3.self_attention.w_o.bias\n",
      "bert.encoder.3.self_layer_norm.weight -> bert.encoder.3.self_layer_norm.weight\n",
      "bert.encoder.3.self_layer_norm.bias -> bert.encoder.3.self_layer_norm.bias\n",
      "bert.encoder.3.position_wise_feed_forward.linear1.weight -> bert.encoder.3.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.3.position_wise_feed_forward.linear1.bias -> bert.encoder.3.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.3.position_wise_feed_forward.linear2.weight -> bert.encoder.3.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.3.position_wise_feed_forward.linear2.bias -> bert.encoder.3.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.3.ffn_layer_norm.weight -> bert.encoder.3.ffn_layer_norm.weight\n",
      "bert.encoder.3.ffn_layer_norm.bias -> bert.encoder.3.ffn_layer_norm.bias\n",
      "bert.encoder.4.self_attention.w_q.weight -> bert.encoder.4.self_attention.w_q.weight\n",
      "bert.encoder.4.self_attention.w_q.bias -> bert.encoder.4.self_attention.w_q.bias\n",
      "bert.encoder.4.self_attention.w_k.weight -> bert.encoder.4.self_attention.w_k.weight\n",
      "bert.encoder.4.self_attention.w_k.bias -> bert.encoder.4.self_attention.w_k.bias\n",
      "bert.encoder.4.self_attention.w_v.weight -> bert.encoder.4.self_attention.w_v.weight\n",
      "bert.encoder.4.self_attention.w_v.bias -> bert.encoder.4.self_attention.w_v.bias\n",
      "bert.encoder.4.self_attention.w_o.weight -> bert.encoder.4.self_attention.w_o.weight\n",
      "bert.encoder.4.self_attention.w_o.bias -> bert.encoder.4.self_attention.w_o.bias\n",
      "bert.encoder.4.self_layer_norm.weight -> bert.encoder.4.self_layer_norm.weight\n",
      "bert.encoder.4.self_layer_norm.bias -> bert.encoder.4.self_layer_norm.bias\n",
      "bert.encoder.4.position_wise_feed_forward.linear1.weight -> bert.encoder.4.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.4.position_wise_feed_forward.linear1.bias -> bert.encoder.4.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.4.position_wise_feed_forward.linear2.weight -> bert.encoder.4.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.4.position_wise_feed_forward.linear2.bias -> bert.encoder.4.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.4.ffn_layer_norm.weight -> bert.encoder.4.ffn_layer_norm.weight\n",
      "bert.encoder.4.ffn_layer_norm.bias -> bert.encoder.4.ffn_layer_norm.bias\n",
      "bert.encoder.5.self_attention.w_q.weight -> bert.encoder.5.self_attention.w_q.weight\n",
      "bert.encoder.5.self_attention.w_q.bias -> bert.encoder.5.self_attention.w_q.bias\n",
      "bert.encoder.5.self_attention.w_k.weight -> bert.encoder.5.self_attention.w_k.weight\n",
      "bert.encoder.5.self_attention.w_k.bias -> bert.encoder.5.self_attention.w_k.bias\n",
      "bert.encoder.5.self_attention.w_v.weight -> bert.encoder.5.self_attention.w_v.weight\n",
      "bert.encoder.5.self_attention.w_v.bias -> bert.encoder.5.self_attention.w_v.bias\n",
      "bert.encoder.5.self_attention.w_o.weight -> bert.encoder.5.self_attention.w_o.weight\n",
      "bert.encoder.5.self_attention.w_o.bias -> bert.encoder.5.self_attention.w_o.bias\n",
      "bert.encoder.5.self_layer_norm.weight -> bert.encoder.5.self_layer_norm.weight\n",
      "bert.encoder.5.self_layer_norm.bias -> bert.encoder.5.self_layer_norm.bias\n",
      "bert.encoder.5.position_wise_feed_forward.linear1.weight -> bert.encoder.5.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.5.position_wise_feed_forward.linear1.bias -> bert.encoder.5.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.5.position_wise_feed_forward.linear2.weight -> bert.encoder.5.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.5.position_wise_feed_forward.linear2.bias -> bert.encoder.5.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.5.ffn_layer_norm.weight -> bert.encoder.5.ffn_layer_norm.weight\n",
      "bert.encoder.5.ffn_layer_norm.bias -> bert.encoder.5.ffn_layer_norm.bias\n",
      "bert.encoder.6.self_attention.w_q.weight -> bert.encoder.6.self_attention.w_q.weight\n",
      "bert.encoder.6.self_attention.w_q.bias -> bert.encoder.6.self_attention.w_q.bias\n",
      "bert.encoder.6.self_attention.w_k.weight -> bert.encoder.6.self_attention.w_k.weight\n",
      "bert.encoder.6.self_attention.w_k.bias -> bert.encoder.6.self_attention.w_k.bias\n",
      "bert.encoder.6.self_attention.w_v.weight -> bert.encoder.6.self_attention.w_v.weight\n",
      "bert.encoder.6.self_attention.w_v.bias -> bert.encoder.6.self_attention.w_v.bias\n",
      "bert.encoder.6.self_attention.w_o.weight -> bert.encoder.6.self_attention.w_o.weight\n",
      "bert.encoder.6.self_attention.w_o.bias -> bert.encoder.6.self_attention.w_o.bias\n",
      "bert.encoder.6.self_layer_norm.weight -> bert.encoder.6.self_layer_norm.weight\n",
      "bert.encoder.6.self_layer_norm.bias -> bert.encoder.6.self_layer_norm.bias\n",
      "bert.encoder.6.position_wise_feed_forward.linear1.weight -> bert.encoder.6.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.6.position_wise_feed_forward.linear1.bias -> bert.encoder.6.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.6.position_wise_feed_forward.linear2.weight -> bert.encoder.6.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.6.position_wise_feed_forward.linear2.bias -> bert.encoder.6.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.6.ffn_layer_norm.weight -> bert.encoder.6.ffn_layer_norm.weight\n",
      "bert.encoder.6.ffn_layer_norm.bias -> bert.encoder.6.ffn_layer_norm.bias\n",
      "bert.encoder.7.self_attention.w_q.weight -> bert.encoder.7.self_attention.w_q.weight\n",
      "bert.encoder.7.self_attention.w_q.bias -> bert.encoder.7.self_attention.w_q.bias\n",
      "bert.encoder.7.self_attention.w_k.weight -> bert.encoder.7.self_attention.w_k.weight\n",
      "bert.encoder.7.self_attention.w_k.bias -> bert.encoder.7.self_attention.w_k.bias\n",
      "bert.encoder.7.self_attention.w_v.weight -> bert.encoder.7.self_attention.w_v.weight\n",
      "bert.encoder.7.self_attention.w_v.bias -> bert.encoder.7.self_attention.w_v.bias\n",
      "bert.encoder.7.self_attention.w_o.weight -> bert.encoder.7.self_attention.w_o.weight\n",
      "bert.encoder.7.self_attention.w_o.bias -> bert.encoder.7.self_attention.w_o.bias\n",
      "bert.encoder.7.self_layer_norm.weight -> bert.encoder.7.self_layer_norm.weight\n",
      "bert.encoder.7.self_layer_norm.bias -> bert.encoder.7.self_layer_norm.bias\n",
      "bert.encoder.7.position_wise_feed_forward.linear1.weight -> bert.encoder.7.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.7.position_wise_feed_forward.linear1.bias -> bert.encoder.7.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.7.position_wise_feed_forward.linear2.weight -> bert.encoder.7.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.7.position_wise_feed_forward.linear2.bias -> bert.encoder.7.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.7.ffn_layer_norm.weight -> bert.encoder.7.ffn_layer_norm.weight\n",
      "bert.encoder.7.ffn_layer_norm.bias -> bert.encoder.7.ffn_layer_norm.bias\n",
      "bert.encoder.8.self_attention.w_q.weight -> bert.encoder.8.self_attention.w_q.weight\n",
      "bert.encoder.8.self_attention.w_q.bias -> bert.encoder.8.self_attention.w_q.bias\n",
      "bert.encoder.8.self_attention.w_k.weight -> bert.encoder.8.self_attention.w_k.weight\n",
      "bert.encoder.8.self_attention.w_k.bias -> bert.encoder.8.self_attention.w_k.bias\n",
      "bert.encoder.8.self_attention.w_v.weight -> bert.encoder.8.self_attention.w_v.weight\n",
      "bert.encoder.8.self_attention.w_v.bias -> bert.encoder.8.self_attention.w_v.bias\n",
      "bert.encoder.8.self_attention.w_o.weight -> bert.encoder.8.self_attention.w_o.weight\n",
      "bert.encoder.8.self_attention.w_o.bias -> bert.encoder.8.self_attention.w_o.bias\n",
      "bert.encoder.8.self_layer_norm.weight -> bert.encoder.8.self_layer_norm.weight\n",
      "bert.encoder.8.self_layer_norm.bias -> bert.encoder.8.self_layer_norm.bias\n",
      "bert.encoder.8.position_wise_feed_forward.linear1.weight -> bert.encoder.8.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.8.position_wise_feed_forward.linear1.bias -> bert.encoder.8.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.8.position_wise_feed_forward.linear2.weight -> bert.encoder.8.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.8.position_wise_feed_forward.linear2.bias -> bert.encoder.8.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.8.ffn_layer_norm.weight -> bert.encoder.8.ffn_layer_norm.weight\n",
      "bert.encoder.8.ffn_layer_norm.bias -> bert.encoder.8.ffn_layer_norm.bias\n",
      "bert.encoder.9.self_attention.w_q.weight -> bert.encoder.9.self_attention.w_q.weight\n",
      "bert.encoder.9.self_attention.w_q.bias -> bert.encoder.9.self_attention.w_q.bias\n",
      "bert.encoder.9.self_attention.w_k.weight -> bert.encoder.9.self_attention.w_k.weight\n",
      "bert.encoder.9.self_attention.w_k.bias -> bert.encoder.9.self_attention.w_k.bias\n",
      "bert.encoder.9.self_attention.w_v.weight -> bert.encoder.9.self_attention.w_v.weight\n",
      "bert.encoder.9.self_attention.w_v.bias -> bert.encoder.9.self_attention.w_v.bias\n",
      "bert.encoder.9.self_attention.w_o.weight -> bert.encoder.9.self_attention.w_o.weight\n",
      "bert.encoder.9.self_attention.w_o.bias -> bert.encoder.9.self_attention.w_o.bias\n",
      "bert.encoder.9.self_layer_norm.weight -> bert.encoder.9.self_layer_norm.weight\n",
      "bert.encoder.9.self_layer_norm.bias -> bert.encoder.9.self_layer_norm.bias\n",
      "bert.encoder.9.position_wise_feed_forward.linear1.weight -> bert.encoder.9.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.9.position_wise_feed_forward.linear1.bias -> bert.encoder.9.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.9.position_wise_feed_forward.linear2.weight -> bert.encoder.9.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.9.position_wise_feed_forward.linear2.bias -> bert.encoder.9.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.9.ffn_layer_norm.weight -> bert.encoder.9.ffn_layer_norm.weight\n",
      "bert.encoder.9.ffn_layer_norm.bias -> bert.encoder.9.ffn_layer_norm.bias\n",
      "bert.encoder.10.self_attention.w_q.weight -> bert.encoder.10.self_attention.w_q.weight\n",
      "bert.encoder.10.self_attention.w_q.bias -> bert.encoder.10.self_attention.w_q.bias\n",
      "bert.encoder.10.self_attention.w_k.weight -> bert.encoder.10.self_attention.w_k.weight\n",
      "bert.encoder.10.self_attention.w_k.bias -> bert.encoder.10.self_attention.w_k.bias\n",
      "bert.encoder.10.self_attention.w_v.weight -> bert.encoder.10.self_attention.w_v.weight\n",
      "bert.encoder.10.self_attention.w_v.bias -> bert.encoder.10.self_attention.w_v.bias\n",
      "bert.encoder.10.self_attention.w_o.weight -> bert.encoder.10.self_attention.w_o.weight\n",
      "bert.encoder.10.self_attention.w_o.bias -> bert.encoder.10.self_attention.w_o.bias\n",
      "bert.encoder.10.self_layer_norm.weight -> bert.encoder.10.self_layer_norm.weight\n",
      "bert.encoder.10.self_layer_norm.bias -> bert.encoder.10.self_layer_norm.bias\n",
      "bert.encoder.10.position_wise_feed_forward.linear1.weight -> bert.encoder.10.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.10.position_wise_feed_forward.linear1.bias -> bert.encoder.10.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.10.position_wise_feed_forward.linear2.weight -> bert.encoder.10.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.10.position_wise_feed_forward.linear2.bias -> bert.encoder.10.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.10.ffn_layer_norm.weight -> bert.encoder.10.ffn_layer_norm.weight\n",
      "bert.encoder.10.ffn_layer_norm.bias -> bert.encoder.10.ffn_layer_norm.bias\n",
      "bert.encoder.11.self_attention.w_q.weight -> bert.encoder.11.self_attention.w_q.weight\n",
      "bert.encoder.11.self_attention.w_q.bias -> bert.encoder.11.self_attention.w_q.bias\n",
      "bert.encoder.11.self_attention.w_k.weight -> bert.encoder.11.self_attention.w_k.weight\n",
      "bert.encoder.11.self_attention.w_k.bias -> bert.encoder.11.self_attention.w_k.bias\n",
      "bert.encoder.11.self_attention.w_v.weight -> bert.encoder.11.self_attention.w_v.weight\n",
      "bert.encoder.11.self_attention.w_v.bias -> bert.encoder.11.self_attention.w_v.bias\n",
      "bert.encoder.11.self_attention.w_o.weight -> bert.encoder.11.self_attention.w_o.weight\n",
      "bert.encoder.11.self_attention.w_o.bias -> bert.encoder.11.self_attention.w_o.bias\n",
      "bert.encoder.11.self_layer_norm.weight -> bert.encoder.11.self_layer_norm.weight\n",
      "bert.encoder.11.self_layer_norm.bias -> bert.encoder.11.self_layer_norm.bias\n",
      "bert.encoder.11.position_wise_feed_forward.linear1.weight -> bert.encoder.11.position_wise_feed_forward.linear1.weight\n",
      "bert.encoder.11.position_wise_feed_forward.linear1.bias -> bert.encoder.11.position_wise_feed_forward.linear1.bias\n",
      "bert.encoder.11.position_wise_feed_forward.linear2.weight -> bert.encoder.11.position_wise_feed_forward.linear2.weight\n",
      "bert.encoder.11.position_wise_feed_forward.linear2.bias -> bert.encoder.11.position_wise_feed_forward.linear2.bias\n",
      "bert.encoder.11.ffn_layer_norm.weight -> bert.encoder.11.ffn_layer_norm.weight\n",
      "bert.encoder.11.ffn_layer_norm.bias -> bert.encoder.11.ffn_layer_norm.bias\n",
      "bert.linear.weight -> bert.linear.weight\n",
      "bert.linear.bias -> bert.linear.bias\n",
      "oper_layers.0.weight -> oper_layers.0.weight\n",
      "oper_layers.0.bias -> oper_layers.0.bias\n",
      "oper_layers.1.weight -> oper_layers.1.weight\n",
      "oper_layers.1.bias -> oper_layers.1.bias\n",
      "oper_layers.2.weight -> oper_layers.2.weight\n",
      "oper_layers.2.bias -> oper_layers.2.bias\n",
      "oper_layers.3.weight -> oper_layers.3.weight\n",
      "oper_layers.3.bias -> oper_layers.3.bias\n",
      "oper_layers.4.weight -> oper_layers.4.weight\n",
      "oper_layers.4.bias -> oper_layers.4.bias\n",
      "oper_layers.5.weight -> oper_layers.5.weight\n",
      "oper_layers.5.bias -> oper_layers.5.bias\n",
      "oper_layers.6.weight -> oper_layers.6.weight\n",
      "oper_layers.6.bias -> oper_layers.6.bias\n",
      "oper_layers.7.weight -> oper_layers.7.weight\n",
      "oper_layers.7.bias -> oper_layers.7.bias\n",
      "oper_layers.8.weight -> oper_layers.8.weight\n",
      "oper_layers.8.bias -> oper_layers.8.bias\n",
      "span_layers.0.weight -> span_layers.0.weight\n",
      "span_layers.0.bias -> span_layers.0.bias\n",
      "span_layers.1.weight -> span_layers.1.weight\n",
      "span_layers.1.bias -> span_layers.1.bias\n",
      "span_layers.2.weight -> span_layers.2.weight\n",
      "span_layers.2.bias -> span_layers.2.bias\n",
      "span_layers.3.weight -> span_layers.3.weight\n",
      "span_layers.3.bias -> span_layers.3.bias\n",
      "span_layers.4.weight -> span_layers.4.weight\n",
      "span_layers.4.bias -> span_layers.4.bias\n",
      "span_layers.5.weight -> span_layers.5.weight\n",
      "span_layers.5.bias -> span_layers.5.bias\n",
      "span_layers.6.weight -> span_layers.6.weight\n",
      "span_layers.6.bias -> span_layers.6.bias\n",
      "span_layers.7.weight -> span_layers.7.weight\n",
      "span_layers.7.bias -> span_layers.7.bias\n",
      "span_layers.8.weight -> span_layers.8.weight\n",
      "span_layers.8.bias -> span_layers.8.bias\n",
      "refer_layers.0.weight -> refer_layers.0.weight\n",
      "refer_layers.0.bias -> refer_layers.0.bias\n",
      "refer_layers.1.weight -> refer_layers.1.weight\n",
      "refer_layers.1.bias -> refer_layers.1.bias\n",
      "refer_layers.2.weight -> refer_layers.2.weight\n",
      "refer_layers.2.bias -> refer_layers.2.bias\n",
      "refer_layers.3.weight -> refer_layers.3.weight\n",
      "refer_layers.3.bias -> refer_layers.3.bias\n",
      "refer_layers.4.weight -> refer_layers.4.weight\n",
      "refer_layers.4.bias -> refer_layers.4.bias\n",
      "refer_layers.5.weight -> refer_layers.5.weight\n",
      "refer_layers.5.bias -> refer_layers.5.bias\n",
      "refer_layers.6.weight -> refer_layers.6.weight\n",
      "refer_layers.6.bias -> refer_layers.6.bias\n",
      "refer_layers.7.weight -> refer_layers.7.weight\n",
      "refer_layers.7.bias -> refer_layers.7.bias\n",
      "refer_layers.8.weight -> refer_layers.8.weight\n",
      "refer_layers.8.bias -> refer_layers.8.bias\n",
      "inform_aux_layer.weight -> inform_aux_layer.weight\n",
      "inform_aux_layer.bias -> inform_aux_layer.bias\n",
      "ds_aux_layer.weight -> ds_aux_layer.weight\n",
      "ds_aux_layer.bias -> ds_aux_layer.bias\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "tods = DeepTODS(CHATBOT_NAME, DOMAINS, ONTOLOGY_PATH, LABEL_MAPS_PATH, POLICY, START, TEMPLATES, NON_REFERABLE_SLOTS, NON_REFERABLE_PAIRS, FROM_SCRATCH)\n",
    "tods.load_dst_model(MODEL_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_gui = chat_gui(tods.infer)\n",
    "chat_gui.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "tods.reset()\n",
    "print(tods.get_dialogue_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "Hi I am Tody, I can help you reserve a restaurant?\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'restaurant-meal': 'lunch'}\n",
      "what type of food do you want and in what area?\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('Hi, I want to book a table for lunch.')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'restaurant-meal': 'lunch', 'restaurant-category': 'egyptian', 'restaurant-location': 'cairo'}\n",
      "what is your preferred price range and rating?\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('I would like to have egyptian food in cairo.')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'restaurant-meal': 'lunch', 'restaurant-category': 'egyptian', 'restaurant-location': 'cairo', 'restaurant-price_range': 'cheap', 'restaurant-rating': 'good'}\n",
      "how many people will be in your party?\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('I want a cheap restaurant with good rating.')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'restaurant-meal': 'lunch', 'restaurant-category': 'egyptian', 'restaurant-location': 'cairo', 'restaurant-price_range': 'cheap', 'restaurant-rating': 'good', 'restaurant-num_people': '4'}\n",
      "what time and date would you like to reserve a table for?\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('I have 4 people in my party.')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'restaurant-meal': 'lunch', 'restaurant-category': 'egyptian', 'restaurant-location': 'cairo', 'restaurant-price_range': 'cheap', 'restaurant-rating': 'good', 'restaurant-num_people': '4', 'restaurant-date': 'tuesday', 'restaurant-time': '04:00 am'}\n",
      "May I suggest kfc restaurant?\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('4am on the tuesday.')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'restaurant-meal': 'lunch', 'restaurant-category': 'egyptian', 'restaurant-location': 'cairo', 'restaurant-price_range': 'cheap', 'restaurant-rating': 'good', 'restaurant-num_people': '4', 'restaurant-date': 'tuesday', 'restaurant-time': '04:00 am'}\n",
      "May I suggest kfc restaurant?\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('No, I hate kfc.')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'restaurant-meal': 'lunch', 'restaurant-category': 'egyptian', 'restaurant-location': 'cairo', 'restaurant-price_range': 'cheap', 'restaurant-rating': 'good', 'restaurant-num_people': '4', 'restaurant-date': 'tuesday', 'restaurant-time': '04:00 am', 'restaurant-restaurant_name': 'kfc'}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "response = tods.infer('Yes, I would love that.')\n",
    "print(tods.get_dialogue_state())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
